{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paren Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import torch\n",
    "from cases.paren_checker import HighLevelParensBalanceChecker, test_HL_parens_balancer_components, BalancedParensDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Balance tests passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_HL_parens_balancer_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making IIT dataset\n",
      "(tensor([3, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2]), tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "hl_model = HighLevelParensBalanceChecker()\n",
    "corr = hl_model.get_correspondence()\n",
    "dataset = BalancedParensDataset(N_samples=5_000, n_ctx=hl_model.get_ll_model_cfg().n_ctx, seed=42)\n",
    "train_set, test_set = dataset.get_IIT_train_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4)\n",
      "[[3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2], [3, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 2], [3, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 2], [3, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 2], [3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], [3, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2], [3, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 2], [3, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 2], [3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2], [3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 2]]\n",
      "[[[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.get_dataset().shape)\n",
    "print(dataset.get_dataset()[:10]['tokens'])\n",
    "print(dataset.get_dataset()[:10]['labels'])\n",
    "for i in range(10):\n",
    "    tokens, labels, hl_outputs = dataset.get_dataset()[i]['tokens'], dataset.get_dataset()[i]['labels'], hl_model((torch.tensor(dataset.get_dataset()[i]['tokens'])[None,:], None, None))\n",
    "    nonzero = (torch.tensor(labels) - hl_outputs[0]).nonzero()\n",
    "    if nonzero.numel() > 0:\n",
    "        print(tokens, torch.unique(nonzero[:,0]))\n",
    "        bad_indices = torch.unique(nonzero[:,0]).tolist()\n",
    "        for idx in bad_indices:\n",
    "            print(labels[idx], hl_outputs[0,idx])\n",
    "    # print(tokens)\n",
    "    # print(hl_outputs[0,:3])\n",
    "    # print(hl_model.get_ll_model().decode(dataset.get_dataset()[i]['tokens']))\n",
    "    # print(hl_model.get_ll_model().decode(dataset.get_dataset()[i]['labels']))\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'relu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 8,\n",
      " 'd_mlp': 128,\n",
      " 'd_model': 32,\n",
      " 'd_vocab': 4,\n",
      " 'd_vocab_out': 4,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='mps'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': True,\n",
      " 'initializer_range': 0.1414213562373095,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'custom',\n",
      " 'n_ctx': 22,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 4,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 3,\n",
      " 'n_params': 36864,\n",
      " 'normalization_type': 'LN',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': None,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': None,\n",
      " 'tokenizer_prepends_bos': None,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "print(hl_model.get_ll_model_cfg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iit.model_pairs.strict_iit_model_pair import StrictIITModelPair\n",
    "from iit.utils.index import Ix\n",
    "import torch\n",
    "\n",
    "n_epochs = 100\n",
    "training_args = {\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"use_single_loss\": False,\n",
    "    \"behavior_weight\": 0., #basically doubles the strict weight's job.\n",
    "    \"iit_weight\": 1.,\n",
    "    \"strict_weight\": 1.0,\n",
    "    \"clip_grad_norm\": 1.0,\n",
    "    \"iit_weight_schedule\" : lambda s, i: s,\n",
    "    \"strict_weight_schedule\" : lambda s, i: s,\n",
    "    \"behavior_weight_schedule\" : lambda s, i: s, #0.955*s if 0.955**i > 0.01 else s, #have behavior weight decay over time\n",
    "    \"early_stop\" : True,\n",
    "    \"lr_scheduler\": torch.optim.lr_scheduler.LinearLR,\n",
    "    \"scheduler_kwargs\": dict(start_factor=1, end_factor=0, total_iters=n_epochs),\n",
    "    \"scheduler_val_metric\": [\"val/accuracy\", \"val/IIA\"], #for ReduceLRonPlateau\n",
    "    \"scheduler_mode\": \"max\", #for ReduceLRonPlateau\n",
    "}\n",
    "\n",
    "class LastTokenStrictIITModelPair(StrictIITModelPair):\n",
    "    def get_label_idxs(self):\n",
    "        return Ix[[None,-1]]\n",
    "\n",
    "    # def loss_fn(self) -> Callable[[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "    #     return torch.nn.CrossEntropyLoss()\n",
    "\n",
    "ll_model = hl_model.get_ll_model()\n",
    "model_pair = StrictIITModelPair(hl_model=hl_model, ll_model=ll_model, corr=corr, training_args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args={'batch_size': 256, 'lr': 0.001, 'num_workers': 0, 'early_stop': True, 'lr_scheduler': <class 'torch.optim.lr_scheduler.LinearLR'>, 'scheduler_val_metric': ['val/accuracy', 'val/IIA'], 'scheduler_mode': 'max', 'scheduler_kwargs': {'start_factor': 1, 'end_factor': 0, 'total_iters': 100}, 'clip_grad_norm': 1.0, 'seed': 0, 'detach_while_caching': True, 'iit_weight_schedule': <function <lambda> at 0x1689663e0>, 'strict_weight_schedule': <function <lambda> at 0x37f6bef20>, 'behavior_weight_schedule': <function <lambda> at 0x37f73a8e0>, 'atol': 0.05, 'use_single_loss': False, 'iit_weight': 1.0, 'behavior_weight': 0.0, 'strict_weight': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26556738858e4a46bb2b653dbabd6eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Training Epochs:   0%|          | 0/100 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: lr: 9.90e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.69e-01, train/behavior_loss: 0.00e+00, train/strict_loss: 1.98e-01, val/iit_loss: 1.26e-01, val/IIA: 95.85, val/accuracy: 96.56, val/strict_accuracy: 96.56\n",
      "Epoch 2: lr: 9.80e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.63e-01, train/behavior_loss: 0.00e+00, train/strict_loss: 7.04e-02, val/iit_loss: 2.04e-01, val/IIA: 93.92, val/accuracy: 98.28, val/strict_accuracy: 98.28\n",
      "Epoch 3: lr: 9.70e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.76e-01, train/behavior_loss: 0.00e+00, train/strict_loss: 4.98e-02, val/iit_loss: 8.42e-02, val/IIA: 97.47, val/accuracy: 99.17, val/strict_accuracy: 99.17\n",
      "Epoch 4: lr: 9.60e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 7.00e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 3.01e-02, val/iit_loss: 7.67e-02, val/IIA: 97.82, val/accuracy: 99.65, val/strict_accuracy: 99.65\n",
      "Epoch 5: lr: 9.50e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.29e-01, train/behavior_loss: 0.00e+00, train/strict_loss: 2.85e-02, val/iit_loss: 9.66e-02, val/IIA: 97.14, val/accuracy: 99.78, val/strict_accuracy: 99.78\n",
      "Epoch 6: lr: 9.40e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 7.96e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.53e-02, val/iit_loss: 4.54e-02, val/IIA: 98.63, val/accuracy: 99.86, val/strict_accuracy: 99.86\n",
      "Epoch 7: lr: 9.30e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 7.11e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 8.93e-03, val/iit_loss: 1.88e-01, val/IIA: 95.34, val/accuracy: 99.97, val/strict_accuracy: 99.97\n",
      "Epoch 8: lr: 9.20e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 8.11e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.44e-02, val/iit_loss: 1.03e-01, val/IIA: 97.36, val/accuracy: 99.93, val/strict_accuracy: 99.93\n",
      "Epoch 9: lr: 9.10e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 6.93e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 7.71e-03, val/iit_loss: 2.36e-02, val/IIA: 99.37, val/accuracy: 99.92, val/strict_accuracy: 99.92\n",
      "Epoch 10: lr: 9.00e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 5.16e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.03e-02, val/iit_loss: 5.62e-02, val/IIA: 98.66, val/accuracy: 99.96, val/strict_accuracy: 99.96\n",
      "Epoch 11: lr: 8.90e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 6.20e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 6.67e-03, val/iit_loss: 4.76e-02, val/IIA: 98.79, val/accuracy: 99.95, val/strict_accuracy: 99.95\n",
      "Epoch 12: lr: 8.80e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 4.00e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 5.70e-03, val/iit_loss: 1.08e-01, val/IIA: 97.12, val/accuracy: 99.95, val/strict_accuracy: 99.95\n",
      "Epoch 13: lr: 8.70e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 4.50e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 3.06e-03, val/iit_loss: 8.99e-02, val/IIA: 97.83, val/accuracy: 99.71, val/strict_accuracy: 99.71\n",
      "Epoch 14: lr: 8.60e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 5.21e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 6.97e-03, val/iit_loss: 3.87e-02, val/IIA: 98.52, val/accuracy: 99.91, val/strict_accuracy: 99.91\n",
      "Epoch 15: lr: 8.50e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 4.27e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 3.75e-03, val/iit_loss: 5.27e-02, val/IIA: 98.29, val/accuracy: 99.92, val/strict_accuracy: 99.92\n",
      "Epoch 16: lr: 8.40e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 3.49e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 3.34e-03, val/iit_loss: 7.33e-02, val/IIA: 97.84, val/accuracy: 99.92, val/strict_accuracy: 99.92\n",
      "Epoch 17: lr: 8.30e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.76e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 3.40e-03, val/iit_loss: 4.01e-02, val/IIA: 99.16, val/accuracy: 99.96, val/strict_accuracy: 99.96\n",
      "Epoch 18: lr: 8.20e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 6.54e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 4.70e-03, val/iit_loss: 4.59e-03, val/IIA: 99.94, val/accuracy: 99.94, val/strict_accuracy: 99.94\n",
      "Epoch 19: lr: 8.10e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.84e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 2.08e-03, val/iit_loss: 1.72e-02, val/IIA: 99.30, val/accuracy: 99.98, val/strict_accuracy: 99.98\n",
      "Epoch 20: lr: 8.00e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 3.65e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 6.20e-03, val/iit_loss: 6.20e-02, val/IIA: 97.68, val/accuracy: 99.93, val/strict_accuracy: 99.93\n",
      "Epoch 21: lr: 7.90e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.29e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.46e-03, val/iit_loss: 3.12e-02, val/IIA: 98.82, val/accuracy: 99.98, val/strict_accuracy: 99.98\n",
      "Epoch 22: lr: 7.80e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.82e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.36e-03, val/iit_loss: 2.93e-02, val/IIA: 98.83, val/accuracy: 99.96, val/strict_accuracy: 99.96\n",
      "Epoch 23: lr: 7.70e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.27e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 2.86e-03, val/iit_loss: 1.06e-02, val/IIA: 99.74, val/accuracy: 99.91, val/strict_accuracy: 99.91\n",
      "Epoch 24: lr: 7.60e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.95e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.15e-03, val/iit_loss: 2.95e-02, val/IIA: 98.88, val/accuracy: 99.96, val/strict_accuracy: 99.96\n",
      "Epoch 25: lr: 7.50e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.99e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.31e-03, val/iit_loss: 8.82e-02, val/IIA: 97.10, val/accuracy: 99.28, val/strict_accuracy: 99.28\n",
      "Epoch 26: lr: 7.40e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 3.93e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 4.25e-03, val/iit_loss: 2.92e-02, val/IIA: 98.80, val/accuracy: 99.94, val/strict_accuracy: 99.94\n",
      "Epoch 27: lr: 7.30e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 3.02e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 7.86e-03, val/iit_loss: 4.65e-03, val/IIA: 99.94, val/accuracy: 99.95, val/strict_accuracy: 99.95\n",
      "Epoch 28: lr: 7.20e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 3.73e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 8.35e-03, val/iit_loss: 1.19e-02, val/IIA: 99.63, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 29: lr: 7.10e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.25e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 8.99e-04, val/iit_loss: 1.25e-02, val/IIA: 99.51, val/accuracy: 99.99, val/strict_accuracy: 99.99\n",
      "Epoch 30: lr: 7.00e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.40e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 5.89e-04, val/iit_loss: 1.95e-02, val/IIA: 99.22, val/accuracy: 99.99, val/strict_accuracy: 99.99\n",
      "Epoch 31: lr: 6.90e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 4.37e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 3.53e-03, val/iit_loss: 3.42e-02, val/IIA: 98.42, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 32: lr: 6.80e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 3.51e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.27e-03, val/iit_loss: 6.04e-02, val/IIA: 97.45, val/accuracy: 99.97, val/strict_accuracy: 99.97\n",
      "Epoch 33: lr: 6.70e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.78e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 3.00e-03, val/iit_loss: 1.74e-03, val/IIA: 99.96, val/accuracy: 99.97, val/strict_accuracy: 99.97\n",
      "Epoch 34: lr: 6.60e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.61e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.94e-03, val/iit_loss: 2.41e-02, val/IIA: 99.09, val/accuracy: 99.99, val/strict_accuracy: 99.99\n",
      "Epoch 35: lr: 6.50e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.33e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 7.42e-04, val/iit_loss: 4.98e-03, val/IIA: 99.87, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 36: lr: 6.40e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.59e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 4.96e-04, val/iit_loss: 2.61e-02, val/IIA: 98.95, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 37: lr: 6.30e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.02e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.68e-03, val/iit_loss: 6.33e-03, val/IIA: 99.77, val/accuracy: 99.99, val/strict_accuracy: 99.99\n",
      "Epoch 38: lr: 6.20e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.56e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 5.46e-04, val/iit_loss: 2.18e-03, val/IIA: 99.92, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 39: lr: 6.10e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.78e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 1.61e-03, val/iit_loss: 1.96e-03, val/IIA: 99.96, val/accuracy: 99.99, val/strict_accuracy: 99.99\n",
      "Epoch 40: lr: 6.00e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 2.01e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 6.11e-03, val/iit_loss: 5.78e-03, val/IIA: 99.88, val/accuracy: 99.97, val/strict_accuracy: 99.97\n",
      "Epoch 41: lr: 5.90e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 6.04e-03, train/behavior_loss: 0.00e+00, train/strict_loss: 4.76e-04, val/iit_loss: 2.41e-02, val/IIA: 99.10, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 42: lr: 5.80e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.28e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 6.89e-04, val/iit_loss: 9.10e-03, val/IIA: 99.71, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 43: lr: 5.70e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.77e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 9.07e-04, val/iit_loss: 2.45e-02, val/IIA: 99.11, val/accuracy: 99.99, val/strict_accuracy: 99.99\n",
      "Epoch 44: lr: 5.60e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.84e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 5.02e-03, val/iit_loss: 5.63e-03, val/IIA: 99.80, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 45: lr: 5.50e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.65e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 5.26e-04, val/iit_loss: 1.09e-02, val/IIA: 99.55, val/accuracy: 100.00, val/strict_accuracy: 100.00\n",
      "Epoch 46: lr: 5.40e-04, iit_weight: 1.00e+00, behavior_weight: 0.00e+00, strict_weight: 1.00e+00, train/iit_loss: 1.66e-02, train/behavior_loss: 0.00e+00, train/strict_loss: 3.95e-03, val/iit_loss: 6.75e-04, val/IIA: 100.00, val/accuracy: 100.00, val/strict_accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "model_pair.train(\n",
    "    train_set=train_set,\n",
    "    test_set=test_set,\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    epochs=n_epochs,\n",
    "    optimizer_kwargs=dict(weight_decay=1e-4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
