{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/evananders/far_cluster/iit/iit/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import iit\n",
    "print(iit.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens as tl\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import wandb\n",
    "\n",
    "from iit.model_pairs.ioi_model_pair import IOI_ModelPair\n",
    "from iit.utils.iit_dataset import train_test_split\n",
    "from iit.utils.iit_dataset import IITDataset\n",
    "# from iit.model_pairs.base_model_pair import *\n",
    "# from iit.utils.metric import *\n",
    "from iit.tasks.ioi import (\n",
    "    NAMES,\n",
    "    make_ioi_dataset_and_hl,\n",
    "    make_corr_dict,\n",
    "    ioi_cfg,\n",
    "    suffixes\n",
    ")\n",
    "from iit.utils.correspondence import Correspondence\n",
    "from iit.utils.argparsing import IOIArgParseNamespace\n",
    "\n",
    "\n",
    "def train_ioi(args: IOIArgParseNamespace) -> IOI_ModelPair:\n",
    "    device = args.device\n",
    "    num_samples = args.num_samples\n",
    "    epochs = args.epochs\n",
    "    use_wandb = args.use_wandb\n",
    "\n",
    "    training_args = {\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"lr\": args.lr,\n",
    "        \"iit_weight\": args.iit,\n",
    "        \"behavior_weight\": args.b,\n",
    "        \"strict_weight\": args.s,\n",
    "        \"next_token\": args.next_token,\n",
    "        \"lr_scheduler\": None,\n",
    "        \"clip_grad_norm\": args.clip_grad_norm,\n",
    "        \"early_stop\": True,\n",
    "        \"use_single_loss\": args.use_single_loss,\n",
    "    }\n",
    "    t.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    ll_cfg = tl.HookedTransformer.from_pretrained(\n",
    "        \"gpt2\"\n",
    "    ).cfg.to_dict()\n",
    "    ll_cfg.update(ioi_cfg)\n",
    "\n",
    "    ll_cfg[\"init_weights\"] = True\n",
    "    ll_model = tl.HookedTransformer(ll_cfg).to(device)\n",
    "    print(\"making ioi dataset and hl\")\n",
    "    ioi_dataset, hl_model = make_ioi_dataset_and_hl(\n",
    "        num_samples, ll_model, NAMES, device=args.device, verbose=True\n",
    "    )\n",
    "    print(\"making IIT dataset\")\n",
    "    train_ioi_dataset, test_ioi_dataset = train_test_split(\n",
    "        ioi_dataset, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_set = IITDataset(train_ioi_dataset, train_ioi_dataset, seed=0)\n",
    "    test_set = IITDataset(test_ioi_dataset, test_ioi_dataset, seed=0)\n",
    "    print(\"making ioi model pair\")\n",
    "    corr_dict = make_corr_dict(include_mlp=args.include_mlp)\n",
    "    corr = Correspondence.make_corr_from_dict(corr_dict, suffixes=suffixes)\n",
    "    model_pair = IOI_ModelPair(\n",
    "        ll_model=ll_model,\n",
    "        hl_model=hl_model,\n",
    "        corr=corr,\n",
    "        training_args=training_args,\n",
    "    )\n",
    "    print(\"training ioi model pair\")\n",
    "    model_pair.train(train_set, test_set, epochs=epochs, use_wandb=use_wandb)\n",
    "    print(f\"done training\")\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "    return model_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "train_args = IOIArgParseNamespace(\n",
    "    include_mlp = True,\n",
    "    use_wandb = False,\n",
    "    num_samples = 120_000,\n",
    "    batch_size = 128,\n",
    "    next_token = False,\n",
    "    \n",
    "    epochs = 100,\n",
    "    lr = 1e-3,\n",
    "    iit = 1.0,\n",
    "    b = 1.0,\n",
    "    s = 1.0,\n",
    "    clip_grad_norm = 1.0,\n",
    "    use_single_loss = True,\n",
    "    save_to_wandb = False\n",
    ")\n",
    "\n",
    "D_MODEL = 32\n",
    "N_CTX = 23\n",
    "N_LAYERS = 3\n",
    "N_HEADS = 4\n",
    "D_VOCAB = 4\n",
    "\n",
    "#Want to specify this somewhere central, e.g., iit.tasks.ioi but iit.tasks.parens\n",
    "ll_cfg = tl.HookedTransformerConfig(\n",
    "        n_layers = N_LAYERS,\n",
    "        d_model = D_MODEL,\n",
    "        n_ctx = N_CTX,\n",
    "        d_head = D_MODEL // N_HEADS,\n",
    "        d_vocab = D_VOCAB,\n",
    "        act_fn = \"relu\",\n",
    ")\n",
    "\n",
    "ll_model = tl.HookedTransformer(ll_cfg).to(train_args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making IIT dataset\n"
     ]
    }
   ],
   "source": [
    "from paren_checker import HighLevelParensBalanceChecker, TwoTaskParensDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "hl_model = HighLevelParensBalanceChecker(device=train_args.device)\n",
    "dataset = TwoTaskParensDataset(\n",
    "    N_samples = 20_000,\n",
    "    n_ctx = N_CTX,\n",
    "    seed = 42,\n",
    ")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets, markers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list or numpy array): List or array of input data.\n",
    "            targets (list or numpy array): List or array of target data.\n",
    "        \"\"\"\n",
    "        self.data = t.tensor(data).to(int)\n",
    "        self.targets = t.tensor(targets).to(int)\n",
    "        self.markers = t.tensor(markers).to(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index\n",
    "        Returns:\n",
    "            tuple: (input tensor, target tensor)\n",
    "        \"\"\"\n",
    "        return self.data[idx], self.targets[idx], self.markers[idx]\n",
    "\n",
    "\n",
    "decorated_dset = CustomDataset(\n",
    "    data = dataset.get_dataset()['tokens'],\n",
    "    targets = np.array(dataset.get_dataset()['labels'])[:, None],\n",
    "    markers = np.array(dataset.get_dataset()['markers'])[:, None]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"making IIT dataset\")\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    decorated_dset, test_size=0.2, random_state=42\n",
    ")\n",
    "train_set = IITDataset(train_dataset, train_dataset, seed=0)\n",
    "test_set = IITDataset(test_dataset, test_dataset, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([3, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2]), tensor([1]), tensor([1])), (tensor([3, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2]), tensor([0]), tensor([2])))\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making model pair\n",
      "{'input_hook': HookPoint(), 'left_parens_hook': HookPoint(), 'right_parens_hook': HookPoint(), 'task_hook': HookPoint(), 'greater_hook': HookPoint(), 'elevation_hook': HookPoint(), 'mlp0_hook': HookPoint(), 'mlp1_hook': HookPoint(), 'horizon_lookback_hook': HookPoint(), 'output_check_hook': HookPoint()}\n",
      "dict_keys([input_hook, left_parens_hook, right_parens_hook, task_hook, mlp0_hook, mlp1_hook, horizon_lookback_hook, output_check_hook])\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "from dataclasses import asdict\n",
    "from iit.utils.index import Ix, TorchIndex\n",
    "from iit.utils.nodes import HLNode\n",
    "\n",
    "all_attns = [f\"blocks.{i}.attn.hook_z\" for i in range(ll_cfg.n_layers)]\n",
    "all_mlps = [f\"blocks.{i}.mlp.hook_post\" for i in range(ll_cfg.n_layers)]\n",
    "all_nodes_hook = \"blocks.0.hook_resid_pre\"\n",
    "head_index: Callable[[int], TorchIndex] = lambda idx: Ix[[None, None, idx, None]]\n",
    "#make correlation\n",
    "corr_dict = {\n",
    "        'input_hook' :           [(all_nodes_hook,  Ix[[None]], None)],\n",
    "        'left_parens_hook' :     [(all_attns[0],    head_index(0), None)],\n",
    "        'right_parens_hook' :    [(all_attns[0],    head_index(1), None)],\n",
    "        'task_hook':             [(all_attns[0],    head_index(2), None)],\n",
    "        'mlp0_hook':             [(all_mlps[0],     Ix[[None]], None)],\n",
    "        'mlp1_hook' :            [(all_mlps[1],     Ix[[None]], None)],\n",
    "        'horizon_lookback_hook': [(all_attns[2],    head_index(3), None)],\n",
    "        'output_check_hook' :    [(all_mlps[2],     Ix[[None]], None)]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"making model pair\")\n",
    "corr = Correspondence.make_corr_from_dict(corr_dict)\n",
    "\n",
    "class ParensModelPair(IOI_ModelPair):\n",
    "\n",
    "    @property\n",
    "    def loss_fn(self) -> Callable[[t.Tensor, t.Tensor], t.Tensor]:\n",
    "\n",
    "        def per_token_weighted_cross_entropy(output: t.Tensor, target: t.Tensor) -> t.Tensor:\n",
    "            if len(target.shape) == 2 and target.shape[1] == 2: #dumb one-hot fix\n",
    "                true_target = t.zeros(target.shape[0])\n",
    "                true_target[target[:, 1] == 1] = 1\n",
    "                target = true_target\n",
    "            return t.nn.BCEWithLogitsLoss()(output[:,-1], target.to(float).squeeze())\n",
    "            \n",
    "        self.__loss_fn = per_token_weighted_cross_entropy\n",
    "        return self.__loss_fn\n",
    "\n",
    "    def get_behaviour_loss_over_batch(\n",
    "        self,\n",
    "        base_input: tuple[t.Tensor, t.Tensor, t.Tensor],\n",
    "        loss_fn: Callable[[t.Tensor, t.Tensor], t.Tensor],\n",
    "    ) -> t.Tensor:\n",
    "        x, y = base_input[0:2]\n",
    "        ll_output = self.ll_model(x)\n",
    "        # hl_argmax = t.argmax(hl_output[:, -1, :], dim=-1)\n",
    "\n",
    "        loss = loss_fn(ll_output[:, -1, :], y[:,0])\n",
    "        return loss\n",
    "    \n",
    "    #TODO: Fix this so that it works with ParensChecker.\n",
    "    def run_eval_step(\n",
    "        self,\n",
    "        base_input: tuple[t.Tensor, t.Tensor, t.Tensor],\n",
    "        ablation_input: tuple[t.Tensor, t.Tensor, t.Tensor],\n",
    "        loss_fn: Callable[[t.Tensor, t.Tensor], t.Tensor],\n",
    "    ) -> dict:\n",
    "        # compute IIT loss and accuracy on last token position only\n",
    "        hl_node = self.sample_hl_name()\n",
    "        hl_output, ll_output = self.do_intervention(base_input, ablation_input, hl_node)\n",
    "        # CrossEntropyLoss needs target probs, not logits\n",
    "        # hl_output = t.nn.functional.softmax(hl_output, dim=-1)\n",
    "        hl_argmax = t.argmax(hl_output[:, -1, :], dim=-1)\n",
    "        hl_one_hot = t.nn.functional.one_hot(hl_argmax, num_classes=hl_output.shape[-1])\n",
    "        hl_probs = hl_one_hot.float()\n",
    "        assert self.hl_model.is_categorical()\n",
    "        loss = loss_fn(ll_output[:, -1, :], hl_probs)\n",
    "        if ll_output.shape == hl_output.shape:\n",
    "            # To handle the case when labels are one-hot\n",
    "            hl_output = t.argmax(hl_output, dim=-1)\n",
    "        top1 = t.argmax(ll_output, dim=-1)\n",
    "        accuracy = (top1[:, -1] == hl_output[:, -1]).float().mean().item()\n",
    "        IIA = accuracy\n",
    "\n",
    "        # compute behavioral accuracy\n",
    "        base_x, base_y = base_input[0:2]\n",
    "        output = self.ll_model(base_x)\n",
    "        top1 = t.argmax(output, dim=-1)  # batch n_ctx\n",
    "        if output.shape == base_y.shape:\n",
    "            # To handle the case when labels are one-hot\n",
    "            # TODO: is there a better way?\n",
    "            base_y = t.argmax(base_y, dim=-1)  # batch n_ctx\n",
    "        per_token_accuracy = (top1 == base_y).float().mean(dim=0).cpu().numpy()\n",
    "\n",
    "\n",
    "        # strict accuracy\n",
    "        base_x, base_y = base_input[0:2]\n",
    "        ablation_x, ablation_y = ablation_input[0:2]\n",
    "        # ll_node = self.sample_ll_node() \n",
    "        _, cache = self.ll_model.run_with_cache(ablation_x)\n",
    "        self.ll_cache = cache\n",
    "        label_idx = self.get_label_idxs()\n",
    "        base_y = base_y[label_idx.as_index].to(self.ll_model.cfg.device)\n",
    "        if self.hl_model.is_categorical:\n",
    "            if len(base_y.shape) == 2:\n",
    "                base_y = t.argmax(base_y, dim=-1)\n",
    "        accuracies = []\n",
    "        for node in self.nodes_not_in_circuit:\n",
    "            out = self.ll_model.run_with_hooks(\n",
    "                base_x, fwd_hooks=[(node.name, self.make_ll_ablation_hook(node))]\n",
    "            )\n",
    "            ll_output = out[label_idx.as_index]\n",
    "            if self.hl_model.is_categorical:\n",
    "                top1 = t.argmax(ll_output, dim=-1)\n",
    "                accuracy = (top1 == base_y).float().mean().item()\n",
    "            else:\n",
    "                accuracy = ((ll_output - base_y).abs() < self.training_args[\"atol\"]).float().mean().item()\n",
    "            accuracies.append(accuracy)\n",
    "        strict_accuracy = np.mean(accuracies)\n",
    "\n",
    "        return {\n",
    "            \"val/iit_loss\": loss.item(),\n",
    "            \"val/IIA\": IIA,\n",
    "            \"val/accuracy\": (\n",
    "                per_token_accuracy.mean().item()\n",
    "                if self.next_token\n",
    "                else per_token_accuracy[-1]\n",
    "            ),\n",
    "            \"val/strict_accuracy\": strict_accuracy,\n",
    "            \"val/per_token_accuracy\": per_token_accuracy,\n",
    "        }\n",
    "\n",
    "model_pair = ParensModelPair(\n",
    "    ll_model=ll_model,\n",
    "    hl_model=hl_model,\n",
    "    corr=corr,\n",
    "    training_args=asdict(train_args),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model pair\n",
      "training_args={'next_token': False, 'non_ioi_thresh': 0.65, 'use_per_token_check': False, 'batch_size': 128, 'lr': 0.001, 'num_workers': 0, 'early_stop': True, 'lr_scheduler': None, 'scheduler_val_metric': ['val/accuracy', 'val/IIA'], 'scheduler_mode': 'max', 'clip_grad_norm': 1.0, 'seed': 0, 'detach_while_caching': True, 'atol': 0.05, 'use_single_loss': True, 'iit_weight': 1.0, 'behavior_weight': 1.0, 'strict_weight': 1.0, 'output_dir': './results', 'include_mlp': True, 'use_wandb': False, 'num_samples': 120000, 'device': 'cpu', 'weights': '100_100_40', 'mean': True, 'load_from_wandb': False, 'epochs': 100, 'iit': 1.0, 'b': 1.0, 's': 1.0, 'save_to_wandb': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:17<00:00,  7.03it/s]\n",
      "  0%|          | 0/100 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining model pair\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_pair\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/far_cluster/iit/iit/model_pairs/base_model_pair.py:263\u001b[0m, in \u001b[0;36mBaseModelPair.train\u001b[0;34m(self, train_set, test_set, epochs, use_wandb, wandb_name_suffix)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m    262\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_train_epoch(train_loader, loss_fn, optimizer)\n\u001b[0;32m--> 263\u001b[0m     test_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_eval_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler_cls:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_scheduler(lr_scheduler, test_metrics)\n",
      "File \u001b[0;32m~/far_cluster/iit/iit/model_pairs/base_model_pair.py:321\u001b[0m, in \u001b[0;36mBaseModelPair._run_eval_epoch\u001b[0;34m(self, loader, loss_fn)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m t\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (base_input, ablation_input) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[1;32m    320\u001b[0m         test_metrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 321\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_eval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mablation_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m         )\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_metrics\n",
      "File \u001b[0;32m~/far_cluster/iit/iit/model_pairs/ioi_model_pair.py:110\u001b[0m, in \u001b[0;36mIOI_ModelPair.run_eval_step\u001b[0;34m(self, base_input, ablation_input, loss_fn)\u001b[0m\n\u001b[1;32m    108\u001b[0m     hl_output \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39margmax(hl_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    109\u001b[0m top1 \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39margmax(ll_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (\u001b[43mtop1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhl_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    111\u001b[0m IIA \u001b[38;5;241m=\u001b[39m accuracy\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# compute behavioral accuracy\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "print(\"training model pair\")\n",
    "model_pair.train(train_set, test_set, epochs=train_args.epochs, use_wandb=train_args.use_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
