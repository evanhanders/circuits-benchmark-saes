{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformer_lens as tl\n",
    "\n",
    "from iit.model_pairs.strict_iit_model_pair import StrictIITModelPair\n",
    "from iit.utils.iit_dataset import train_test_split\n",
    "from iit.utils.iit_dataset import IITDataset\n",
    "\n",
    "import circuits_benchmark.benchmark.cases.case_3 as case3\n",
    "import circuits_benchmark.benchmark.cases.case_4 as case4\n",
    "from circuits_benchmark.utils.ll_model_loader.ll_model_loader_factory import get_ll_model_loader\n",
    "from circuits_benchmark.utils.iit.iit_hl_model import IITHLModel\n",
    "from circuits_benchmark.transformers.hooked_tracr_transformer import HookedTracrTransformer\n",
    "from circuits_benchmark.benchmark.vocabs import TRACR_BOS, TRACR_PAD\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cases from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "#load cases\n",
    "cases = [case3.Case3(), case4.Case4()]\n",
    "# cases = [case3.Case3(),]\n",
    "# cases = [case4.Case4()]\n",
    "corrs = []\n",
    "ll_models = []\n",
    "hl_models = []\n",
    "model_pairs = []\n",
    "for case in cases:\n",
    "    ll_model_loader = get_ll_model_loader(case, interp_bench=True)\n",
    "    corr, ll_model = ll_model_loader.load_ll_model_and_correspondence(device=device)\n",
    "    hl_model = case.get_hl_model()\n",
    "\n",
    "    if isinstance(hl_model, HookedTracrTransformer):\n",
    "        hl_model = IITHLModel(hl_model, eval_mode=True)\n",
    "\n",
    "    model_pair = case.build_model_pair(ll_model=ll_model, hl_model=hl_model)\n",
    "\n",
    "    corrs.append(corr)\n",
    "    ll_models.append(ll_model)\n",
    "    hl_models.append(hl_model)\n",
    "    model_pairs.append(model_pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate mixed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 10\n"
     ]
    }
   ],
   "source": [
    "#find overall min and max sequence length (we'll have to pad some)\n",
    "min_seq_len = 100000\n",
    "max_seq_len = 0\n",
    "\n",
    "for case in cases:\n",
    "    if case.get_min_seq_len() < min_seq_len:\n",
    "        min_seq_len = case.get_min_seq_len()\n",
    "    if case.get_max_seq_len() > max_seq_len:\n",
    "        max_seq_len = case.get_max_seq_len()\n",
    "print(min_seq_len, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# find max vocab size (some tasks will just use a subset of vocab size)\n",
    "vocab_size = 0\n",
    "for case in cases:\n",
    "    if len(case.get_vocab()) > vocab_size:\n",
    "        vocab_size = len(case.get_vocab())\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[320, 10000]\n"
     ]
    }
   ],
   "source": [
    "max_case_samples = 10_000\n",
    "case_samples = []\n",
    "for case in cases:\n",
    "    num_samples = min(max_case_samples, case.get_total_data_len())\n",
    "    case_samples.append(num_samples)\n",
    "print(case_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n",
      "WARNING:absl:Creating a SequenceMap with both inputs being the same SOp is discouraged. You should use a Map instead.\n"
     ]
    }
   ],
   "source": [
    "# generate clean and corrupted datasets\n",
    "clean_datasets = []\n",
    "corrupted_datasets = []\n",
    "masks = []\n",
    "for task_id, case, samples, hl_model in zip(range(len(cases)), cases, case_samples, hl_models):\n",
    "    dataset = case.get_clean_data(max_samples=samples)\n",
    "    encoder = hl_model.tracr_input_encoder    \n",
    "    # print(encoder.encoding_map)\n",
    "    def encode(tok):\n",
    "        return encoder.encoding_map[tok]\n",
    "\n",
    "\n",
    "    #Input\n",
    "    #put task_id after BOS token and pads after EOS.\n",
    "    inputs = dataset.inputs\n",
    "    str_tokens = [encoder.decode(inputs[i].tolist()) for i in range(inputs.shape[0])]\n",
    "    str_task_id = encoder.decode([task_id])\n",
    "    pads = [TRACR_PAD] * (max_seq_len - case.get_max_seq_len())\n",
    "    str_tokens = [str_task_id + [TRACR_BOS] + tokens[1:] + pads for tokens in str_tokens]\n",
    "    inputs = torch.tensor([list(map(encode, tokens)) for tokens in str_tokens])\n",
    "\n",
    "    #Target\n",
    "    #add 0 to beginning of seq and a bunch of 0s to end.\n",
    "    target = dataset.targets\n",
    "    label = torch.zeros((target.shape[0], 1, target.shape[2]), dtype=target.dtype)\n",
    "    pads = torch.zeros((target.shape[0], max_seq_len - case.get_max_seq_len(), target.shape[2]), dtype=target.dtype)\n",
    "    target = torch.cat((label, target, pads), dim=1)\n",
    "    dataset.inputs = inputs\n",
    "    dataset.targets = target\n",
    "    # print(clean_dataset.inputs.shape, dataset.inputs.shape)\n",
    "    # print(clean_dataset.targets.shape, dataset.targets.shape)\n",
    "    clean_datasets.append(dataset)\n",
    "\n",
    "    # if case.get_max_seq_len() < max_seq_len:\n",
    "    #     # pad sequences\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9920, 11])\n"
     ]
    }
   ],
   "source": [
    "#make all datasets ~the same length by duplicating shorter datasets\n",
    "max_length = max([clean_dataset.inputs.shape[0] for clean_dataset in clean_datasets])\n",
    "\n",
    "for dset_list in [clean_datasets,]:\n",
    "    for dataset in dset_list:\n",
    "        if dataset.inputs.shape[0] < max_length:\n",
    "            num_dups = max_length // dataset.inputs.shape[0]\n",
    "            dataset.inputs = dataset.inputs.repeat(num_dups, 1)\n",
    "            dataset.targets = dataset.targets.repeat(num_dups, 1, 1)\n",
    "print(clean_datasets[0].inputs.shape,)# clean_datasets[1].inputs.shape)\n",
    "# print(corrupted_datasets[0].inputs.shape)#, corrupted_datasets[1].inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 2,  ..., 1, 1, 1],\n",
      "        [0, 0, 4,  ..., 1, 1, 1],\n",
      "        [0, 0, 5,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 5,  ..., 1, 1, 1],\n",
      "        [0, 0, 2,  ..., 1, 1, 1],\n",
      "        [0, 0, 3,  ..., 1, 1, 1]])\n",
      "torch.Size([19920, 11])\n",
      "torch.Size([19920, 11, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/qny1f95136l2lpppg_d78n6c0000gq/T/ipykernel_12252/284937192.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(data).to(int)\n",
      "/var/folders/ry/qny1f95136l2lpppg_d78n6c0000gq/T/ipykernel_12252/284937192.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.targets = torch.tensor(targets)\n"
     ]
    }
   ],
   "source": [
    "#smush datasets together into one big dataset, then shuffle\n",
    "datasets = []\n",
    "for dset_list in [clean_datasets,]:\n",
    "    print(dset_list[0].inputs)\n",
    "    inputs = torch.cat([dset.inputs for dset in dset_list], dim=0)\n",
    "    targets = torch.cat([dset.targets for dset in dset_list], dim=0)\n",
    "\n",
    "    # shuffle dataset contents, keeping inputs and targets in sync\n",
    "    indices = torch.randperm(inputs.shape[0])\n",
    "    inputs = inputs[indices]\n",
    "    targets = targets[indices]\n",
    "\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, data, targets):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                data (list or numpy array): List or array of input data.\n",
    "                targets (list or numpy array): List or array of target data.\n",
    "            \"\"\"\n",
    "            self.data = torch.tensor(data).to(int)\n",
    "            self.targets = torch.tensor(targets)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                idx (int): Index\n",
    "            Returns:\n",
    "                tuple: (input tensor, target tensor)\n",
    "            \"\"\"\n",
    "            return self.data[idx], self.targets[idx]\n",
    "\n",
    "    decorated_dset = CustomDataset(\n",
    "        data = inputs,\n",
    "        targets = targets,\n",
    "    )\n",
    "    print(decorated_dset.data.shape)\n",
    "    print(decorated_dset.targets.shape)\n",
    "\n",
    "    train_dataset, test_dataset = train_test_split(\n",
    "        decorated_dset, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_set = IITDataset(train_dataset, train_dataset, seed=0)\n",
    "    test_set = IITDataset(test_dataset, test_dataset, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test making a loader works\n",
    "loader = train_set.make_loader(batch_size=32, num_workers=0)\n",
    "for b, s in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Polysemantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 4, 3, 3, 3, 1, 1, 1, 1, 1])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "tensor([[[False],\n",
      "         [ True],\n",
      "         [ True],\n",
      "         [ True],\n",
      "         [ True],\n",
      "         [ True],\n",
      "         [False],\n",
      "         [False],\n",
      "         [False],\n",
      "         [False],\n",
      "         [False]]])\n"
     ]
    }
   ],
   "source": [
    "from poly_hl_model import PolyHLModel\n",
    "\n",
    "model = PolyHLModel(hl_models, corrs, cases)\n",
    "\n",
    "input, target = train_dataset[4]\n",
    "output, cache = model.run_with_cache(input[None,:])\n",
    "print(input)\n",
    "# print(hl_models[0](input[1:6]))\n",
    "print(output[:1])\n",
    "print(target)\n",
    "print(model.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import LL model and build model pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "D_MODEL = max(model.tracr_d_models)\n",
    "N_CTX = model.n_ctx\n",
    "N_LAYERS = model.n_layers\n",
    "N_HEADS = model.n_heads\n",
    "D_VOCAB = max([hl_model.cfg.d_vocab for hl_model in model.hl_models])\n",
    "\n",
    "#Want to specify this somewhere central, e.g., iit.tasks.ioi but iit.tasks.parens\n",
    "ll_cfg = tl.HookedTransformerConfig(\n",
    "        n_layers = N_LAYERS,\n",
    "        d_model = D_MODEL,\n",
    "        n_ctx = N_CTX,\n",
    "        d_head = D_MODEL // N_HEADS,\n",
    "        d_vocab = D_VOCAB,\n",
    "        act_fn = \"relu\",\n",
    ")\n",
    "\n",
    "class SingleOutputHookedTransformer(tl.HookedTransformer):\n",
    "    def forward(self, x):\n",
    "        output = super().forward(x)\n",
    "        return output[:,:,:1]\n",
    "\n",
    "ll_model = SingleOutputHookedTransformer(ll_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "training_args = {\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"use_single_loss\": True,\n",
    "    \"behavior_weight\": 1.,\n",
    "    \"iit_weight\": 1.,\n",
    "    \"strict_weight\": 0.4,\n",
    "    \"clip_grad_norm\": 1.0,\n",
    "    \"early_stop\" : True,\n",
    "    \"lr_scheduler\": torch.optim.lr_scheduler.LinearLR,\n",
    "    \"scheduler_kwargs\": dict(start_factor=1, end_factor=0, total_iters=n_epochs),\n",
    "    \"scheduler_val_metric\": [\"val/accuracy\", \"val/IIA\"], #for ReduceLRonPlateau\n",
    "    \"scheduler_mode\": \"max\", #for ReduceLRonPlateau\n",
    "}\n",
    "model_pair = StrictIITModelPair(hl_model=model, ll_model=ll_model, corr=model.corr, training_args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{input_hook: {LLNode(name='blocks.0.hook_resid_pre', index=[:], subspace=None)}, task_hook: {LLNode(name='blocks.0.attn.hook_z', index=[:, :, 0, :], subspace=None)}, mlp_hooks.0: {LLNode(name='blocks.0.mlp.hook_post', index=[:], subspace=None)}, attn_hooks.1.0: {LLNode(name='blocks.1.attn.hook_z', index=[:, :, 0, :], subspace=None)}, mlp_hooks.1: {LLNode(name='blocks.1.mlp.hook_post', index=[:], subspace=None)}}\n",
      "[LLNode(name='blocks.0.attn.hook_z', index=[:, :, 1, :], subspace=None), LLNode(name='blocks.1.attn.hook_z', index=[:, :, 1, :], subspace=None)]\n"
     ]
    }
   ],
   "source": [
    "print(model.corr)\n",
    "print(model_pair.nodes_not_in_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args={'batch_size': 256, 'lr': 0.001, 'num_workers': 0, 'early_stop': True, 'lr_scheduler': <class 'torch.optim.lr_scheduler.LinearLR'>, 'scheduler_val_metric': ['val/accuracy', 'val/IIA'], 'scheduler_mode': 'max', 'scheduler_kwargs': {'start_factor': 1, 'end_factor': 0, 'total_iters': 1000}, 'clip_grad_norm': 1.0, 'seed': 0, 'detach_while_caching': True, 'atol': 0.05, 'use_single_loss': False, 'iit_weight': 1.0, 'behavior_weight': 1.0, 'strict_weight': 0.4}\n",
      "Epoch 1: lr: 9.99e-04, train/iit_loss: 0.0453, train/behavior_loss: 0.0170, train/strict_loss: 0.0056, val/iit_loss: 0.0218, val/IIA: 50.92%, val/accuracy: 68.98%, val/strict_accuracy: 43.54%\n",
      "Epoch 2: lr: 9.98e-04, train/iit_loss: 0.0206, train/behavior_loss: 0.0030, train/strict_loss: 0.0031, val/iit_loss: 0.0227, val/IIA: 48.09%, val/accuracy: 81.20%, val/strict_accuracy: 48.34%\n",
      "Epoch 3: lr: 9.97e-04, train/iit_loss: 0.0149, train/behavior_loss: 0.0019, train/strict_loss: 0.0025, val/iit_loss: 0.0127, val/IIA: 62.61%, val/accuracy: 87.97%, val/strict_accuracy: 51.27%\n",
      "Epoch 4: lr: 9.96e-04, train/iit_loss: 0.0130, train/behavior_loss: 0.0016, train/strict_loss: 0.0025, val/iit_loss: 0.0116, val/IIA: 60.58%, val/accuracy: 85.59%, val/strict_accuracy: 49.61%\n",
      "Epoch 5: lr: 9.95e-04, train/iit_loss: 0.0113, train/behavior_loss: 0.0011, train/strict_loss: 0.0026, val/iit_loss: 0.0087, val/IIA: 68.98%, val/accuracy: 91.32%, val/strict_accuracy: 56.47%\n",
      "Epoch 6: lr: 9.94e-04, train/iit_loss: 0.0126, train/behavior_loss: 0.0012, train/strict_loss: 0.0024, val/iit_loss: 0.0115, val/IIA: 57.22%, val/accuracy: 80.69%, val/strict_accuracy: 46.12%\n",
      "Epoch 7: lr: 9.93e-04, train/iit_loss: 0.0082, train/behavior_loss: 0.0009, train/strict_loss: 0.0022, val/iit_loss: 0.0110, val/IIA: 57.02%, val/accuracy: 90.54%, val/strict_accuracy: 54.94%\n",
      "Epoch 8: lr: 9.92e-04, train/iit_loss: 0.0082, train/behavior_loss: 0.0010, train/strict_loss: 0.0020, val/iit_loss: 0.0085, val/IIA: 71.90%, val/accuracy: 96.46%, val/strict_accuracy: 57.65%\n",
      "Epoch 9: lr: 9.91e-04, train/iit_loss: 0.0097, train/behavior_loss: 0.0010, train/strict_loss: 0.0020, val/iit_loss: 0.0086, val/IIA: 65.84%, val/accuracy: 93.06%, val/strict_accuracy: 57.69%\n",
      "Epoch 10: lr: 9.90e-04, train/iit_loss: 0.0102, train/behavior_loss: 0.0010, train/strict_loss: 0.0019, val/iit_loss: 0.0086, val/IIA: 65.85%, val/accuracy: 94.22%, val/strict_accuracy: 53.71%\n",
      "Epoch 11: lr: 9.89e-04, train/iit_loss: 0.0086, train/behavior_loss: 0.0010, train/strict_loss: 0.0017, val/iit_loss: 0.0064, val/IIA: 73.29%, val/accuracy: 96.86%, val/strict_accuracy: 58.50%\n",
      "Epoch 12: lr: 9.88e-04, train/iit_loss: 0.0075, train/behavior_loss: 0.0009, train/strict_loss: 0.0016, val/iit_loss: 0.0062, val/IIA: 69.96%, val/accuracy: 94.92%, val/strict_accuracy: 58.79%\n",
      "Epoch 13: lr: 9.87e-04, train/iit_loss: 0.0078, train/behavior_loss: 0.0009, train/strict_loss: 0.0016, val/iit_loss: 0.0058, val/IIA: 71.47%, val/accuracy: 93.15%, val/strict_accuracy: 59.08%\n",
      "Epoch 14: lr: 9.86e-04, train/iit_loss: 0.0091, train/behavior_loss: 0.0013, train/strict_loss: 0.0017, val/iit_loss: 0.0033, val/IIA: 78.74%, val/accuracy: 97.48%, val/strict_accuracy: 61.67%\n",
      "Epoch 15: lr: 9.85e-04, train/iit_loss: 0.0061, train/behavior_loss: 0.0008, train/strict_loss: 0.0016, val/iit_loss: 0.0073, val/IIA: 68.08%, val/accuracy: 97.10%, val/strict_accuracy: 62.48%\n",
      "Epoch 16: lr: 9.84e-04, train/iit_loss: 0.0074, train/behavior_loss: 0.0009, train/strict_loss: 0.0016, val/iit_loss: 0.0068, val/IIA: 66.33%, val/accuracy: 95.40%, val/strict_accuracy: 61.92%\n",
      "Epoch 17: lr: 9.83e-04, train/iit_loss: 0.0076, train/behavior_loss: 0.0013, train/strict_loss: 0.0015, val/iit_loss: 0.0041, val/IIA: 71.37%, val/accuracy: 97.73%, val/strict_accuracy: 62.69%\n",
      "Epoch 18: lr: 9.82e-04, train/iit_loss: 0.0053, train/behavior_loss: 0.0007, train/strict_loss: 0.0015, val/iit_loss: 0.0054, val/IIA: 69.80%, val/accuracy: 89.35%, val/strict_accuracy: 58.54%\n",
      "Epoch 19: lr: 9.81e-04, train/iit_loss: 0.0064, train/behavior_loss: 0.0011, train/strict_loss: 0.0015, val/iit_loss: 0.0063, val/IIA: 67.77%, val/accuracy: 84.30%, val/strict_accuracy: 54.63%\n",
      "Epoch 20: lr: 9.80e-04, train/iit_loss: 0.0051, train/behavior_loss: 0.0006, train/strict_loss: 0.0011, val/iit_loss: 0.0088, val/IIA: 56.61%, val/accuracy: 77.14%, val/strict_accuracy: 48.40%\n",
      "Epoch 21: lr: 9.79e-04, train/iit_loss: 0.0075, train/behavior_loss: 0.0011, train/strict_loss: 0.0014, val/iit_loss: 0.0061, val/IIA: 66.38%, val/accuracy: 92.78%, val/strict_accuracy: 61.29%\n",
      "Epoch 22: lr: 9.78e-04, train/iit_loss: 0.0072, train/behavior_loss: 0.0011, train/strict_loss: 0.0016, val/iit_loss: 0.0071, val/IIA: 66.61%, val/accuracy: 92.25%, val/strict_accuracy: 61.29%\n",
      "Epoch 23: lr: 9.77e-04, train/iit_loss: 0.0053, train/behavior_loss: 0.0006, train/strict_loss: 0.0013, val/iit_loss: 0.0043, val/IIA: 74.48%, val/accuracy: 97.82%, val/strict_accuracy: 65.56%\n",
      "Epoch 24: lr: 9.76e-04, train/iit_loss: 0.0051, train/behavior_loss: 0.0007, train/strict_loss: 0.0012, val/iit_loss: 0.0044, val/IIA: 72.05%, val/accuracy: 91.33%, val/strict_accuracy: 63.55%\n",
      "Epoch 25: lr: 9.75e-04, train/iit_loss: 0.0058, train/behavior_loss: 0.0009, train/strict_loss: 0.0011, val/iit_loss: 0.0050, val/IIA: 75.24%, val/accuracy: 98.04%, val/strict_accuracy: 64.73%\n",
      "Epoch 26: lr: 9.74e-04, train/iit_loss: 0.0057, train/behavior_loss: 0.0007, train/strict_loss: 0.0012, val/iit_loss: 0.0065, val/IIA: 73.44%, val/accuracy: 99.10%, val/strict_accuracy: 67.14%\n",
      "Epoch 27: lr: 9.73e-04, train/iit_loss: 0.0053, train/behavior_loss: 0.0008, train/strict_loss: 0.0013, val/iit_loss: 0.0042, val/IIA: 74.87%, val/accuracy: 94.46%, val/strict_accuracy: 60.78%\n",
      "Epoch 28: lr: 9.72e-04, train/iit_loss: 0.0046, train/behavior_loss: 0.0005, train/strict_loss: 0.0011, val/iit_loss: 0.0034, val/IIA: 75.14%, val/accuracy: 89.62%, val/strict_accuracy: 64.18%\n",
      "Epoch 29: lr: 9.71e-04, train/iit_loss: 0.0056, train/behavior_loss: 0.0010, train/strict_loss: 0.0012, val/iit_loss: 0.0046, val/IIA: 75.02%, val/accuracy: 97.01%, val/strict_accuracy: 66.78%\n",
      "Epoch 30: lr: 9.70e-04, train/iit_loss: 0.0048, train/behavior_loss: 0.0006, train/strict_loss: 0.0010, val/iit_loss: 0.0044, val/IIA: 75.24%, val/accuracy: 96.24%, val/strict_accuracy: 65.71%\n",
      "Epoch 31: lr: 9.69e-04, train/iit_loss: 0.0049, train/behavior_loss: 0.0007, train/strict_loss: 0.0010, val/iit_loss: 0.0039, val/IIA: 78.97%, val/accuracy: 97.82%, val/strict_accuracy: 68.03%\n",
      "Epoch 32: lr: 9.68e-04, train/iit_loss: 0.0041, train/behavior_loss: 0.0006, train/strict_loss: 0.0010, val/iit_loss: 0.0040, val/IIA: 75.13%, val/accuracy: 98.47%, val/strict_accuracy: 68.63%\n",
      "Epoch 33: lr: 9.67e-04, train/iit_loss: 0.0042, train/behavior_loss: 0.0006, train/strict_loss: 0.0009, val/iit_loss: 0.0037, val/IIA: 69.76%, val/accuracy: 85.50%, val/strict_accuracy: 62.17%\n",
      "Epoch 34: lr: 9.66e-04, train/iit_loss: 0.0040, train/behavior_loss: 0.0006, train/strict_loss: 0.0009, val/iit_loss: 0.0028, val/IIA: 81.48%, val/accuracy: 99.19%, val/strict_accuracy: 73.17%\n",
      "Epoch 35: lr: 9.65e-04, train/iit_loss: 0.0036, train/behavior_loss: 0.0005, train/strict_loss: 0.0007, val/iit_loss: 0.0029, val/IIA: 79.21%, val/accuracy: 99.09%, val/strict_accuracy: 71.35%\n",
      "Epoch 36: lr: 9.64e-04, train/iit_loss: 0.0045, train/behavior_loss: 0.0011, train/strict_loss: 0.0009, val/iit_loss: 0.0067, val/IIA: 68.95%, val/accuracy: 88.61%, val/strict_accuracy: 61.67%\n",
      "Epoch 37: lr: 9.63e-04, train/iit_loss: 0.0046, train/behavior_loss: 0.0008, train/strict_loss: 0.0015, val/iit_loss: 0.0042, val/IIA: 75.35%, val/accuracy: 97.40%, val/strict_accuracy: 69.79%\n",
      "Epoch 38: lr: 9.62e-04, train/iit_loss: 0.0035, train/behavior_loss: 0.0005, train/strict_loss: 0.0009, val/iit_loss: 0.0025, val/IIA: 84.05%, val/accuracy: 98.97%, val/strict_accuracy: 75.34%\n",
      "Epoch 39: lr: 9.61e-04, train/iit_loss: 0.0042, train/behavior_loss: 0.0006, train/strict_loss: 0.0011, val/iit_loss: 0.0050, val/IIA: 73.76%, val/accuracy: 97.46%, val/strict_accuracy: 72.05%\n",
      "Epoch 40: lr: 9.60e-04, train/iit_loss: 0.0042, train/behavior_loss: 0.0007, train/strict_loss: 0.0011, val/iit_loss: 0.0046, val/IIA: 76.90%, val/accuracy: 99.56%, val/strict_accuracy: 72.83%\n",
      "Epoch 41: lr: 9.59e-04, train/iit_loss: 0.0036, train/behavior_loss: 0.0006, train/strict_loss: 0.0009, val/iit_loss: 0.0044, val/IIA: 78.45%, val/accuracy: 99.21%, val/strict_accuracy: 74.60%\n",
      "Epoch 42: lr: 9.58e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0004, train/strict_loss: 0.0008, val/iit_loss: 0.0035, val/IIA: 82.17%, val/accuracy: 99.21%, val/strict_accuracy: 72.74%\n",
      "Epoch 43: lr: 9.57e-04, train/iit_loss: 0.0042, train/behavior_loss: 0.0008, train/strict_loss: 0.0010, val/iit_loss: 0.0028, val/IIA: 83.75%, val/accuracy: 99.66%, val/strict_accuracy: 75.19%\n",
      "Epoch 44: lr: 9.56e-04, train/iit_loss: 0.0034, train/behavior_loss: 0.0003, train/strict_loss: 0.0008, val/iit_loss: 0.0045, val/IIA: 76.50%, val/accuracy: 99.12%, val/strict_accuracy: 70.94%\n",
      "Epoch 45: lr: 9.55e-04, train/iit_loss: 0.0056, train/behavior_loss: 0.0009, train/strict_loss: 0.0013, val/iit_loss: 0.0047, val/IIA: 76.00%, val/accuracy: 87.46%, val/strict_accuracy: 65.08%\n",
      "Epoch 46: lr: 9.54e-04, train/iit_loss: 0.0041, train/behavior_loss: 0.0005, train/strict_loss: 0.0009, val/iit_loss: 0.0035, val/IIA: 75.79%, val/accuracy: 87.74%, val/strict_accuracy: 67.76%\n",
      "Epoch 47: lr: 9.53e-04, train/iit_loss: 0.0030, train/behavior_loss: 0.0004, train/strict_loss: 0.0008, val/iit_loss: 0.0022, val/IIA: 85.46%, val/accuracy: 99.61%, val/strict_accuracy: 75.11%\n",
      "Epoch 48: lr: 9.52e-04, train/iit_loss: 0.0039, train/behavior_loss: 0.0004, train/strict_loss: 0.0008, val/iit_loss: 0.0033, val/IIA: 80.93%, val/accuracy: 99.47%, val/strict_accuracy: 73.82%\n",
      "Epoch 49: lr: 9.51e-04, train/iit_loss: 0.0032, train/behavior_loss: 0.0004, train/strict_loss: 0.0007, val/iit_loss: 0.0038, val/IIA: 77.57%, val/accuracy: 97.71%, val/strict_accuracy: 75.09%\n",
      "Epoch 50: lr: 9.50e-04, train/iit_loss: 0.0065, train/behavior_loss: 0.0016, train/strict_loss: 0.0023, val/iit_loss: 0.0043, val/IIA: 74.55%, val/accuracy: 93.58%, val/strict_accuracy: 67.97%\n",
      "Epoch 51: lr: 9.49e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0005, train/strict_loss: 0.0011, val/iit_loss: 0.0040, val/IIA: 77.57%, val/accuracy: 99.13%, val/strict_accuracy: 74.23%\n",
      "Epoch 52: lr: 9.48e-04, train/iit_loss: 0.0040, train/behavior_loss: 0.0004, train/strict_loss: 0.0009, val/iit_loss: 0.0042, val/IIA: 72.11%, val/accuracy: 88.20%, val/strict_accuracy: 64.59%\n",
      "Epoch 53: lr: 9.47e-04, train/iit_loss: 0.0033, train/behavior_loss: 0.0004, train/strict_loss: 0.0007, val/iit_loss: 0.0040, val/IIA: 73.12%, val/accuracy: 85.28%, val/strict_accuracy: 66.52%\n",
      "Epoch 54: lr: 9.46e-04, train/iit_loss: 0.0039, train/behavior_loss: 0.0007, train/strict_loss: 0.0009, val/iit_loss: 0.0041, val/IIA: 78.67%, val/accuracy: 99.84%, val/strict_accuracy: 75.84%\n",
      "Epoch 55: lr: 9.45e-04, train/iit_loss: 0.0030, train/behavior_loss: 0.0005, train/strict_loss: 0.0007, val/iit_loss: 0.0060, val/IIA: 71.47%, val/accuracy: 87.88%, val/strict_accuracy: 63.16%\n",
      "Epoch 56: lr: 9.44e-04, train/iit_loss: 0.0054, train/behavior_loss: 0.0010, train/strict_loss: 0.0012, val/iit_loss: 0.0027, val/IIA: 81.86%, val/accuracy: 98.43%, val/strict_accuracy: 73.35%\n",
      "Epoch 57: lr: 9.43e-04, train/iit_loss: 0.0035, train/behavior_loss: 0.0003, train/strict_loss: 0.0008, val/iit_loss: 0.0039, val/IIA: 80.88%, val/accuracy: 99.62%, val/strict_accuracy: 75.81%\n",
      "Epoch 58: lr: 9.42e-04, train/iit_loss: 0.0036, train/behavior_loss: 0.0003, train/strict_loss: 0.0007, val/iit_loss: 0.0018, val/IIA: 86.07%, val/accuracy: 97.99%, val/strict_accuracy: 74.60%\n",
      "Epoch 59: lr: 9.41e-04, train/iit_loss: 0.0033, train/behavior_loss: 0.0005, train/strict_loss: 0.0006, val/iit_loss: 0.0062, val/IIA: 72.88%, val/accuracy: 89.19%, val/strict_accuracy: 62.61%\n",
      "Epoch 60: lr: 9.40e-04, train/iit_loss: 0.0066, train/behavior_loss: 0.0011, train/strict_loss: 0.0026, val/iit_loss: 0.0066, val/IIA: 63.37%, val/accuracy: 89.13%, val/strict_accuracy: 64.06%\n",
      "Epoch 61: lr: 9.39e-04, train/iit_loss: 0.0031, train/behavior_loss: 0.0005, train/strict_loss: 0.0012, val/iit_loss: 0.0027, val/IIA: 81.03%, val/accuracy: 98.29%, val/strict_accuracy: 74.09%\n",
      "Epoch 62: lr: 9.38e-04, train/iit_loss: 0.0027, train/behavior_loss: 0.0001, train/strict_loss: 0.0006, val/iit_loss: 0.0040, val/IIA: 80.52%, val/accuracy: 99.89%, val/strict_accuracy: 78.20%\n",
      "Epoch 63: lr: 9.37e-04, train/iit_loss: 0.0027, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0039, val/IIA: 79.53%, val/accuracy: 99.70%, val/strict_accuracy: 78.04%\n",
      "Epoch 64: lr: 9.36e-04, train/iit_loss: 0.0037, train/behavior_loss: 0.0005, train/strict_loss: 0.0007, val/iit_loss: 0.0032, val/IIA: 85.26%, val/accuracy: 99.21%, val/strict_accuracy: 75.89%\n",
      "Epoch 65: lr: 9.35e-04, train/iit_loss: 0.0035, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0033, val/IIA: 81.36%, val/accuracy: 99.50%, val/strict_accuracy: 76.21%\n",
      "Epoch 66: lr: 9.34e-04, train/iit_loss: 0.0048, train/behavior_loss: 0.0009, train/strict_loss: 0.0019, val/iit_loss: 0.0053, val/IIA: 75.22%, val/accuracy: 94.67%, val/strict_accuracy: 67.26%\n",
      "Epoch 67: lr: 9.33e-04, train/iit_loss: 0.0047, train/behavior_loss: 0.0005, train/strict_loss: 0.0018, val/iit_loss: 0.0042, val/IIA: 77.44%, val/accuracy: 99.39%, val/strict_accuracy: 74.61%\n",
      "Epoch 68: lr: 9.32e-04, train/iit_loss: 0.0034, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0029, val/IIA: 84.31%, val/accuracy: 99.51%, val/strict_accuracy: 77.28%\n",
      "Epoch 69: lr: 9.31e-04, train/iit_loss: 0.0029, train/behavior_loss: 0.0003, train/strict_loss: 0.0006, val/iit_loss: 0.0037, val/IIA: 75.40%, val/accuracy: 88.48%, val/strict_accuracy: 76.83%\n",
      "Epoch 70: lr: 9.30e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0003, train/strict_loss: 0.0006, val/iit_loss: 0.0032, val/IIA: 82.11%, val/accuracy: 99.36%, val/strict_accuracy: 75.95%\n",
      "Epoch 71: lr: 9.29e-04, train/iit_loss: 0.0029, train/behavior_loss: 0.0004, train/strict_loss: 0.0006, val/iit_loss: 0.0011, val/IIA: 90.76%, val/accuracy: 97.36%, val/strict_accuracy: 76.01%\n",
      "Epoch 72: lr: 9.28e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0004, train/strict_loss: 0.0009, val/iit_loss: 0.0038, val/IIA: 77.50%, val/accuracy: 98.33%, val/strict_accuracy: 72.29%\n",
      "Epoch 73: lr: 9.27e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0026, val/IIA: 83.07%, val/accuracy: 99.92%, val/strict_accuracy: 81.61%\n",
      "Epoch 74: lr: 9.26e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0036, val/IIA: 82.34%, val/accuracy: 99.56%, val/strict_accuracy: 80.68%\n",
      "Epoch 75: lr: 9.25e-04, train/iit_loss: 0.0065, train/behavior_loss: 0.0013, train/strict_loss: 0.0014, val/iit_loss: 0.0096, val/IIA: 41.93%, val/accuracy: 62.57%, val/strict_accuracy: 37.01%\n",
      "Epoch 76: lr: 9.24e-04, train/iit_loss: 0.0025, train/behavior_loss: 0.0004, train/strict_loss: 0.0010, val/iit_loss: 0.0035, val/IIA: 76.90%, val/accuracy: 93.14%, val/strict_accuracy: 71.95%\n",
      "Epoch 77: lr: 9.23e-04, train/iit_loss: 0.0025, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0027, val/IIA: 83.68%, val/accuracy: 98.66%, val/strict_accuracy: 76.81%\n",
      "Epoch 78: lr: 9.22e-04, train/iit_loss: 0.0045, train/behavior_loss: 0.0010, train/strict_loss: 0.0022, val/iit_loss: 0.0059, val/IIA: 78.03%, val/accuracy: 95.74%, val/strict_accuracy: 70.62%\n",
      "Epoch 79: lr: 9.21e-04, train/iit_loss: 0.0029, train/behavior_loss: 0.0004, train/strict_loss: 0.0012, val/iit_loss: 0.0030, val/IIA: 81.46%, val/accuracy: 98.18%, val/strict_accuracy: 77.84%\n",
      "Epoch 80: lr: 9.20e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0020, val/IIA: 87.50%, val/accuracy: 98.80%, val/strict_accuracy: 79.57%\n",
      "Epoch 81: lr: 9.19e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0003, train/strict_loss: 0.0007, val/iit_loss: 0.0029, val/IIA: 85.25%, val/accuracy: 99.00%, val/strict_accuracy: 80.52%\n",
      "Epoch 82: lr: 9.18e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0003, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 84.14%, val/accuracy: 99.76%, val/strict_accuracy: 81.41%\n",
      "Epoch 83: lr: 9.17e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0014, val/IIA: 90.66%, val/accuracy: 99.92%, val/strict_accuracy: 80.98%\n",
      "Epoch 84: lr: 9.16e-04, train/iit_loss: 0.0025, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0032, val/IIA: 80.26%, val/accuracy: 98.99%, val/strict_accuracy: 79.18%\n",
      "Epoch 85: lr: 9.15e-04, train/iit_loss: 0.0041, train/behavior_loss: 0.0005, train/strict_loss: 0.0006, val/iit_loss: 0.0030, val/IIA: 75.32%, val/accuracy: 83.72%, val/strict_accuracy: 64.17%\n",
      "Epoch 86: lr: 9.14e-04, train/iit_loss: 0.0045, train/behavior_loss: 0.0007, train/strict_loss: 0.0025, val/iit_loss: 0.0044, val/IIA: 75.30%, val/accuracy: 99.15%, val/strict_accuracy: 75.40%\n",
      "Epoch 87: lr: 9.13e-04, train/iit_loss: 0.0031, train/behavior_loss: 0.0003, train/strict_loss: 0.0006, val/iit_loss: 0.0029, val/IIA: 81.02%, val/accuracy: 92.91%, val/strict_accuracy: 76.83%\n",
      "Epoch 88: lr: 9.12e-04, train/iit_loss: 0.0030, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0022, val/IIA: 85.61%, val/accuracy: 99.88%, val/strict_accuracy: 80.11%\n",
      "Epoch 89: lr: 9.11e-04, train/iit_loss: 0.0022, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0019, val/IIA: 89.08%, val/accuracy: 99.57%, val/strict_accuracy: 81.65%\n",
      "Epoch 90: lr: 9.10e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0015, val/IIA: 89.75%, val/accuracy: 99.24%, val/strict_accuracy: 80.49%\n",
      "Epoch 91: lr: 9.09e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0011, val/IIA: 93.03%, val/accuracy: 99.92%, val/strict_accuracy: 82.25%\n",
      "Epoch 92: lr: 9.08e-04, train/iit_loss: 0.0033, train/behavior_loss: 0.0007, train/strict_loss: 0.0008, val/iit_loss: 0.0029, val/IIA: 85.11%, val/accuracy: 95.69%, val/strict_accuracy: 76.18%\n",
      "Epoch 93: lr: 9.07e-04, train/iit_loss: 0.0033, train/behavior_loss: 0.0006, train/strict_loss: 0.0014, val/iit_loss: 0.0021, val/IIA: 86.55%, val/accuracy: 99.79%, val/strict_accuracy: 80.20%\n",
      "Epoch 94: lr: 9.06e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0024, val/IIA: 86.09%, val/accuracy: 99.56%, val/strict_accuracy: 80.20%\n",
      "Epoch 95: lr: 9.05e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0019, val/IIA: 90.25%, val/accuracy: 99.76%, val/strict_accuracy: 81.28%\n",
      "Epoch 96: lr: 9.04e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0012, val/IIA: 90.97%, val/accuracy: 99.92%, val/strict_accuracy: 80.71%\n",
      "Epoch 97: lr: 9.03e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0022, val/IIA: 82.66%, val/accuracy: 90.34%, val/strict_accuracy: 74.64%\n",
      "Epoch 98: lr: 9.02e-04, train/iit_loss: 0.0053, train/behavior_loss: 0.0011, train/strict_loss: 0.0022, val/iit_loss: 0.0012, val/IIA: 91.88%, val/accuracy: 99.50%, val/strict_accuracy: 76.79%\n",
      "Epoch 99: lr: 9.01e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0009, val/IIA: 93.13%, val/accuracy: 99.93%, val/strict_accuracy: 80.20%\n",
      "Epoch 100: lr: 9.00e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0019, val/IIA: 88.56%, val/accuracy: 99.99%, val/strict_accuracy: 82.56%\n",
      "Epoch 101: lr: 8.99e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0024, val/IIA: 85.46%, val/accuracy: 99.88%, val/strict_accuracy: 82.01%\n",
      "Epoch 102: lr: 8.98e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0023, val/IIA: 85.54%, val/accuracy: 99.41%, val/strict_accuracy: 82.13%\n",
      "Epoch 103: lr: 8.97e-04, train/iit_loss: 0.0025, train/behavior_loss: 0.0003, train/strict_loss: 0.0007, val/iit_loss: 0.0035, val/IIA: 71.67%, val/accuracy: 79.67%, val/strict_accuracy: 71.36%\n",
      "Epoch 104: lr: 8.96e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0004, train/strict_loss: 0.0006, val/iit_loss: 0.0009, val/IIA: 93.61%, val/accuracy: 99.96%, val/strict_accuracy: 81.58%\n",
      "Epoch 105: lr: 8.95e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0003, train/strict_loss: 0.0004, val/iit_loss: 0.0017, val/IIA: 88.55%, val/accuracy: 99.69%, val/strict_accuracy: 81.15%\n",
      "Epoch 106: lr: 8.94e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0013, val/IIA: 91.55%, val/accuracy: 99.74%, val/strict_accuracy: 84.34%\n",
      "Epoch 107: lr: 8.93e-04, train/iit_loss: 0.0034, train/behavior_loss: 0.0009, train/strict_loss: 0.0016, val/iit_loss: 0.0040, val/IIA: 71.13%, val/accuracy: 78.65%, val/strict_accuracy: 58.48%\n",
      "Epoch 108: lr: 8.92e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0004, train/strict_loss: 0.0007, val/iit_loss: 0.0010, val/IIA: 93.82%, val/accuracy: 99.96%, val/strict_accuracy: 82.90%\n",
      "Epoch 109: lr: 8.91e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0030, val/IIA: 84.95%, val/accuracy: 99.84%, val/strict_accuracy: 81.13%\n",
      "Epoch 110: lr: 8.90e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0009, val/IIA: 92.66%, val/accuracy: 99.97%, val/strict_accuracy: 80.59%\n",
      "Epoch 111: lr: 8.89e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0004, val/iit_loss: 0.0016, val/IIA: 86.94%, val/accuracy: 97.00%, val/strict_accuracy: 76.38%\n",
      "Epoch 112: lr: 8.88e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0008, train/strict_loss: 0.0018, val/iit_loss: 0.0024, val/IIA: 85.93%, val/accuracy: 98.53%, val/strict_accuracy: 76.29%\n",
      "Epoch 113: lr: 8.87e-04, train/iit_loss: 0.0034, train/behavior_loss: 0.0005, train/strict_loss: 0.0015, val/iit_loss: 0.0030, val/IIA: 89.82%, val/accuracy: 98.92%, val/strict_accuracy: 77.09%\n",
      "Epoch 114: lr: 8.86e-04, train/iit_loss: 0.0049, train/behavior_loss: 0.0007, train/strict_loss: 0.0021, val/iit_loss: 0.0044, val/IIA: 78.49%, val/accuracy: 98.14%, val/strict_accuracy: 68.76%\n",
      "Epoch 115: lr: 8.85e-04, train/iit_loss: 0.0032, train/behavior_loss: 0.0004, train/strict_loss: 0.0021, val/iit_loss: 0.0022, val/IIA: 88.81%, val/accuracy: 99.81%, val/strict_accuracy: 77.37%\n",
      "Epoch 116: lr: 8.84e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0001, train/strict_loss: 0.0010, val/iit_loss: 0.0041, val/IIA: 83.33%, val/accuracy: 99.92%, val/strict_accuracy: 78.79%\n",
      "Epoch 117: lr: 8.83e-04, train/iit_loss: 0.0032, train/behavior_loss: 0.0004, train/strict_loss: 0.0012, val/iit_loss: 0.0009, val/IIA: 92.14%, val/accuracy: 99.65%, val/strict_accuracy: 81.71%\n",
      "Epoch 118: lr: 8.82e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0012, val/IIA: 91.61%, val/accuracy: 99.90%, val/strict_accuracy: 82.41%\n",
      "Epoch 119: lr: 8.81e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0019, val/IIA: 89.26%, val/accuracy: 99.90%, val/strict_accuracy: 81.57%\n",
      "Epoch 120: lr: 8.80e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0029, val/IIA: 83.53%, val/accuracy: 99.94%, val/strict_accuracy: 80.87%\n",
      "Epoch 121: lr: 8.79e-04, train/iit_loss: 0.0022, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0019, val/IIA: 89.75%, val/accuracy: 99.92%, val/strict_accuracy: 83.64%\n",
      "Epoch 122: lr: 8.78e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0026, val/IIA: 86.35%, val/accuracy: 99.93%, val/strict_accuracy: 83.32%\n",
      "Epoch 123: lr: 8.77e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0004, train/strict_loss: 0.0005, val/iit_loss: 0.0008, val/IIA: 94.36%, val/accuracy: 99.92%, val/strict_accuracy: 84.87%\n",
      "Epoch 124: lr: 8.76e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0019, val/IIA: 87.70%, val/accuracy: 99.87%, val/strict_accuracy: 80.93%\n",
      "Epoch 125: lr: 8.75e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 91.70%, val/accuracy: 99.97%, val/strict_accuracy: 86.82%\n",
      "Epoch 126: lr: 8.74e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 91.84%, val/accuracy: 99.99%, val/strict_accuracy: 85.74%\n",
      "Epoch 127: lr: 8.73e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0008, val/IIA: 92.01%, val/accuracy: 97.28%, val/strict_accuracy: 82.82%\n",
      "Epoch 128: lr: 8.72e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0009, val/IIA: 93.68%, val/accuracy: 99.92%, val/strict_accuracy: 85.65%\n",
      "Epoch 129: lr: 8.71e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0003, train/strict_loss: 0.0004, val/iit_loss: 0.0018, val/IIA: 85.96%, val/accuracy: 99.07%, val/strict_accuracy: 82.59%\n",
      "Epoch 130: lr: 8.70e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0005, val/IIA: 96.15%, val/accuracy: 99.90%, val/strict_accuracy: 80.71%\n",
      "Epoch 131: lr: 8.69e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0006, train/strict_loss: 0.0022, val/iit_loss: 0.0032, val/IIA: 81.95%, val/accuracy: 99.45%, val/strict_accuracy: 77.93%\n",
      "Epoch 132: lr: 8.68e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0003, train/strict_loss: 0.0009, val/iit_loss: 0.0026, val/IIA: 82.14%, val/accuracy: 98.83%, val/strict_accuracy: 76.23%\n",
      "Epoch 133: lr: 8.67e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0024, val/IIA: 86.87%, val/accuracy: 99.92%, val/strict_accuracy: 86.68%\n",
      "Epoch 134: lr: 8.66e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0019, val/IIA: 88.52%, val/accuracy: 99.96%, val/strict_accuracy: 85.73%\n",
      "Epoch 135: lr: 8.65e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0074, val/IIA: 66.48%, val/accuracy: 84.76%, val/strict_accuracy: 72.36%\n",
      "Epoch 136: lr: 8.64e-04, train/iit_loss: 0.0047, train/behavior_loss: 0.0008, train/strict_loss: 0.0017, val/iit_loss: 0.0025, val/IIA: 86.06%, val/accuracy: 99.61%, val/strict_accuracy: 81.09%\n",
      "Epoch 137: lr: 8.63e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0019, val/IIA: 86.77%, val/accuracy: 98.16%, val/strict_accuracy: 84.15%\n",
      "Epoch 138: lr: 8.62e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0004, val/IIA: 96.16%, val/accuracy: 99.95%, val/strict_accuracy: 86.77%\n",
      "Epoch 139: lr: 8.61e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0017, val/IIA: 89.56%, val/accuracy: 98.97%, val/strict_accuracy: 88.10%\n",
      "Epoch 140: lr: 8.60e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0018, val/IIA: 90.35%, val/accuracy: 99.95%, val/strict_accuracy: 86.76%\n",
      "Epoch 141: lr: 8.59e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0004, train/strict_loss: 0.0003, val/iit_loss: 0.0026, val/IIA: 83.24%, val/accuracy: 98.35%, val/strict_accuracy: 79.94%\n",
      "Epoch 142: lr: 8.58e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 92.53%, val/accuracy: 99.90%, val/strict_accuracy: 88.08%\n",
      "Epoch 143: lr: 8.57e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 91.84%, val/accuracy: 99.95%, val/strict_accuracy: 88.08%\n",
      "Epoch 144: lr: 8.56e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0016, val/IIA: 90.87%, val/accuracy: 99.98%, val/strict_accuracy: 88.95%\n",
      "Epoch 145: lr: 8.55e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0003, val/iit_loss: 0.0021, val/IIA: 88.79%, val/accuracy: 98.18%, val/strict_accuracy: 80.68%\n",
      "Epoch 146: lr: 8.54e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0012, val/IIA: 92.97%, val/accuracy: 99.98%, val/strict_accuracy: 87.88%\n",
      "Epoch 147: lr: 8.53e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 94.89%, val/accuracy: 99.94%, val/strict_accuracy: 87.69%\n",
      "Epoch 148: lr: 8.52e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 93.15%, val/accuracy: 99.95%, val/strict_accuracy: 88.54%\n",
      "Epoch 149: lr: 8.51e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0020, val/IIA: 90.56%, val/accuracy: 99.97%, val/strict_accuracy: 88.71%\n",
      "Epoch 150: lr: 8.50e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0008, val/IIA: 93.72%, val/accuracy: 100.00%, val/strict_accuracy: 89.02%\n",
      "Epoch 151: lr: 8.49e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 93.89%, val/accuracy: 99.97%, val/strict_accuracy: 88.83%\n",
      "Epoch 152: lr: 8.48e-04, train/iit_loss: 0.0031, train/behavior_loss: 0.0007, train/strict_loss: 0.0017, val/iit_loss: 0.0103, val/IIA: 78.93%, val/accuracy: 96.63%, val/strict_accuracy: 78.35%\n",
      "Epoch 153: lr: 8.47e-04, train/iit_loss: 0.0051, train/behavior_loss: 0.0007, train/strict_loss: 0.0025, val/iit_loss: 0.0027, val/IIA: 83.35%, val/accuracy: 95.42%, val/strict_accuracy: 76.31%\n",
      "Epoch 154: lr: 8.46e-04, train/iit_loss: 0.0030, train/behavior_loss: 0.0004, train/strict_loss: 0.0013, val/iit_loss: 0.0016, val/IIA: 89.82%, val/accuracy: 99.90%, val/strict_accuracy: 84.29%\n",
      "Epoch 155: lr: 8.45e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0018, val/IIA: 91.01%, val/accuracy: 99.92%, val/strict_accuracy: 85.16%\n",
      "Epoch 156: lr: 8.44e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 93.24%, val/accuracy: 99.94%, val/strict_accuracy: 87.22%\n",
      "Epoch 157: lr: 8.43e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0019, val/IIA: 90.70%, val/accuracy: 99.92%, val/strict_accuracy: 87.61%\n",
      "Epoch 158: lr: 8.42e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0015, val/IIA: 91.03%, val/accuracy: 99.12%, val/strict_accuracy: 79.58%\n",
      "Epoch 159: lr: 8.41e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 92.03%, val/accuracy: 99.01%, val/strict_accuracy: 86.36%\n",
      "Epoch 160: lr: 8.40e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 92.17%, val/accuracy: 99.85%, val/strict_accuracy: 86.03%\n",
      "Epoch 161: lr: 8.39e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0016, val/IIA: 91.78%, val/accuracy: 99.98%, val/strict_accuracy: 87.37%\n",
      "Epoch 162: lr: 8.38e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 92.33%, val/accuracy: 100.00%, val/strict_accuracy: 88.28%\n",
      "Epoch 163: lr: 8.37e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0004, train/strict_loss: 0.0002, val/iit_loss: 0.0023, val/IIA: 85.78%, val/accuracy: 96.19%, val/strict_accuracy: 70.58%\n",
      "Epoch 164: lr: 8.36e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0022, val/IIA: 89.56%, val/accuracy: 100.00%, val/strict_accuracy: 86.07%\n",
      "Epoch 165: lr: 8.35e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 91.83%, val/accuracy: 99.93%, val/strict_accuracy: 84.74%\n",
      "Epoch 166: lr: 8.34e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0021, val/IIA: 90.21%, val/accuracy: 99.92%, val/strict_accuracy: 85.35%\n",
      "Epoch 167: lr: 8.33e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0016, val/IIA: 90.40%, val/accuracy: 99.83%, val/strict_accuracy: 87.21%\n",
      "Epoch 168: lr: 8.32e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0016, val/IIA: 91.93%, val/accuracy: 99.96%, val/strict_accuracy: 86.85%\n",
      "Epoch 169: lr: 8.31e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 92.62%, val/accuracy: 99.96%, val/strict_accuracy: 88.56%\n",
      "Epoch 170: lr: 8.30e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 92.79%, val/accuracy: 99.98%, val/strict_accuracy: 89.46%\n",
      "Epoch 171: lr: 8.29e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0003, val/iit_loss: 0.0016, val/IIA: 91.43%, val/accuracy: 99.54%, val/strict_accuracy: 85.09%\n",
      "Epoch 172: lr: 8.28e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0022, val/IIA: 89.59%, val/accuracy: 99.96%, val/strict_accuracy: 87.85%\n",
      "Epoch 173: lr: 8.27e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 93.08%, val/accuracy: 99.21%, val/strict_accuracy: 89.04%\n",
      "Epoch 174: lr: 8.26e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 92.49%, val/accuracy: 99.82%, val/strict_accuracy: 87.65%\n",
      "Epoch 175: lr: 8.25e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0011, val/IIA: 93.83%, val/accuracy: 99.95%, val/strict_accuracy: 86.68%\n",
      "Epoch 176: lr: 8.24e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 92.23%, val/accuracy: 98.57%, val/strict_accuracy: 88.00%\n",
      "Epoch 177: lr: 8.23e-04, train/iit_loss: 0.0042, train/behavior_loss: 0.0009, train/strict_loss: 0.0025, val/iit_loss: 0.0025, val/IIA: 87.67%, val/accuracy: 99.23%, val/strict_accuracy: 67.12%\n",
      "Epoch 178: lr: 8.22e-04, train/iit_loss: 0.0033, train/behavior_loss: 0.0003, train/strict_loss: 0.0021, val/iit_loss: 0.0024, val/IIA: 86.85%, val/accuracy: 98.75%, val/strict_accuracy: 78.55%\n",
      "Epoch 179: lr: 8.21e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0002, train/strict_loss: 0.0011, val/iit_loss: 0.0017, val/IIA: 89.21%, val/accuracy: 98.20%, val/strict_accuracy: 82.54%\n",
      "Epoch 180: lr: 8.20e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0005, val/iit_loss: 0.0012, val/IIA: 92.47%, val/accuracy: 99.11%, val/strict_accuracy: 83.96%\n",
      "Epoch 181: lr: 8.19e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0019, val/IIA: 92.07%, val/accuracy: 99.99%, val/strict_accuracy: 88.85%\n",
      "Epoch 182: lr: 8.18e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 94.23%, val/accuracy: 99.95%, val/strict_accuracy: 90.14%\n",
      "Epoch 183: lr: 8.17e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 90.95%, val/accuracy: 98.97%, val/strict_accuracy: 85.73%\n",
      "Epoch 184: lr: 8.16e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 92.95%, val/accuracy: 99.97%, val/strict_accuracy: 88.95%\n",
      "Epoch 185: lr: 8.15e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0026, val/IIA: 88.50%, val/accuracy: 99.69%, val/strict_accuracy: 85.46%\n",
      "Epoch 186: lr: 8.14e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0017, val/IIA: 91.14%, val/accuracy: 99.87%, val/strict_accuracy: 90.21%\n",
      "Epoch 187: lr: 8.13e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 93.14%, val/accuracy: 99.97%, val/strict_accuracy: 86.85%\n",
      "Epoch 188: lr: 8.12e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 95.51%, val/accuracy: 99.97%, val/strict_accuracy: 90.61%\n",
      "Epoch 189: lr: 8.11e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0008, val/IIA: 94.80%, val/accuracy: 99.95%, val/strict_accuracy: 88.79%\n",
      "Epoch 190: lr: 8.10e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0030, val/IIA: 88.11%, val/accuracy: 99.91%, val/strict_accuracy: 87.77%\n",
      "Epoch 191: lr: 8.09e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 95.11%, val/accuracy: 99.92%, val/strict_accuracy: 88.57%\n",
      "Epoch 192: lr: 8.08e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0016, val/IIA: 93.58%, val/accuracy: 99.97%, val/strict_accuracy: 90.01%\n",
      "Epoch 193: lr: 8.07e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0022, val/IIA: 90.46%, val/accuracy: 99.86%, val/strict_accuracy: 87.34%\n",
      "Epoch 194: lr: 8.06e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0016, val/IIA: 91.83%, val/accuracy: 98.98%, val/strict_accuracy: 87.37%\n",
      "Epoch 195: lr: 8.05e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0005, train/strict_loss: 0.0009, val/iit_loss: 0.0026, val/IIA: 80.41%, val/accuracy: 88.31%, val/strict_accuracy: 71.98%\n",
      "Epoch 196: lr: 8.04e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0004, train/strict_loss: 0.0012, val/iit_loss: 0.0023, val/IIA: 88.95%, val/accuracy: 98.15%, val/strict_accuracy: 83.12%\n",
      "Epoch 197: lr: 8.03e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0003, train/strict_loss: 0.0007, val/iit_loss: 0.0022, val/IIA: 86.83%, val/accuracy: 98.16%, val/strict_accuracy: 85.97%\n",
      "Epoch 198: lr: 8.02e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 93.16%, val/accuracy: 99.97%, val/strict_accuracy: 89.64%\n",
      "Epoch 199: lr: 8.01e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0020, val/IIA: 90.45%, val/accuracy: 99.96%, val/strict_accuracy: 87.55%\n",
      "Epoch 200: lr: 8.00e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 93.16%, val/accuracy: 99.92%, val/strict_accuracy: 88.63%\n",
      "Epoch 201: lr: 7.99e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 93.64%, val/accuracy: 99.99%, val/strict_accuracy: 90.77%\n",
      "Epoch 202: lr: 7.98e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0004, train/strict_loss: 0.0003, val/iit_loss: 0.0017, val/IIA: 92.14%, val/accuracy: 99.82%, val/strict_accuracy: 87.08%\n",
      "Epoch 203: lr: 7.97e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 93.26%, val/accuracy: 100.00%, val/strict_accuracy: 88.57%\n",
      "Epoch 204: lr: 7.96e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 94.86%, val/accuracy: 99.99%, val/strict_accuracy: 92.08%\n",
      "Epoch 205: lr: 7.95e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0018, val/IIA: 90.43%, val/accuracy: 98.15%, val/strict_accuracy: 81.36%\n",
      "Epoch 206: lr: 7.94e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 93.86%, val/accuracy: 99.80%, val/strict_accuracy: 87.14%\n",
      "Epoch 207: lr: 7.93e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 94.20%, val/accuracy: 99.98%, val/strict_accuracy: 90.16%\n",
      "Epoch 208: lr: 7.92e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0019, val/IIA: 92.29%, val/accuracy: 99.97%, val/strict_accuracy: 86.89%\n",
      "Epoch 209: lr: 7.91e-04, train/iit_loss: 0.0042, train/behavior_loss: 0.0006, train/strict_loss: 0.0038, val/iit_loss: 0.0022, val/IIA: 86.27%, val/accuracy: 98.31%, val/strict_accuracy: 75.39%\n",
      "Epoch 210: lr: 7.90e-04, train/iit_loss: 0.0032, train/behavior_loss: 0.0004, train/strict_loss: 0.0014, val/iit_loss: 0.0036, val/IIA: 86.40%, val/accuracy: 98.60%, val/strict_accuracy: 84.05%\n",
      "Epoch 211: lr: 7.89e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0008, val/IIA: 95.07%, val/accuracy: 99.98%, val/strict_accuracy: 88.86%\n",
      "Epoch 212: lr: 7.88e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 94.16%, val/accuracy: 99.98%, val/strict_accuracy: 89.85%\n",
      "Epoch 213: lr: 7.87e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0016, val/IIA: 92.38%, val/accuracy: 99.99%, val/strict_accuracy: 91.72%\n",
      "Epoch 214: lr: 7.86e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 93.20%, val/accuracy: 99.66%, val/strict_accuracy: 89.48%\n",
      "Epoch 215: lr: 7.85e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 91.72%, val/accuracy: 99.90%, val/strict_accuracy: 90.21%\n",
      "Epoch 216: lr: 7.84e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 95.18%, val/accuracy: 99.94%, val/strict_accuracy: 89.62%\n",
      "Epoch 217: lr: 7.83e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0030, val/IIA: 82.57%, val/accuracy: 89.44%, val/strict_accuracy: 69.90%\n",
      "Epoch 218: lr: 7.82e-04, train/iit_loss: 0.0032, train/behavior_loss: 0.0006, train/strict_loss: 0.0017, val/iit_loss: 0.0021, val/IIA: 87.69%, val/accuracy: 98.01%, val/strict_accuracy: 79.75%\n",
      "Epoch 219: lr: 7.81e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0018, val/IIA: 91.35%, val/accuracy: 99.99%, val/strict_accuracy: 90.07%\n",
      "Epoch 220: lr: 7.80e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0000, train/strict_loss: 0.0002, val/iit_loss: 0.0006, val/IIA: 96.10%, val/accuracy: 99.99%, val/strict_accuracy: 90.14%\n",
      "Epoch 221: lr: 7.79e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 93.67%, val/accuracy: 99.94%, val/strict_accuracy: 88.41%\n",
      "Epoch 222: lr: 7.78e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0019, val/IIA: 90.90%, val/accuracy: 100.00%, val/strict_accuracy: 90.83%\n",
      "Epoch 223: lr: 7.77e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0004, train/strict_loss: 0.0004, val/iit_loss: 0.0031, val/IIA: 82.11%, val/accuracy: 85.66%, val/strict_accuracy: 72.95%\n",
      "Epoch 224: lr: 7.76e-04, train/iit_loss: 0.0025, train/behavior_loss: 0.0004, train/strict_loss: 0.0009, val/iit_loss: 0.0017, val/IIA: 91.75%, val/accuracy: 99.98%, val/strict_accuracy: 90.09%\n",
      "Epoch 225: lr: 7.75e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 94.97%, val/accuracy: 99.99%, val/strict_accuracy: 91.02%\n",
      "Epoch 226: lr: 7.74e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 92.65%, val/accuracy: 99.98%, val/strict_accuracy: 88.17%\n",
      "Epoch 227: lr: 7.73e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 93.48%, val/accuracy: 100.00%, val/strict_accuracy: 90.96%\n",
      "Epoch 228: lr: 7.72e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.91%, val/accuracy: 99.92%, val/strict_accuracy: 90.62%\n",
      "Epoch 229: lr: 7.71e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 93.41%, val/accuracy: 100.00%, val/strict_accuracy: 91.77%\n",
      "Epoch 230: lr: 7.70e-04, train/iit_loss: 0.0028, train/behavior_loss: 0.0005, train/strict_loss: 0.0010, val/iit_loss: 0.0022, val/IIA: 86.13%, val/accuracy: 96.13%, val/strict_accuracy: 75.64%\n",
      "Epoch 231: lr: 7.69e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0003, train/strict_loss: 0.0009, val/iit_loss: 0.0019, val/IIA: 90.76%, val/accuracy: 99.98%, val/strict_accuracy: 86.70%\n",
      "Epoch 232: lr: 7.68e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0018, val/IIA: 91.61%, val/accuracy: 99.97%, val/strict_accuracy: 91.05%\n",
      "Epoch 233: lr: 7.67e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 92.00%, val/accuracy: 100.00%, val/strict_accuracy: 91.51%\n",
      "Epoch 234: lr: 7.66e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0019, val/IIA: 92.94%, val/accuracy: 100.00%, val/strict_accuracy: 89.02%\n",
      "Epoch 235: lr: 7.65e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0019, val/IIA: 91.22%, val/accuracy: 100.00%, val/strict_accuracy: 91.97%\n",
      "Epoch 236: lr: 7.64e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 93.93%, val/accuracy: 99.53%, val/strict_accuracy: 88.36%\n",
      "Epoch 237: lr: 7.63e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 94.57%, val/accuracy: 99.88%, val/strict_accuracy: 91.17%\n",
      "Epoch 238: lr: 7.62e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 96.61%, val/accuracy: 99.99%, val/strict_accuracy: 90.64%\n",
      "Epoch 239: lr: 7.61e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0017, val/IIA: 92.77%, val/accuracy: 100.00%, val/strict_accuracy: 89.39%\n",
      "Epoch 240: lr: 7.60e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0007, val/IIA: 95.75%, val/accuracy: 99.99%, val/strict_accuracy: 89.42%\n",
      "Epoch 241: lr: 7.59e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 94.10%, val/accuracy: 99.95%, val/strict_accuracy: 87.69%\n",
      "Epoch 242: lr: 7.58e-04, train/iit_loss: 0.0025, train/behavior_loss: 0.0006, train/strict_loss: 0.0012, val/iit_loss: 0.0034, val/IIA: 83.91%, val/accuracy: 98.77%, val/strict_accuracy: 73.13%\n",
      "Epoch 243: lr: 7.57e-04, train/iit_loss: 0.0039, train/behavior_loss: 0.0006, train/strict_loss: 0.0026, val/iit_loss: 0.0022, val/IIA: 88.50%, val/accuracy: 98.45%, val/strict_accuracy: 75.71%\n",
      "Epoch 244: lr: 7.56e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0006, val/iit_loss: 0.0013, val/IIA: 92.71%, val/accuracy: 100.00%, val/strict_accuracy: 88.77%\n",
      "Epoch 245: lr: 7.55e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0007, val/IIA: 95.52%, val/accuracy: 99.99%, val/strict_accuracy: 90.77%\n",
      "Epoch 246: lr: 7.54e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0000, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 94.30%, val/accuracy: 100.00%, val/strict_accuracy: 91.71%\n",
      "Epoch 247: lr: 7.53e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0006, val/IIA: 96.73%, val/accuracy: 100.00%, val/strict_accuracy: 90.18%\n",
      "Epoch 248: lr: 7.52e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 94.34%, val/accuracy: 99.99%, val/strict_accuracy: 91.29%\n",
      "Epoch 249: lr: 7.51e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 94.81%, val/accuracy: 100.00%, val/strict_accuracy: 91.40%\n",
      "Epoch 250: lr: 7.50e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 93.39%, val/accuracy: 99.92%, val/strict_accuracy: 87.24%\n",
      "Epoch 251: lr: 7.49e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0004, train/strict_loss: 0.0008, val/iit_loss: 0.0043, val/IIA: 73.14%, val/accuracy: 91.14%, val/strict_accuracy: 54.62%\n",
      "Epoch 252: lr: 7.48e-04, train/iit_loss: 0.0030, train/behavior_loss: 0.0004, train/strict_loss: 0.0029, val/iit_loss: 0.0020, val/IIA: 87.89%, val/accuracy: 96.24%, val/strict_accuracy: 75.51%\n",
      "Epoch 253: lr: 7.47e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0004, train/strict_loss: 0.0019, val/iit_loss: 0.0060, val/IIA: 87.41%, val/accuracy: 99.96%, val/strict_accuracy: 83.12%\n",
      "Epoch 254: lr: 7.46e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0004, train/strict_loss: 0.0019, val/iit_loss: 0.0011, val/IIA: 92.15%, val/accuracy: 99.21%, val/strict_accuracy: 78.96%\n",
      "Epoch 255: lr: 7.45e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0004, train/strict_loss: 0.0022, val/iit_loss: 0.0016, val/IIA: 90.34%, val/accuracy: 99.87%, val/strict_accuracy: 83.37%\n",
      "Epoch 256: lr: 7.44e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0003, train/strict_loss: 0.0014, val/iit_loss: 0.0021, val/IIA: 81.34%, val/accuracy: 86.66%, val/strict_accuracy: 73.87%\n",
      "Epoch 257: lr: 7.43e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0003, train/strict_loss: 0.0012, val/iit_loss: 0.0044, val/IIA: 91.42%, val/accuracy: 99.45%, val/strict_accuracy: 82.38%\n",
      "Epoch 258: lr: 7.42e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0004, train/strict_loss: 0.0012, val/iit_loss: 0.0015, val/IIA: 90.36%, val/accuracy: 99.99%, val/strict_accuracy: 88.25%\n",
      "Epoch 259: lr: 7.41e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0000, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 93.10%, val/accuracy: 100.00%, val/strict_accuracy: 90.66%\n",
      "Epoch 260: lr: 7.40e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 91.86%, val/accuracy: 100.00%, val/strict_accuracy: 91.06%\n",
      "Epoch 261: lr: 7.39e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0002, val/iit_loss: 0.0008, val/IIA: 94.38%, val/accuracy: 100.00%, val/strict_accuracy: 91.22%\n",
      "Epoch 262: lr: 7.38e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 93.85%, val/accuracy: 99.98%, val/strict_accuracy: 85.34%\n",
      "Epoch 263: lr: 7.37e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 94.75%, val/accuracy: 100.00%, val/strict_accuracy: 91.48%\n",
      "Epoch 264: lr: 7.36e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 91.27%, val/accuracy: 99.84%, val/strict_accuracy: 83.02%\n",
      "Epoch 265: lr: 7.35e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 93.80%, val/accuracy: 99.98%, val/strict_accuracy: 90.92%\n",
      "Epoch 266: lr: 7.34e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 90.20%, val/accuracy: 96.43%, val/strict_accuracy: 80.13%\n",
      "Epoch 267: lr: 7.33e-04, train/iit_loss: 0.0032, train/behavior_loss: 0.0006, train/strict_loss: 0.0025, val/iit_loss: 0.0041, val/IIA: 83.84%, val/accuracy: 98.62%, val/strict_accuracy: 82.26%\n",
      "Epoch 268: lr: 7.32e-04, train/iit_loss: 0.0028, train/behavior_loss: 0.0004, train/strict_loss: 0.0015, val/iit_loss: 0.0018, val/IIA: 89.77%, val/accuracy: 99.69%, val/strict_accuracy: 81.64%\n",
      "Epoch 269: lr: 7.31e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0007, val/IIA: 95.84%, val/accuracy: 99.97%, val/strict_accuracy: 85.51%\n",
      "Epoch 270: lr: 7.30e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0016, val/IIA: 90.29%, val/accuracy: 99.86%, val/strict_accuracy: 88.70%\n",
      "Epoch 271: lr: 7.29e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 93.11%, val/accuracy: 100.00%, val/strict_accuracy: 90.58%\n",
      "Epoch 272: lr: 7.28e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 95.06%, val/accuracy: 100.00%, val/strict_accuracy: 90.99%\n",
      "Epoch 273: lr: 7.27e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 92.24%, val/accuracy: 100.00%, val/strict_accuracy: 90.88%\n",
      "Epoch 274: lr: 7.26e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0004, val/IIA: 96.40%, val/accuracy: 100.00%, val/strict_accuracy: 90.56%\n",
      "Epoch 275: lr: 7.25e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 91.41%, val/accuracy: 99.80%, val/strict_accuracy: 85.26%\n",
      "Epoch 276: lr: 7.24e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 95.27%, val/accuracy: 100.00%, val/strict_accuracy: 90.14%\n",
      "Epoch 277: lr: 7.23e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.74%, val/accuracy: 100.00%, val/strict_accuracy: 91.90%\n",
      "Epoch 278: lr: 7.22e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0019, val/IIA: 88.82%, val/accuracy: 100.00%, val/strict_accuracy: 88.65%\n",
      "Epoch 279: lr: 7.21e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0008, val/IIA: 95.83%, val/accuracy: 99.96%, val/strict_accuracy: 91.48%\n",
      "Epoch 280: lr: 7.20e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0004, train/strict_loss: 0.0009, val/iit_loss: 0.0029, val/IIA: 88.12%, val/accuracy: 99.95%, val/strict_accuracy: 82.28%\n",
      "Epoch 281: lr: 7.19e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0014, val/IIA: 93.81%, val/accuracy: 99.99%, val/strict_accuracy: 90.60%\n",
      "Epoch 282: lr: 7.18e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 90.97%, val/accuracy: 100.00%, val/strict_accuracy: 92.40%\n",
      "Epoch 283: lr: 7.17e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.44%, val/accuracy: 100.00%, val/strict_accuracy: 92.71%\n",
      "Epoch 284: lr: 7.16e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0019, val/IIA: 84.37%, val/accuracy: 94.01%, val/strict_accuracy: 87.83%\n",
      "Epoch 285: lr: 7.15e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 95.30%, val/accuracy: 100.00%, val/strict_accuracy: 92.34%\n",
      "Epoch 286: lr: 7.14e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 90.71%, val/accuracy: 98.11%, val/strict_accuracy: 87.19%\n",
      "Epoch 287: lr: 7.13e-04, train/iit_loss: 0.0040, train/behavior_loss: 0.0008, train/strict_loss: 0.0019, val/iit_loss: 0.0015, val/IIA: 89.70%, val/accuracy: 99.67%, val/strict_accuracy: 73.86%\n",
      "Epoch 288: lr: 7.12e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0001, train/strict_loss: 0.0009, val/iit_loss: 0.0009, val/IIA: 94.44%, val/accuracy: 99.99%, val/strict_accuracy: 88.84%\n",
      "Epoch 289: lr: 7.11e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 92.69%, val/accuracy: 100.00%, val/strict_accuracy: 91.85%\n",
      "Epoch 290: lr: 7.10e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 94.34%, val/accuracy: 100.00%, val/strict_accuracy: 91.56%\n",
      "Epoch 291: lr: 7.09e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.95%, val/accuracy: 100.00%, val/strict_accuracy: 91.50%\n",
      "Epoch 292: lr: 7.08e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 97.14%, val/accuracy: 100.00%, val/strict_accuracy: 92.44%\n",
      "Epoch 293: lr: 7.07e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 93.47%, val/accuracy: 99.97%, val/strict_accuracy: 91.85%\n",
      "Epoch 294: lr: 7.06e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 96.06%, val/accuracy: 99.96%, val/strict_accuracy: 92.22%\n",
      "Epoch 295: lr: 7.05e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0005, val/iit_loss: 0.0031, val/IIA: 83.86%, val/accuracy: 96.28%, val/strict_accuracy: 76.09%\n",
      "Epoch 296: lr: 7.04e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0007, train/strict_loss: 0.0022, val/iit_loss: 0.0019, val/IIA: 88.37%, val/accuracy: 96.67%, val/strict_accuracy: 73.85%\n",
      "Epoch 297: lr: 7.03e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0005, val/IIA: 96.22%, val/accuracy: 100.00%, val/strict_accuracy: 90.02%\n",
      "Epoch 298: lr: 7.02e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 94.78%, val/accuracy: 100.00%, val/strict_accuracy: 92.29%\n",
      "Epoch 299: lr: 7.01e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 91.89%, val/accuracy: 100.00%, val/strict_accuracy: 93.15%\n",
      "Epoch 300: lr: 7.00e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.84%, val/accuracy: 100.00%, val/strict_accuracy: 92.90%\n",
      "Epoch 301: lr: 6.99e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 92.84%, val/accuracy: 100.00%, val/strict_accuracy: 93.48%\n",
      "Epoch 302: lr: 6.98e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 95.55%, val/accuracy: 99.96%, val/strict_accuracy: 92.25%\n",
      "Epoch 303: lr: 6.97e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 96.03%, val/accuracy: 100.00%, val/strict_accuracy: 93.45%\n",
      "Epoch 304: lr: 6.96e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 96.00%, val/accuracy: 99.75%, val/strict_accuracy: 90.02%\n",
      "Epoch 305: lr: 6.95e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 93.25%, val/accuracy: 100.00%, val/strict_accuracy: 93.36%\n",
      "Epoch 306: lr: 6.94e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 92.56%, val/accuracy: 99.92%, val/strict_accuracy: 89.17%\n",
      "Epoch 307: lr: 6.93e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 93.44%, val/accuracy: 100.00%, val/strict_accuracy: 93.55%\n",
      "Epoch 308: lr: 6.92e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 95.89%, val/accuracy: 100.00%, val/strict_accuracy: 90.09%\n",
      "Epoch 309: lr: 6.91e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 95.52%, val/accuracy: 100.00%, val/strict_accuracy: 93.99%\n",
      "Epoch 310: lr: 6.90e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0017, val/IIA: 90.11%, val/accuracy: 99.98%, val/strict_accuracy: 90.25%\n",
      "Epoch 311: lr: 6.89e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.60%, val/accuracy: 100.00%, val/strict_accuracy: 91.24%\n",
      "Epoch 312: lr: 6.88e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0007, val/IIA: 96.15%, val/accuracy: 100.00%, val/strict_accuracy: 92.24%\n",
      "Epoch 313: lr: 6.87e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 95.13%, val/accuracy: 100.00%, val/strict_accuracy: 94.14%\n",
      "Epoch 314: lr: 6.86e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 92.01%, val/accuracy: 99.97%, val/strict_accuracy: 89.41%\n",
      "Epoch 315: lr: 6.85e-04, train/iit_loss: 0.0035, train/behavior_loss: 0.0007, train/strict_loss: 0.0025, val/iit_loss: 0.0020, val/IIA: 83.87%, val/accuracy: 91.65%, val/strict_accuracy: 75.95%\n",
      "Epoch 316: lr: 6.84e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0004, train/strict_loss: 0.0017, val/iit_loss: 0.0008, val/IIA: 95.43%, val/accuracy: 99.98%, val/strict_accuracy: 88.81%\n",
      "Epoch 317: lr: 6.83e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0014, val/IIA: 89.88%, val/accuracy: 99.35%, val/strict_accuracy: 83.86%\n",
      "Epoch 318: lr: 6.82e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0013, val/IIA: 93.28%, val/accuracy: 99.98%, val/strict_accuracy: 91.86%\n",
      "Epoch 319: lr: 6.81e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 94.33%, val/accuracy: 100.00%, val/strict_accuracy: 93.23%\n",
      "Epoch 320: lr: 6.80e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 96.03%, val/accuracy: 100.00%, val/strict_accuracy: 92.58%\n",
      "Epoch 321: lr: 6.79e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 94.00%, val/accuracy: 100.00%, val/strict_accuracy: 93.82%\n",
      "Epoch 322: lr: 6.78e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 95.87%, val/accuracy: 100.00%, val/strict_accuracy: 94.24%\n",
      "Epoch 323: lr: 6.77e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.45%, val/accuracy: 100.00%, val/strict_accuracy: 93.04%\n",
      "Epoch 324: lr: 6.76e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0008, val/IIA: 94.74%, val/accuracy: 100.00%, val/strict_accuracy: 92.54%\n",
      "Epoch 325: lr: 6.75e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 92.91%, val/accuracy: 100.00%, val/strict_accuracy: 94.17%\n",
      "Epoch 326: lr: 6.74e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 95.55%, val/accuracy: 99.97%, val/strict_accuracy: 93.20%\n",
      "Epoch 327: lr: 6.73e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0003, train/strict_loss: 0.0003, val/iit_loss: 0.0015, val/IIA: 91.40%, val/accuracy: 99.75%, val/strict_accuracy: 80.93%\n",
      "Epoch 328: lr: 6.72e-04, train/iit_loss: 0.0022, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0005, val/IIA: 95.81%, val/accuracy: 100.00%, val/strict_accuracy: 92.98%\n",
      "Epoch 329: lr: 6.71e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 96.43%, val/accuracy: 100.00%, val/strict_accuracy: 93.75%\n",
      "Epoch 330: lr: 6.70e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 93.47%, val/accuracy: 100.00%, val/strict_accuracy: 94.32%\n",
      "Epoch 331: lr: 6.69e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.96%, val/accuracy: 100.00%, val/strict_accuracy: 93.59%\n",
      "Epoch 332: lr: 6.68e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 91.46%, val/accuracy: 98.26%, val/strict_accuracy: 88.05%\n",
      "Epoch 333: lr: 6.67e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 95.20%, val/accuracy: 100.00%, val/strict_accuracy: 91.69%\n",
      "Epoch 334: lr: 6.66e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 95.13%, val/accuracy: 100.00%, val/strict_accuracy: 94.21%\n",
      "Epoch 335: lr: 6.65e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 94.98%, val/accuracy: 100.00%, val/strict_accuracy: 94.19%\n",
      "Epoch 336: lr: 6.64e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.51%, val/accuracy: 99.98%, val/strict_accuracy: 93.97%\n",
      "Epoch 337: lr: 6.63e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.94%, val/accuracy: 100.00%, val/strict_accuracy: 93.88%\n",
      "Epoch 338: lr: 6.62e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 92.67%, val/accuracy: 99.50%, val/strict_accuracy: 92.66%\n",
      "Epoch 339: lr: 6.61e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.97%, val/accuracy: 100.00%, val/strict_accuracy: 93.72%\n",
      "Epoch 340: lr: 6.60e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 91.41%, val/accuracy: 99.79%, val/strict_accuracy: 86.91%\n",
      "Epoch 341: lr: 6.59e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 92.81%, val/accuracy: 100.00%, val/strict_accuracy: 90.04%\n",
      "Epoch 342: lr: 6.58e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0006, val/IIA: 96.06%, val/accuracy: 99.99%, val/strict_accuracy: 92.83%\n",
      "Epoch 343: lr: 6.57e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 85.47%, val/accuracy: 97.67%, val/strict_accuracy: 83.40%\n",
      "Epoch 344: lr: 6.56e-04, train/iit_loss: 0.0031, train/behavior_loss: 0.0005, train/strict_loss: 0.0021, val/iit_loss: 0.0045, val/IIA: 81.67%, val/accuracy: 97.46%, val/strict_accuracy: 77.61%\n",
      "Epoch 345: lr: 6.55e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0012, val/iit_loss: 0.0012, val/IIA: 93.10%, val/accuracy: 99.94%, val/strict_accuracy: 86.24%\n",
      "Epoch 346: lr: 6.54e-04, train/iit_loss: 0.0025, train/behavior_loss: 0.0005, train/strict_loss: 0.0022, val/iit_loss: 0.0038, val/IIA: 83.03%, val/accuracy: 99.69%, val/strict_accuracy: 83.36%\n",
      "Epoch 347: lr: 6.53e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 92.54%, val/accuracy: 100.00%, val/strict_accuracy: 91.33%\n",
      "Epoch 348: lr: 6.52e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 96.10%, val/accuracy: 99.99%, val/strict_accuracy: 93.38%\n",
      "Epoch 349: lr: 6.51e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 96.04%, val/accuracy: 100.00%, val/strict_accuracy: 93.69%\n",
      "Epoch 350: lr: 6.50e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 95.58%, val/accuracy: 100.00%, val/strict_accuracy: 93.80%\n",
      "Epoch 351: lr: 6.49e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 94.54%, val/accuracy: 100.00%, val/strict_accuracy: 93.14%\n",
      "Epoch 352: lr: 6.48e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 95.22%, val/accuracy: 99.99%, val/strict_accuracy: 91.79%\n",
      "Epoch 353: lr: 6.47e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0003, val/IIA: 97.78%, val/accuracy: 100.00%, val/strict_accuracy: 92.66%\n",
      "Epoch 354: lr: 6.46e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.99%, val/accuracy: 99.99%, val/strict_accuracy: 93.37%\n",
      "Epoch 355: lr: 6.45e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 94.72%, val/accuracy: 100.00%, val/strict_accuracy: 93.45%\n",
      "Epoch 356: lr: 6.44e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.46%, val/accuracy: 100.00%, val/strict_accuracy: 93.78%\n",
      "Epoch 357: lr: 6.43e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.24%, val/accuracy: 100.00%, val/strict_accuracy: 94.30%\n",
      "Epoch 358: lr: 6.42e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 93.19%, val/accuracy: 99.93%, val/strict_accuracy: 88.65%\n",
      "Epoch 359: lr: 6.41e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0003, train/strict_loss: 0.0007, val/iit_loss: 0.0010, val/IIA: 93.84%, val/accuracy: 99.98%, val/strict_accuracy: 88.92%\n",
      "Epoch 360: lr: 6.40e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0006, val/IIA: 95.68%, val/accuracy: 99.89%, val/strict_accuracy: 91.10%\n",
      "Epoch 361: lr: 6.39e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.63%, val/accuracy: 100.00%, val/strict_accuracy: 92.87%\n",
      "Epoch 362: lr: 6.38e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.20%, val/accuracy: 100.00%, val/strict_accuracy: 92.75%\n",
      "Epoch 363: lr: 6.37e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0008, val/IIA: 93.56%, val/accuracy: 100.00%, val/strict_accuracy: 91.70%\n",
      "Epoch 364: lr: 6.36e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.26%, val/accuracy: 100.00%, val/strict_accuracy: 93.45%\n",
      "Epoch 365: lr: 6.35e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 92.87%, val/accuracy: 100.00%, val/strict_accuracy: 88.64%\n",
      "Epoch 366: lr: 6.34e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.79%, val/accuracy: 100.00%, val/strict_accuracy: 93.55%\n",
      "Epoch 367: lr: 6.33e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.20%, val/accuracy: 100.00%, val/strict_accuracy: 93.89%\n",
      "Epoch 368: lr: 6.32e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0006, val/IIA: 95.84%, val/accuracy: 99.90%, val/strict_accuracy: 92.05%\n",
      "Epoch 369: lr: 6.31e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.82%, val/accuracy: 100.00%, val/strict_accuracy: 93.36%\n",
      "Epoch 370: lr: 6.30e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0025, val/IIA: 82.21%, val/accuracy: 91.85%, val/strict_accuracy: 71.64%\n",
      "Epoch 371: lr: 6.29e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0005, train/strict_loss: 0.0010, val/iit_loss: 0.0014, val/IIA: 88.48%, val/accuracy: 99.35%, val/strict_accuracy: 88.02%\n",
      "Epoch 372: lr: 6.28e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.71%, val/accuracy: 100.00%, val/strict_accuracy: 93.22%\n",
      "Epoch 373: lr: 6.27e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.05%, val/accuracy: 100.00%, val/strict_accuracy: 94.39%\n",
      "Epoch 374: lr: 6.26e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0003, train/strict_loss: 0.0003, val/iit_loss: 0.0038, val/IIA: 72.01%, val/accuracy: 78.75%, val/strict_accuracy: 70.78%\n",
      "Epoch 375: lr: 6.25e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0005, val/IIA: 96.35%, val/accuracy: 100.00%, val/strict_accuracy: 92.70%\n",
      "Epoch 376: lr: 6.24e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.37%, val/accuracy: 100.00%, val/strict_accuracy: 94.56%\n",
      "Epoch 377: lr: 6.23e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.31%, val/accuracy: 100.00%, val/strict_accuracy: 94.64%\n",
      "Epoch 378: lr: 6.22e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 91.66%, val/accuracy: 96.98%, val/strict_accuracy: 88.18%\n",
      "Epoch 379: lr: 6.21e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.45%, val/accuracy: 100.00%, val/strict_accuracy: 95.10%\n",
      "Epoch 380: lr: 6.20e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 93.41%, val/accuracy: 99.79%, val/strict_accuracy: 90.24%\n",
      "Epoch 381: lr: 6.19e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0019, val/IIA: 83.26%, val/accuracy: 90.75%, val/strict_accuracy: 78.57%\n",
      "Epoch 382: lr: 6.18e-04, train/iit_loss: 0.0025, train/behavior_loss: 0.0005, train/strict_loss: 0.0025, val/iit_loss: 0.0054, val/IIA: 83.42%, val/accuracy: 99.07%, val/strict_accuracy: 81.16%\n",
      "Epoch 383: lr: 6.17e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0004, train/strict_loss: 0.0015, val/iit_loss: 0.0012, val/IIA: 89.27%, val/accuracy: 99.98%, val/strict_accuracy: 87.44%\n",
      "Epoch 384: lr: 6.16e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0000, train/strict_loss: 0.0002, val/iit_loss: 0.0006, val/IIA: 95.24%, val/accuracy: 99.98%, val/strict_accuracy: 93.22%\n",
      "Epoch 385: lr: 6.15e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.48%, val/accuracy: 100.00%, val/strict_accuracy: 94.03%\n",
      "Epoch 386: lr: 6.14e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.44%, val/accuracy: 100.00%, val/strict_accuracy: 94.26%\n",
      "Epoch 387: lr: 6.13e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.03%, val/accuracy: 99.99%, val/strict_accuracy: 92.73%\n",
      "Epoch 388: lr: 6.12e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 92.05%, val/accuracy: 100.00%, val/strict_accuracy: 94.33%\n",
      "Epoch 389: lr: 6.11e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.84%, val/accuracy: 100.00%, val/strict_accuracy: 92.68%\n",
      "Epoch 390: lr: 6.10e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 94.08%, val/accuracy: 100.00%, val/strict_accuracy: 91.32%\n",
      "Epoch 391: lr: 6.09e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 92.55%, val/accuracy: 99.67%, val/strict_accuracy: 89.66%\n",
      "Epoch 392: lr: 6.08e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0004, val/IIA: 97.59%, val/accuracy: 100.00%, val/strict_accuracy: 93.86%\n",
      "Epoch 393: lr: 6.07e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.78%, val/accuracy: 100.00%, val/strict_accuracy: 94.75%\n",
      "Epoch 394: lr: 6.06e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 94.86%, val/accuracy: 99.35%, val/strict_accuracy: 89.96%\n",
      "Epoch 395: lr: 6.05e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0004, val/IIA: 96.46%, val/accuracy: 100.00%, val/strict_accuracy: 93.17%\n",
      "Epoch 396: lr: 6.04e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.40%, val/accuracy: 100.00%, val/strict_accuracy: 94.66%\n",
      "Epoch 397: lr: 6.03e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.40%, val/accuracy: 99.97%, val/strict_accuracy: 93.69%\n",
      "Epoch 398: lr: 6.02e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.10%, val/accuracy: 100.00%, val/strict_accuracy: 95.19%\n",
      "Epoch 399: lr: 6.01e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 90.13%, val/accuracy: 99.98%, val/strict_accuracy: 93.08%\n",
      "Epoch 400: lr: 6.00e-04, train/iit_loss: 0.0036, train/behavior_loss: 0.0006, train/strict_loss: 0.0029, val/iit_loss: 0.0012, val/IIA: 92.47%, val/accuracy: 98.78%, val/strict_accuracy: 71.32%\n",
      "Epoch 401: lr: 5.99e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0009, val/iit_loss: 0.0006, val/IIA: 95.92%, val/accuracy: 100.00%, val/strict_accuracy: 91.49%\n",
      "Epoch 402: lr: 5.98e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.01%, val/accuracy: 100.00%, val/strict_accuracy: 93.40%\n",
      "Epoch 403: lr: 5.97e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.35%, val/accuracy: 100.00%, val/strict_accuracy: 93.94%\n",
      "Epoch 404: lr: 5.96e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.84%, val/accuracy: 100.00%, val/strict_accuracy: 94.48%\n",
      "Epoch 405: lr: 5.95e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.39%, val/accuracy: 100.00%, val/strict_accuracy: 94.73%\n",
      "Epoch 406: lr: 5.94e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 93.12%, val/accuracy: 99.83%, val/strict_accuracy: 90.43%\n",
      "Epoch 407: lr: 5.93e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.75%, val/accuracy: 100.00%, val/strict_accuracy: 93.72%\n",
      "Epoch 408: lr: 5.92e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 95.69%, val/accuracy: 100.00%, val/strict_accuracy: 95.37%\n",
      "Epoch 409: lr: 5.91e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.18%, val/accuracy: 100.00%, val/strict_accuracy: 95.18%\n",
      "Epoch 410: lr: 5.90e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0006, train/strict_loss: 0.0009, val/iit_loss: 0.0034, val/IIA: 80.95%, val/accuracy: 95.70%, val/strict_accuracy: 68.53%\n",
      "Epoch 411: lr: 5.89e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0005, val/IIA: 96.18%, val/accuracy: 99.98%, val/strict_accuracy: 92.76%\n",
      "Epoch 412: lr: 5.88e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.91%, val/accuracy: 100.00%, val/strict_accuracy: 94.80%\n",
      "Epoch 413: lr: 5.87e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.66%, val/accuracy: 100.00%, val/strict_accuracy: 95.02%\n",
      "Epoch 414: lr: 5.86e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.89%, val/accuracy: 100.00%, val/strict_accuracy: 95.32%\n",
      "Epoch 415: lr: 5.85e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.31%, val/accuracy: 100.00%, val/strict_accuracy: 94.03%\n",
      "Epoch 416: lr: 5.84e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.29%, val/accuracy: 100.00%, val/strict_accuracy: 94.31%\n",
      "Epoch 417: lr: 5.83e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 95.60%, val/accuracy: 99.97%, val/strict_accuracy: 93.06%\n",
      "Epoch 418: lr: 5.82e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.56%, val/accuracy: 99.99%, val/strict_accuracy: 92.73%\n",
      "Epoch 419: lr: 5.81e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.28%, val/accuracy: 100.00%, val/strict_accuracy: 95.47%\n",
      "Epoch 420: lr: 5.80e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.18%, val/accuracy: 100.00%, val/strict_accuracy: 94.61%\n",
      "Epoch 421: lr: 5.79e-04, train/iit_loss: 0.0028, train/behavior_loss: 0.0004, train/strict_loss: 0.0019, val/iit_loss: 0.0026, val/IIA: 84.97%, val/accuracy: 98.56%, val/strict_accuracy: 71.21%\n",
      "Epoch 422: lr: 5.78e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0014, val/iit_loss: 0.0006, val/IIA: 95.45%, val/accuracy: 100.00%, val/strict_accuracy: 91.31%\n",
      "Epoch 423: lr: 5.77e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.94%, val/accuracy: 100.00%, val/strict_accuracy: 94.82%\n",
      "Epoch 424: lr: 5.76e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.31%, val/accuracy: 100.00%, val/strict_accuracy: 95.36%\n",
      "Epoch 425: lr: 5.75e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.47%, val/accuracy: 100.00%, val/strict_accuracy: 95.57%\n",
      "Epoch 426: lr: 5.74e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 91.08%, val/accuracy: 97.66%, val/strict_accuracy: 78.72%\n",
      "Epoch 427: lr: 5.73e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.41%, val/accuracy: 100.00%, val/strict_accuracy: 95.79%\n",
      "Epoch 428: lr: 5.72e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.12%, val/accuracy: 100.00%, val/strict_accuracy: 95.04%\n",
      "Epoch 429: lr: 5.71e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.10%, val/accuracy: 99.97%, val/strict_accuracy: 94.20%\n",
      "Epoch 430: lr: 5.70e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.08%, val/accuracy: 100.00%, val/strict_accuracy: 93.79%\n",
      "Epoch 431: lr: 5.69e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.05%, val/accuracy: 100.00%, val/strict_accuracy: 94.48%\n",
      "Epoch 432: lr: 5.68e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.49%, val/accuracy: 99.94%, val/strict_accuracy: 87.69%\n",
      "Epoch 433: lr: 5.67e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.58%, val/accuracy: 100.00%, val/strict_accuracy: 96.09%\n",
      "Epoch 434: lr: 5.66e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.57%, val/accuracy: 99.97%, val/strict_accuracy: 94.59%\n",
      "Epoch 435: lr: 5.65e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.25%, val/accuracy: 100.00%, val/strict_accuracy: 95.06%\n",
      "Epoch 436: lr: 5.64e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.91%, val/accuracy: 99.99%, val/strict_accuracy: 93.75%\n",
      "Epoch 437: lr: 5.63e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.07%, val/accuracy: 100.00%, val/strict_accuracy: 94.01%\n",
      "Epoch 438: lr: 5.62e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.44%, val/accuracy: 100.00%, val/strict_accuracy: 95.24%\n",
      "Epoch 439: lr: 5.61e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 93.40%, val/accuracy: 99.69%, val/strict_accuracy: 93.42%\n",
      "Epoch 440: lr: 5.60e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.54%, val/accuracy: 100.00%, val/strict_accuracy: 96.13%\n",
      "Epoch 441: lr: 5.59e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.30%, val/accuracy: 99.84%, val/strict_accuracy: 89.05%\n",
      "Epoch 442: lr: 5.58e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.21%, val/accuracy: 99.99%, val/strict_accuracy: 94.39%\n",
      "Epoch 443: lr: 5.57e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.64%, val/accuracy: 99.96%, val/strict_accuracy: 94.06%\n",
      "Epoch 444: lr: 5.56e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0003, train/strict_loss: 0.0009, val/iit_loss: 0.0085, val/IIA: 84.12%, val/accuracy: 99.37%, val/strict_accuracy: 78.06%\n",
      "Epoch 445: lr: 5.55e-04, train/iit_loss: 0.0033, train/behavior_loss: 0.0005, train/strict_loss: 0.0028, val/iit_loss: 0.0010, val/IIA: 90.34%, val/accuracy: 97.65%, val/strict_accuracy: 83.06%\n",
      "Epoch 446: lr: 5.54e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0004, val/IIA: 95.99%, val/accuracy: 100.00%, val/strict_accuracy: 93.39%\n",
      "Epoch 447: lr: 5.53e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.04%, val/accuracy: 100.00%, val/strict_accuracy: 94.89%\n",
      "Epoch 448: lr: 5.52e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.62%, val/accuracy: 100.00%, val/strict_accuracy: 95.51%\n",
      "Epoch 449: lr: 5.51e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.11%, val/accuracy: 100.00%, val/strict_accuracy: 96.01%\n",
      "Epoch 450: lr: 5.50e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.10%, val/accuracy: 100.00%, val/strict_accuracy: 95.83%\n",
      "Epoch 451: lr: 5.49e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.82%, val/accuracy: 100.00%, val/strict_accuracy: 96.14%\n",
      "Epoch 452: lr: 5.48e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.58%, val/accuracy: 99.99%, val/strict_accuracy: 89.29%\n",
      "Epoch 453: lr: 5.47e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.49%, val/accuracy: 100.00%, val/strict_accuracy: 96.41%\n",
      "Epoch 454: lr: 5.46e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.09%, val/accuracy: 100.00%, val/strict_accuracy: 94.66%\n",
      "Epoch 455: lr: 5.45e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.58%, val/accuracy: 100.00%, val/strict_accuracy: 96.24%\n",
      "Epoch 456: lr: 5.44e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.32%, val/accuracy: 100.00%, val/strict_accuracy: 93.74%\n",
      "Epoch 457: lr: 5.43e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.49%, val/accuracy: 100.00%, val/strict_accuracy: 95.25%\n",
      "Epoch 458: lr: 5.42e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.20%, val/accuracy: 100.00%, val/strict_accuracy: 96.25%\n",
      "Epoch 459: lr: 5.41e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.16%, val/accuracy: 100.00%, val/strict_accuracy: 96.66%\n",
      "Epoch 460: lr: 5.40e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0024, val/IIA: 84.39%, val/accuracy: 89.38%, val/strict_accuracy: 69.37%\n",
      "Epoch 461: lr: 5.39e-04, train/iit_loss: 0.0040, train/behavior_loss: 0.0004, train/strict_loss: 0.0024, val/iit_loss: 0.0023, val/IIA: 89.74%, val/accuracy: 99.09%, val/strict_accuracy: 86.10%\n",
      "Epoch 462: lr: 5.38e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0006, val/iit_loss: 0.0005, val/IIA: 96.03%, val/accuracy: 100.00%, val/strict_accuracy: 93.06%\n",
      "Epoch 463: lr: 5.37e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.77%, val/accuracy: 100.00%, val/strict_accuracy: 94.83%\n",
      "Epoch 464: lr: 5.36e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 96.90%, val/accuracy: 100.00%, val/strict_accuracy: 95.51%\n",
      "Epoch 465: lr: 5.35e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.85%, val/accuracy: 100.00%, val/strict_accuracy: 95.80%\n",
      "Epoch 466: lr: 5.34e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.20%, val/accuracy: 100.00%, val/strict_accuracy: 93.27%\n",
      "Epoch 467: lr: 5.33e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 96.88%, val/accuracy: 100.00%, val/strict_accuracy: 96.13%\n",
      "Epoch 468: lr: 5.32e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.85%, val/accuracy: 100.00%, val/strict_accuracy: 96.00%\n",
      "Epoch 469: lr: 5.31e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.31%, val/accuracy: 100.00%, val/strict_accuracy: 96.23%\n",
      "Epoch 470: lr: 5.30e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.10%, val/accuracy: 100.00%, val/strict_accuracy: 96.09%\n",
      "Epoch 471: lr: 5.29e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.30%, val/accuracy: 100.00%, val/strict_accuracy: 96.25%\n",
      "Epoch 472: lr: 5.28e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.70%, val/accuracy: 100.00%, val/strict_accuracy: 96.46%\n",
      "Epoch 473: lr: 5.27e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0033, val/IIA: 84.06%, val/accuracy: 93.39%, val/strict_accuracy: 73.15%\n",
      "Epoch 474: lr: 5.26e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0023, val/IIA: 90.12%, val/accuracy: 99.98%, val/strict_accuracy: 85.77%\n",
      "Epoch 475: lr: 5.25e-04, train/iit_loss: 0.0022, train/behavior_loss: 0.0004, train/strict_loss: 0.0014, val/iit_loss: 0.0014, val/IIA: 85.39%, val/accuracy: 91.83%, val/strict_accuracy: 75.08%\n",
      "Epoch 476: lr: 5.24e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0003, val/IIA: 97.42%, val/accuracy: 100.00%, val/strict_accuracy: 94.81%\n",
      "Epoch 477: lr: 5.23e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.50%, val/accuracy: 100.00%, val/strict_accuracy: 96.09%\n",
      "Epoch 478: lr: 5.22e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.48%, val/accuracy: 100.00%, val/strict_accuracy: 95.04%\n",
      "Epoch 479: lr: 5.21e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.52%, val/accuracy: 100.00%, val/strict_accuracy: 95.82%\n",
      "Epoch 480: lr: 5.20e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.05%, val/accuracy: 100.00%, val/strict_accuracy: 93.57%\n",
      "Epoch 481: lr: 5.19e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.75%, val/accuracy: 100.00%, val/strict_accuracy: 96.31%\n",
      "Epoch 482: lr: 5.18e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.34%, val/accuracy: 100.00%, val/strict_accuracy: 96.36%\n",
      "Epoch 483: lr: 5.17e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 93.60%, val/accuracy: 100.00%, val/strict_accuracy: 95.27%\n",
      "Epoch 484: lr: 5.16e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.72%, val/accuracy: 100.00%, val/strict_accuracy: 95.33%\n",
      "Epoch 485: lr: 5.15e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.55%, val/accuracy: 100.00%, val/strict_accuracy: 96.23%\n",
      "Epoch 486: lr: 5.14e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.02%, val/accuracy: 100.00%, val/strict_accuracy: 96.64%\n",
      "Epoch 487: lr: 5.13e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 97.90%, val/accuracy: 100.00%, val/strict_accuracy: 96.58%\n",
      "Epoch 488: lr: 5.12e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.45%, val/accuracy: 100.00%, val/strict_accuracy: 96.83%\n",
      "Epoch 489: lr: 5.11e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 97.04%, val/accuracy: 100.00%, val/strict_accuracy: 92.61%\n",
      "Epoch 490: lr: 5.10e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0021, val/IIA: 84.04%, val/accuracy: 93.70%, val/strict_accuracy: 63.51%\n",
      "Epoch 491: lr: 5.09e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0003, train/strict_loss: 0.0030, val/iit_loss: 0.0011, val/IIA: 93.33%, val/accuracy: 100.00%, val/strict_accuracy: 90.67%\n",
      "Epoch 492: lr: 5.08e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.95%, val/accuracy: 100.00%, val/strict_accuracy: 95.28%\n",
      "Epoch 493: lr: 5.07e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.12%, val/accuracy: 100.00%, val/strict_accuracy: 95.99%\n",
      "Epoch 494: lr: 5.06e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.83%, val/accuracy: 100.00%, val/strict_accuracy: 95.98%\n",
      "Epoch 495: lr: 5.05e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.05%, val/accuracy: 100.00%, val/strict_accuracy: 96.64%\n",
      "Epoch 496: lr: 5.04e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.63%, val/accuracy: 100.00%, val/strict_accuracy: 96.14%\n",
      "Epoch 497: lr: 5.03e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.98%, val/accuracy: 100.00%, val/strict_accuracy: 96.19%\n",
      "Epoch 498: lr: 5.02e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.78%, val/accuracy: 100.00%, val/strict_accuracy: 96.74%\n",
      "Epoch 499: lr: 5.01e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.29%, val/accuracy: 100.00%, val/strict_accuracy: 96.86%\n",
      "Epoch 500: lr: 5.00e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.37%, val/accuracy: 100.00%, val/strict_accuracy: 96.57%\n",
      "Epoch 501: lr: 4.99e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 88.47%, val/accuracy: 91.03%, val/strict_accuracy: 91.24%\n",
      "Epoch 502: lr: 4.98e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.80%, val/accuracy: 99.94%, val/strict_accuracy: 87.14%\n",
      "Epoch 503: lr: 4.97e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.22%, val/accuracy: 100.00%, val/strict_accuracy: 96.85%\n",
      "Epoch 504: lr: 4.96e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.38%, val/accuracy: 100.00%, val/strict_accuracy: 96.26%\n",
      "Epoch 505: lr: 4.95e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 96.69%, val/accuracy: 100.00%, val/strict_accuracy: 95.89%\n",
      "Epoch 506: lr: 4.94e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.82%, val/accuracy: 100.00%, val/strict_accuracy: 96.33%\n",
      "Epoch 507: lr: 4.93e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.77%, val/accuracy: 100.00%, val/strict_accuracy: 96.93%\n",
      "Epoch 508: lr: 4.92e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.53%, val/accuracy: 100.00%, val/strict_accuracy: 95.15%\n",
      "Epoch 509: lr: 4.91e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.15%, val/accuracy: 99.93%, val/strict_accuracy: 92.80%\n",
      "Epoch 510: lr: 4.90e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.25%, val/accuracy: 100.00%, val/strict_accuracy: 94.59%\n",
      "Epoch 511: lr: 4.89e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 95.34%, val/accuracy: 99.99%, val/strict_accuracy: 93.01%\n",
      "Epoch 512: lr: 4.88e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0005, val/IIA: 95.35%, val/accuracy: 99.99%, val/strict_accuracy: 91.20%\n",
      "Epoch 513: lr: 4.87e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 99.01%, val/accuracy: 100.00%, val/strict_accuracy: 96.75%\n",
      "Epoch 514: lr: 4.86e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.96%, val/accuracy: 100.00%, val/strict_accuracy: 96.52%\n",
      "Epoch 515: lr: 4.85e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0004, train/strict_loss: 0.0019, val/iit_loss: 0.0025, val/IIA: 83.25%, val/accuracy: 95.05%, val/strict_accuracy: 68.67%\n",
      "Epoch 516: lr: 4.84e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0004, val/IIA: 96.88%, val/accuracy: 99.97%, val/strict_accuracy: 92.54%\n",
      "Epoch 517: lr: 4.83e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0006, val/IIA: 95.21%, val/accuracy: 100.00%, val/strict_accuracy: 90.65%\n",
      "Epoch 518: lr: 4.82e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0000, train/strict_loss: 0.0002, val/iit_loss: 0.0003, val/IIA: 96.88%, val/accuracy: 100.00%, val/strict_accuracy: 93.50%\n",
      "Epoch 519: lr: 4.81e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0002, val/iit_loss: 0.0003, val/IIA: 98.34%, val/accuracy: 100.00%, val/strict_accuracy: 94.70%\n",
      "Epoch 520: lr: 4.80e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0004, val/IIA: 96.44%, val/accuracy: 99.98%, val/strict_accuracy: 94.18%\n",
      "Epoch 521: lr: 4.79e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.85%, val/accuracy: 100.00%, val/strict_accuracy: 95.77%\n",
      "Epoch 522: lr: 4.78e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.79%, val/accuracy: 100.00%, val/strict_accuracy: 96.63%\n",
      "Epoch 523: lr: 4.77e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.79%, val/accuracy: 100.00%, val/strict_accuracy: 96.63%\n",
      "Epoch 524: lr: 4.76e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.09%, val/accuracy: 99.93%, val/strict_accuracy: 89.74%\n",
      "Epoch 525: lr: 4.75e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 97.97%, val/accuracy: 100.00%, val/strict_accuracy: 96.90%\n",
      "Epoch 526: lr: 4.74e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.88%, val/accuracy: 100.00%, val/strict_accuracy: 95.40%\n",
      "Epoch 527: lr: 4.73e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.53%, val/accuracy: 100.00%, val/strict_accuracy: 97.13%\n",
      "Epoch 528: lr: 4.72e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.84%, val/accuracy: 100.00%, val/strict_accuracy: 96.93%\n",
      "Epoch 529: lr: 4.71e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 92.39%, val/accuracy: 99.98%, val/strict_accuracy: 92.60%\n",
      "Epoch 530: lr: 4.70e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.88%, val/accuracy: 99.99%, val/strict_accuracy: 93.63%\n",
      "Epoch 531: lr: 4.69e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.62%, val/accuracy: 100.00%, val/strict_accuracy: 97.12%\n",
      "Epoch 532: lr: 4.68e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.51%, val/accuracy: 100.00%, val/strict_accuracy: 95.11%\n",
      "Epoch 533: lr: 4.67e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0008, val/IIA: 94.77%, val/accuracy: 100.00%, val/strict_accuracy: 93.73%\n",
      "Epoch 534: lr: 4.66e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0014, val/iit_loss: 0.0017, val/IIA: 86.66%, val/accuracy: 97.84%, val/strict_accuracy: 77.31%\n",
      "Epoch 535: lr: 4.65e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0010, val/iit_loss: 0.0009, val/IIA: 90.97%, val/accuracy: 99.65%, val/strict_accuracy: 87.35%\n",
      "Epoch 536: lr: 4.64e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0006, val/iit_loss: 0.0006, val/IIA: 95.21%, val/accuracy: 99.99%, val/strict_accuracy: 92.93%\n",
      "Epoch 537: lr: 4.63e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.67%, val/accuracy: 100.00%, val/strict_accuracy: 95.82%\n",
      "Epoch 538: lr: 4.62e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.84%, val/accuracy: 100.00%, val/strict_accuracy: 96.33%\n",
      "Epoch 539: lr: 4.61e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.13%, val/accuracy: 100.00%, val/strict_accuracy: 96.71%\n",
      "Epoch 540: lr: 4.60e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.94%, val/accuracy: 100.00%, val/strict_accuracy: 97.16%\n",
      "Epoch 541: lr: 4.59e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 97.51%, val/accuracy: 99.95%, val/strict_accuracy: 93.04%\n",
      "Epoch 542: lr: 4.58e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.10%, val/accuracy: 100.00%, val/strict_accuracy: 96.49%\n",
      "Epoch 543: lr: 4.57e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.27%, val/accuracy: 100.00%, val/strict_accuracy: 97.25%\n",
      "Epoch 544: lr: 4.56e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.61%, val/accuracy: 100.00%, val/strict_accuracy: 97.29%\n",
      "Epoch 545: lr: 4.55e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.82%, val/accuracy: 100.00%, val/strict_accuracy: 97.16%\n",
      "Epoch 546: lr: 4.54e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.53%, val/accuracy: 100.00%, val/strict_accuracy: 93.76%\n",
      "Epoch 547: lr: 4.53e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.41%, val/accuracy: 100.00%, val/strict_accuracy: 97.16%\n",
      "Epoch 548: lr: 4.52e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.99%, val/accuracy: 100.00%, val/strict_accuracy: 94.80%\n",
      "Epoch 549: lr: 4.51e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 97.53%, val/accuracy: 100.00%, val/strict_accuracy: 97.11%\n",
      "Epoch 550: lr: 4.50e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.60%, val/accuracy: 100.00%, val/strict_accuracy: 97.33%\n",
      "Epoch 551: lr: 4.49e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.21%, val/accuracy: 100.00%, val/strict_accuracy: 97.30%\n",
      "Epoch 552: lr: 4.48e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.85%, val/accuracy: 100.00%, val/strict_accuracy: 97.48%\n",
      "Epoch 553: lr: 4.47e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0021, val/IIA: 82.94%, val/accuracy: 94.58%, val/strict_accuracy: 78.10%\n",
      "Epoch 554: lr: 4.46e-04, train/iit_loss: 0.0034, train/behavior_loss: 0.0004, train/strict_loss: 0.0014, val/iit_loss: 0.0026, val/IIA: 82.10%, val/accuracy: 92.07%, val/strict_accuracy: 69.05%\n",
      "Epoch 555: lr: 4.45e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0015, val/iit_loss: 0.0004, val/IIA: 96.58%, val/accuracy: 99.98%, val/strict_accuracy: 90.89%\n",
      "Epoch 556: lr: 4.44e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.25%, val/accuracy: 99.99%, val/strict_accuracy: 95.50%\n",
      "Epoch 557: lr: 4.43e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.02%, val/accuracy: 99.99%, val/strict_accuracy: 96.43%\n",
      "Epoch 558: lr: 4.42e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.79%, val/accuracy: 100.00%, val/strict_accuracy: 96.70%\n",
      "Epoch 559: lr: 4.41e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.82%, val/accuracy: 100.00%, val/strict_accuracy: 96.92%\n",
      "Epoch 560: lr: 4.40e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.35%, val/accuracy: 100.00%, val/strict_accuracy: 96.60%\n",
      "Epoch 561: lr: 4.39e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.26%, val/accuracy: 100.00%, val/strict_accuracy: 97.13%\n",
      "Epoch 562: lr: 4.38e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.29%, val/accuracy: 100.00%, val/strict_accuracy: 97.08%\n",
      "Epoch 563: lr: 4.37e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.29%, val/accuracy: 100.00%, val/strict_accuracy: 96.89%\n",
      "Epoch 564: lr: 4.36e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.11%, val/accuracy: 100.00%, val/strict_accuracy: 97.26%\n",
      "Epoch 565: lr: 4.35e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 97.64%, val/accuracy: 100.00%, val/strict_accuracy: 95.22%\n",
      "Epoch 566: lr: 4.34e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.53%, val/accuracy: 100.00%, val/strict_accuracy: 97.25%\n",
      "Epoch 567: lr: 4.33e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.05%, val/accuracy: 100.00%, val/strict_accuracy: 96.05%\n",
      "Epoch 568: lr: 4.32e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.21%, val/accuracy: 100.00%, val/strict_accuracy: 97.17%\n",
      "Epoch 569: lr: 4.31e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.89%, val/accuracy: 100.00%, val/strict_accuracy: 94.99%\n",
      "Epoch 570: lr: 4.30e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.09%, val/accuracy: 100.00%, val/strict_accuracy: 96.33%\n",
      "Epoch 571: lr: 4.29e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.51%, val/accuracy: 99.87%, val/strict_accuracy: 92.58%\n",
      "Epoch 572: lr: 4.28e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.73%, val/accuracy: 100.00%, val/strict_accuracy: 95.93%\n",
      "Epoch 573: lr: 4.27e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 97.95%, val/accuracy: 100.00%, val/strict_accuracy: 96.48%\n",
      "Epoch 574: lr: 4.26e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0015, val/IIA: 91.60%, val/accuracy: 99.98%, val/strict_accuracy: 81.55%\n",
      "Epoch 575: lr: 4.25e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0003, val/IIA: 96.74%, val/accuracy: 100.00%, val/strict_accuracy: 95.77%\n",
      "Epoch 576: lr: 4.24e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.98%, val/accuracy: 100.00%, val/strict_accuracy: 97.04%\n",
      "Epoch 577: lr: 4.23e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.14%, val/accuracy: 100.00%, val/strict_accuracy: 97.31%\n",
      "Epoch 578: lr: 4.22e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.65%, val/accuracy: 100.00%, val/strict_accuracy: 97.34%\n",
      "Epoch 579: lr: 4.21e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.62%, val/accuracy: 100.00%, val/strict_accuracy: 97.22%\n",
      "Epoch 580: lr: 4.20e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0014, val/IIA: 89.75%, val/accuracy: 99.94%, val/strict_accuracy: 88.75%\n",
      "Epoch 581: lr: 4.19e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0003, val/IIA: 97.98%, val/accuracy: 100.00%, val/strict_accuracy: 95.95%\n",
      "Epoch 582: lr: 4.18e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.37%, val/accuracy: 100.00%, val/strict_accuracy: 97.31%\n",
      "Epoch 583: lr: 4.17e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0009, val/IIA: 93.78%, val/accuracy: 100.00%, val/strict_accuracy: 97.45%\n",
      "Epoch 584: lr: 4.16e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.03%, val/accuracy: 100.00%, val/strict_accuracy: 97.68%\n",
      "Epoch 585: lr: 4.15e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.88%, val/accuracy: 100.00%, val/strict_accuracy: 97.37%\n",
      "Epoch 586: lr: 4.14e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 92.15%, val/accuracy: 99.87%, val/strict_accuracy: 91.39%\n",
      "Epoch 587: lr: 4.13e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.91%, val/accuracy: 100.00%, val/strict_accuracy: 97.22%\n",
      "Epoch 588: lr: 4.12e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.60%, val/accuracy: 100.00%, val/strict_accuracy: 97.46%\n",
      "Epoch 589: lr: 4.11e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 93.74%, val/accuracy: 100.00%, val/strict_accuracy: 95.40%\n",
      "Epoch 590: lr: 4.10e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.54%, val/accuracy: 100.00%, val/strict_accuracy: 97.72%\n",
      "Epoch 591: lr: 4.09e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.84%, val/accuracy: 100.00%, val/strict_accuracy: 96.04%\n",
      "Epoch 592: lr: 4.08e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.25%, val/accuracy: 100.00%, val/strict_accuracy: 97.28%\n",
      "Epoch 593: lr: 4.07e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.24%, val/accuracy: 100.00%, val/strict_accuracy: 97.79%\n",
      "Epoch 594: lr: 4.06e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.93%, val/accuracy: 100.00%, val/strict_accuracy: 97.61%\n",
      "Epoch 595: lr: 4.05e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.32%, val/accuracy: 100.00%, val/strict_accuracy: 97.18%\n",
      "Epoch 596: lr: 4.04e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.71%, val/accuracy: 100.00%, val/strict_accuracy: 97.57%\n",
      "Epoch 597: lr: 4.03e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 94.80%, val/accuracy: 100.00%, val/strict_accuracy: 97.23%\n",
      "Epoch 598: lr: 4.02e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.61%, val/accuracy: 99.99%, val/strict_accuracy: 96.79%\n",
      "Epoch 599: lr: 4.01e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 95.35%, val/accuracy: 99.99%, val/strict_accuracy: 94.87%\n",
      "Epoch 600: lr: 4.00e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 97.47%, val/accuracy: 99.92%, val/strict_accuracy: 95.79%\n",
      "Epoch 601: lr: 3.99e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.64%, val/accuracy: 100.00%, val/strict_accuracy: 97.71%\n",
      "Epoch 602: lr: 3.98e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.99%, val/accuracy: 100.00%, val/strict_accuracy: 97.47%\n",
      "Epoch 603: lr: 3.97e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0010, val/IIA: 91.47%, val/accuracy: 97.78%, val/strict_accuracy: 80.21%\n",
      "Epoch 604: lr: 3.96e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0011, val/iit_loss: 0.0003, val/IIA: 97.90%, val/accuracy: 100.00%, val/strict_accuracy: 95.70%\n",
      "Epoch 605: lr: 3.95e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.93%, val/accuracy: 100.00%, val/strict_accuracy: 97.33%\n",
      "Epoch 606: lr: 3.94e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.39%, val/accuracy: 100.00%, val/strict_accuracy: 97.43%\n",
      "Epoch 607: lr: 3.93e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.66%, val/accuracy: 100.00%, val/strict_accuracy: 97.77%\n",
      "Epoch 608: lr: 3.92e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.82%, val/accuracy: 100.00%, val/strict_accuracy: 97.55%\n",
      "Epoch 609: lr: 3.91e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.41%, val/accuracy: 100.00%, val/strict_accuracy: 97.81%\n",
      "Epoch 610: lr: 3.90e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 91.27%, val/accuracy: 100.00%, val/strict_accuracy: 97.00%\n",
      "Epoch 611: lr: 3.89e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0006, val/IIA: 94.46%, val/accuracy: 100.00%, val/strict_accuracy: 86.96%\n",
      "Epoch 612: lr: 3.88e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0002, val/iit_loss: 0.0002, val/IIA: 98.97%, val/accuracy: 100.00%, val/strict_accuracy: 96.66%\n",
      "Epoch 613: lr: 3.87e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.84%, val/accuracy: 100.00%, val/strict_accuracy: 97.53%\n",
      "Epoch 614: lr: 3.86e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 96.67%, val/accuracy: 100.00%, val/strict_accuracy: 97.68%\n",
      "Epoch 615: lr: 3.85e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.92%, val/accuracy: 100.00%, val/strict_accuracy: 97.67%\n",
      "Epoch 616: lr: 3.84e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.18%, val/accuracy: 100.00%, val/strict_accuracy: 97.19%\n",
      "Epoch 617: lr: 3.83e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.39%, val/accuracy: 100.00%, val/strict_accuracy: 97.81%\n",
      "Epoch 618: lr: 3.82e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 96.68%, val/accuracy: 100.00%, val/strict_accuracy: 97.71%\n",
      "Epoch 619: lr: 3.81e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 99.06%, val/accuracy: 100.00%, val/strict_accuracy: 97.41%\n",
      "Epoch 620: lr: 3.80e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.10%, val/accuracy: 100.00%, val/strict_accuracy: 98.03%\n",
      "Epoch 621: lr: 3.79e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.23%, val/accuracy: 100.00%, val/strict_accuracy: 96.88%\n",
      "Epoch 622: lr: 3.78e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.29%, val/accuracy: 100.00%, val/strict_accuracy: 97.39%\n",
      "Epoch 623: lr: 3.77e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0005, val/IIA: 96.92%, val/accuracy: 100.00%, val/strict_accuracy: 97.31%\n",
      "Epoch 624: lr: 3.76e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.21%, val/accuracy: 100.00%, val/strict_accuracy: 97.53%\n",
      "Epoch 625: lr: 3.75e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.48%, val/accuracy: 100.00%, val/strict_accuracy: 97.68%\n",
      "Epoch 626: lr: 3.74e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.13%, val/accuracy: 100.00%, val/strict_accuracy: 97.88%\n",
      "Epoch 627: lr: 3.73e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.97%, val/accuracy: 100.00%, val/strict_accuracy: 96.53%\n",
      "Epoch 628: lr: 3.72e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.15%, val/accuracy: 100.00%, val/strict_accuracy: 96.63%\n",
      "Epoch 629: lr: 3.71e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 98.23%, val/accuracy: 99.99%, val/strict_accuracy: 93.63%\n",
      "Epoch 630: lr: 3.70e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.45%, val/accuracy: 100.00%, val/strict_accuracy: 98.01%\n",
      "Epoch 631: lr: 3.69e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 96.35%, val/accuracy: 100.00%, val/strict_accuracy: 96.50%\n",
      "Epoch 632: lr: 3.68e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.33%, val/accuracy: 100.00%, val/strict_accuracy: 97.81%\n",
      "Epoch 633: lr: 3.67e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.05%, val/accuracy: 100.00%, val/strict_accuracy: 97.35%\n",
      "Epoch 634: lr: 3.66e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.34%, val/accuracy: 100.00%, val/strict_accuracy: 95.31%\n",
      "Epoch 635: lr: 3.65e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 99.03%, val/accuracy: 100.00%, val/strict_accuracy: 97.25%\n",
      "Epoch 636: lr: 3.64e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.25%, val/accuracy: 100.00%, val/strict_accuracy: 97.72%\n",
      "Epoch 637: lr: 3.63e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 97.62%, val/accuracy: 100.00%, val/strict_accuracy: 96.99%\n",
      "Epoch 638: lr: 3.62e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.21%, val/accuracy: 100.00%, val/strict_accuracy: 98.25%\n",
      "Epoch 639: lr: 3.61e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0004, val/IIA: 96.73%, val/accuracy: 100.00%, val/strict_accuracy: 93.50%\n",
      "Epoch 640: lr: 3.60e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0013, val/iit_loss: 0.0004, val/IIA: 96.75%, val/accuracy: 99.99%, val/strict_accuracy: 89.37%\n",
      "Epoch 641: lr: 3.59e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0007, val/iit_loss: 0.0002, val/IIA: 98.75%, val/accuracy: 99.99%, val/strict_accuracy: 95.27%\n",
      "Epoch 642: lr: 3.58e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.17%, val/accuracy: 100.00%, val/strict_accuracy: 97.24%\n",
      "Epoch 643: lr: 3.57e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.35%, val/accuracy: 100.00%, val/strict_accuracy: 97.66%\n",
      "Epoch 644: lr: 3.56e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.51%, val/accuracy: 100.00%, val/strict_accuracy: 97.81%\n",
      "Epoch 645: lr: 3.55e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.45%, val/accuracy: 100.00%, val/strict_accuracy: 97.81%\n",
      "Epoch 646: lr: 3.54e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.40%, val/accuracy: 100.00%, val/strict_accuracy: 97.66%\n",
      "Epoch 647: lr: 3.53e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.19%, val/accuracy: 100.00%, val/strict_accuracy: 95.88%\n",
      "Epoch 648: lr: 3.52e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 99.03%, val/accuracy: 100.00%, val/strict_accuracy: 97.96%\n",
      "Epoch 649: lr: 3.51e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.59%, val/accuracy: 100.00%, val/strict_accuracy: 97.58%\n",
      "Epoch 650: lr: 3.50e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.24%, val/accuracy: 100.00%, val/strict_accuracy: 97.92%\n",
      "Epoch 651: lr: 3.49e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0004, val/IIA: 96.53%, val/accuracy: 100.00%, val/strict_accuracy: 96.08%\n",
      "Epoch 652: lr: 3.48e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 96.59%, val/accuracy: 100.00%, val/strict_accuracy: 96.79%\n",
      "Epoch 653: lr: 3.47e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0011, val/IIA: 91.19%, val/accuracy: 99.85%, val/strict_accuracy: 88.13%\n",
      "Epoch 654: lr: 3.46e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0009, val/iit_loss: 0.0005, val/IIA: 96.09%, val/accuracy: 100.00%, val/strict_accuracy: 92.04%\n",
      "Epoch 655: lr: 3.45e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 99.13%, val/accuracy: 100.00%, val/strict_accuracy: 97.12%\n",
      "Epoch 656: lr: 3.44e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.24%, val/accuracy: 100.00%, val/strict_accuracy: 97.75%\n",
      "Epoch 657: lr: 3.43e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.53%, val/accuracy: 100.00%, val/strict_accuracy: 97.83%\n",
      "Epoch 658: lr: 3.42e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.39%, val/accuracy: 100.00%, val/strict_accuracy: 97.82%\n",
      "Epoch 659: lr: 3.41e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.24%, val/accuracy: 100.00%, val/strict_accuracy: 97.86%\n",
      "Epoch 660: lr: 3.40e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 97.70%, val/accuracy: 100.00%, val/strict_accuracy: 98.04%\n",
      "Epoch 661: lr: 3.39e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.51%, val/accuracy: 100.00%, val/strict_accuracy: 97.89%\n",
      "Epoch 662: lr: 3.38e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.25%, val/accuracy: 100.00%, val/strict_accuracy: 97.55%\n",
      "Epoch 663: lr: 3.37e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.62%, val/accuracy: 100.00%, val/strict_accuracy: 96.91%\n",
      "Epoch 664: lr: 3.36e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0009, val/iit_loss: 0.0009, val/IIA: 93.32%, val/accuracy: 100.00%, val/strict_accuracy: 88.59%\n",
      "Epoch 665: lr: 3.35e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0003, val/IIA: 98.06%, val/accuracy: 100.00%, val/strict_accuracy: 95.60%\n",
      "Epoch 666: lr: 3.34e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.41%, val/accuracy: 100.00%, val/strict_accuracy: 97.49%\n",
      "Epoch 667: lr: 3.33e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.27%, val/accuracy: 100.00%, val/strict_accuracy: 97.75%\n",
      "Epoch 668: lr: 3.32e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.59%, val/accuracy: 100.00%, val/strict_accuracy: 97.95%\n",
      "Epoch 669: lr: 3.31e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.35%, val/accuracy: 100.00%, val/strict_accuracy: 97.85%\n",
      "Epoch 670: lr: 3.30e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.81%, val/accuracy: 100.00%, val/strict_accuracy: 97.84%\n",
      "Epoch 671: lr: 3.29e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.36%, val/accuracy: 100.00%, val/strict_accuracy: 96.28%\n",
      "Epoch 672: lr: 3.28e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.02%, val/accuracy: 100.00%, val/strict_accuracy: 97.89%\n",
      "Epoch 673: lr: 3.27e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.03%, val/accuracy: 100.00%, val/strict_accuracy: 97.99%\n",
      "Epoch 674: lr: 3.26e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.16%, val/accuracy: 100.00%, val/strict_accuracy: 97.43%\n",
      "Epoch 675: lr: 3.25e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.22%, val/accuracy: 100.00%, val/strict_accuracy: 98.13%\n",
      "Epoch 676: lr: 3.24e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 97.67%, val/accuracy: 100.00%, val/strict_accuracy: 97.58%\n",
      "Epoch 677: lr: 3.23e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.29%, val/accuracy: 100.00%, val/strict_accuracy: 97.75%\n",
      "Epoch 678: lr: 3.22e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.12%, val/accuracy: 100.00%, val/strict_accuracy: 97.98%\n",
      "Epoch 679: lr: 3.21e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0004, val/IIA: 97.26%, val/accuracy: 99.82%, val/strict_accuracy: 92.82%\n",
      "Epoch 680: lr: 3.20e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0003, val/IIA: 98.09%, val/accuracy: 100.00%, val/strict_accuracy: 97.80%\n",
      "Epoch 681: lr: 3.19e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0030, val/IIA: 90.03%, val/accuracy: 100.00%, val/strict_accuracy: 92.16%\n",
      "Epoch 682: lr: 3.18e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0002, val/IIA: 98.26%, val/accuracy: 100.00%, val/strict_accuracy: 97.72%\n",
      "Epoch 683: lr: 3.17e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.84%, val/accuracy: 100.00%, val/strict_accuracy: 97.81%\n",
      "Epoch 684: lr: 3.16e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.38%, val/accuracy: 100.00%, val/strict_accuracy: 98.23%\n",
      "Epoch 685: lr: 3.15e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.40%, val/accuracy: 100.00%, val/strict_accuracy: 98.25%\n",
      "Epoch 686: lr: 3.14e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.73%, val/accuracy: 100.00%, val/strict_accuracy: 98.04%\n",
      "Epoch 687: lr: 3.13e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 96.93%, val/accuracy: 100.00%, val/strict_accuracy: 97.95%\n",
      "Epoch 688: lr: 3.12e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0009, val/IIA: 90.75%, val/accuracy: 99.99%, val/strict_accuracy: 86.19%\n",
      "Epoch 689: lr: 3.11e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0002, val/IIA: 99.04%, val/accuracy: 100.00%, val/strict_accuracy: 95.03%\n",
      "Epoch 690: lr: 3.10e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.63%, val/accuracy: 100.00%, val/strict_accuracy: 97.43%\n",
      "Epoch 691: lr: 3.09e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.62%, val/accuracy: 100.00%, val/strict_accuracy: 98.06%\n",
      "Epoch 692: lr: 3.08e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.40%, val/accuracy: 100.00%, val/strict_accuracy: 97.93%\n",
      "Epoch 693: lr: 3.07e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.21%, val/accuracy: 100.00%, val/strict_accuracy: 98.06%\n",
      "Epoch 694: lr: 3.06e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 97.51%, val/accuracy: 100.00%, val/strict_accuracy: 97.05%\n",
      "Epoch 695: lr: 3.05e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.46%, val/accuracy: 100.00%, val/strict_accuracy: 98.10%\n",
      "Epoch 696: lr: 3.04e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 96.99%, val/accuracy: 100.00%, val/strict_accuracy: 97.80%\n",
      "Epoch 697: lr: 3.03e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.40%, val/accuracy: 100.00%, val/strict_accuracy: 98.30%\n",
      "Epoch 698: lr: 3.02e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.49%, val/accuracy: 100.00%, val/strict_accuracy: 98.25%\n",
      "Epoch 699: lr: 3.01e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.27%, val/accuracy: 100.00%, val/strict_accuracy: 97.79%\n",
      "Epoch 700: lr: 3.00e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.76%, val/accuracy: 100.00%, val/strict_accuracy: 97.75%\n",
      "Epoch 701: lr: 2.99e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.91%, val/accuracy: 100.00%, val/strict_accuracy: 98.18%\n",
      "Epoch 702: lr: 2.98e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.30%, val/accuracy: 100.00%, val/strict_accuracy: 97.01%\n",
      "Epoch 703: lr: 2.97e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.05%, val/accuracy: 100.00%, val/strict_accuracy: 98.05%\n",
      "Epoch 704: lr: 2.96e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.26%, val/accuracy: 100.00%, val/strict_accuracy: 98.36%\n",
      "Epoch 705: lr: 2.95e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.10%, val/accuracy: 100.00%, val/strict_accuracy: 97.52%\n",
      "Epoch 706: lr: 2.94e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 99.10%, val/accuracy: 100.00%, val/strict_accuracy: 97.98%\n",
      "Epoch 707: lr: 2.93e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.53%, val/accuracy: 100.00%, val/strict_accuracy: 97.34%\n",
      "Epoch 708: lr: 2.92e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.36%, val/accuracy: 100.00%, val/strict_accuracy: 97.99%\n",
      "Epoch 709: lr: 2.91e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.22%, val/accuracy: 100.00%, val/strict_accuracy: 97.18%\n",
      "Epoch 710: lr: 2.90e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.01%, val/accuracy: 100.00%, val/strict_accuracy: 97.30%\n",
      "Epoch 711: lr: 2.89e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0006, val/IIA: 96.93%, val/accuracy: 99.99%, val/strict_accuracy: 92.76%\n",
      "Epoch 712: lr: 2.88e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0004, val/IIA: 97.19%, val/accuracy: 100.00%, val/strict_accuracy: 94.26%\n",
      "Epoch 713: lr: 2.87e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0003, val/iit_loss: 0.0002, val/IIA: 98.94%, val/accuracy: 100.00%, val/strict_accuracy: 95.95%\n",
      "Epoch 714: lr: 2.86e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.15%, val/accuracy: 100.00%, val/strict_accuracy: 97.03%\n",
      "Epoch 715: lr: 2.85e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.19%, val/accuracy: 100.00%, val/strict_accuracy: 97.66%\n",
      "Epoch 716: lr: 2.84e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.45%, val/accuracy: 100.00%, val/strict_accuracy: 97.96%\n",
      "Epoch 717: lr: 2.83e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.46%, val/accuracy: 100.00%, val/strict_accuracy: 97.94%\n",
      "Epoch 718: lr: 2.82e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.49%, val/accuracy: 100.00%, val/strict_accuracy: 98.10%\n",
      "Epoch 719: lr: 2.81e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.52%, val/accuracy: 100.00%, val/strict_accuracy: 98.21%\n",
      "Epoch 720: lr: 2.80e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.46%, val/accuracy: 100.00%, val/strict_accuracy: 98.19%\n",
      "Epoch 721: lr: 2.79e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.03%, val/accuracy: 100.00%, val/strict_accuracy: 98.16%\n",
      "Epoch 722: lr: 2.78e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.37%, val/accuracy: 100.00%, val/strict_accuracy: 97.95%\n",
      "Epoch 723: lr: 2.77e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.44%, val/accuracy: 100.00%, val/strict_accuracy: 98.23%\n",
      "Epoch 724: lr: 2.76e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 96.92%, val/accuracy: 100.00%, val/strict_accuracy: 98.35%\n",
      "Epoch 725: lr: 2.75e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.60%, val/accuracy: 100.00%, val/strict_accuracy: 98.39%\n",
      "Epoch 726: lr: 2.74e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0002, val/IIA: 98.95%, val/accuracy: 100.00%, val/strict_accuracy: 97.95%\n",
      "Epoch 727: lr: 2.73e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.64%, val/accuracy: 100.00%, val/strict_accuracy: 98.47%\n",
      "Epoch 728: lr: 2.72e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.20%, val/accuracy: 100.00%, val/strict_accuracy: 98.22%\n",
      "Epoch 729: lr: 2.71e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.70%, val/accuracy: 100.00%, val/strict_accuracy: 98.24%\n",
      "Epoch 730: lr: 2.70e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.17%, val/accuracy: 100.00%, val/strict_accuracy: 97.61%\n",
      "Epoch 731: lr: 2.69e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.45%, val/accuracy: 100.00%, val/strict_accuracy: 98.37%\n",
      "Epoch 732: lr: 2.68e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 97.63%, val/accuracy: 100.00%, val/strict_accuracy: 97.78%\n",
      "Epoch 733: lr: 2.67e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.83%, val/accuracy: 100.00%, val/strict_accuracy: 98.27%\n",
      "Epoch 734: lr: 2.66e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 98.44%, val/accuracy: 100.00%, val/strict_accuracy: 96.55%\n",
      "Epoch 735: lr: 2.65e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0007, val/iit_loss: 0.0003, val/IIA: 97.31%, val/accuracy: 99.97%, val/strict_accuracy: 92.86%\n",
      "Epoch 736: lr: 2.64e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.52%, val/accuracy: 100.00%, val/strict_accuracy: 97.76%\n",
      "Epoch 737: lr: 2.63e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.43%, val/accuracy: 100.00%, val/strict_accuracy: 98.41%\n",
      "Epoch 738: lr: 2.62e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.39%, val/accuracy: 100.00%, val/strict_accuracy: 98.40%\n",
      "Epoch 739: lr: 2.61e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.16%, val/accuracy: 100.00%, val/strict_accuracy: 98.05%\n",
      "Epoch 740: lr: 2.60e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.85%, val/accuracy: 100.00%, val/strict_accuracy: 98.37%\n",
      "Epoch 741: lr: 2.59e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 96.62%, val/accuracy: 100.00%, val/strict_accuracy: 97.81%\n",
      "Epoch 742: lr: 2.58e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.43%, val/accuracy: 100.00%, val/strict_accuracy: 98.24%\n",
      "Epoch 743: lr: 2.57e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.17%, val/accuracy: 100.00%, val/strict_accuracy: 98.44%\n",
      "Epoch 744: lr: 2.56e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.69%, val/accuracy: 100.00%, val/strict_accuracy: 98.42%\n",
      "Epoch 745: lr: 2.55e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.47%, val/accuracy: 100.00%, val/strict_accuracy: 98.23%\n",
      "Epoch 746: lr: 2.54e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.17%, val/accuracy: 100.00%, val/strict_accuracy: 98.15%\n",
      "Epoch 747: lr: 2.53e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.46%, val/accuracy: 100.00%, val/strict_accuracy: 98.22%\n",
      "Epoch 748: lr: 2.52e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.48%, val/accuracy: 100.00%, val/strict_accuracy: 98.53%\n",
      "Epoch 749: lr: 2.51e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.53%, val/accuracy: 100.00%, val/strict_accuracy: 98.45%\n",
      "Epoch 750: lr: 2.50e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.45%, val/accuracy: 100.00%, val/strict_accuracy: 98.29%\n",
      "Epoch 751: lr: 2.49e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.34%, val/accuracy: 100.00%, val/strict_accuracy: 98.15%\n",
      "Epoch 752: lr: 2.48e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 97.48%, val/accuracy: 100.00%, val/strict_accuracy: 96.95%\n",
      "Epoch 753: lr: 2.47e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0002, val/IIA: 98.66%, val/accuracy: 100.00%, val/strict_accuracy: 97.39%\n",
      "Epoch 754: lr: 2.46e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.36%, val/accuracy: 100.00%, val/strict_accuracy: 97.77%\n",
      "Epoch 755: lr: 2.45e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.52%, val/accuracy: 100.00%, val/strict_accuracy: 98.30%\n",
      "Epoch 756: lr: 2.44e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.63%, val/accuracy: 100.00%, val/strict_accuracy: 98.54%\n",
      "Epoch 757: lr: 2.43e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.36%, val/accuracy: 100.00%, val/strict_accuracy: 98.53%\n",
      "Epoch 758: lr: 2.42e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.49%, val/accuracy: 100.00%, val/strict_accuracy: 98.54%\n",
      "Epoch 759: lr: 2.41e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.61%, val/accuracy: 100.00%, val/strict_accuracy: 98.51%\n",
      "Epoch 760: lr: 2.40e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.38%, val/accuracy: 100.00%, val/strict_accuracy: 98.36%\n",
      "Epoch 761: lr: 2.39e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.20%, val/accuracy: 100.00%, val/strict_accuracy: 98.39%\n",
      "Epoch 762: lr: 2.38e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.59%, val/accuracy: 100.00%, val/strict_accuracy: 98.56%\n",
      "Epoch 763: lr: 2.37e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.71%, val/accuracy: 100.00%, val/strict_accuracy: 98.62%\n",
      "Epoch 764: lr: 2.36e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 97.41%, val/accuracy: 100.00%, val/strict_accuracy: 97.32%\n",
      "Epoch 765: lr: 2.35e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.71%, val/accuracy: 100.00%, val/strict_accuracy: 98.53%\n",
      "Epoch 766: lr: 2.34e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.51%, val/accuracy: 100.00%, val/strict_accuracy: 98.62%\n",
      "Epoch 767: lr: 2.33e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.36%, val/accuracy: 100.00%, val/strict_accuracy: 98.49%\n",
      "Epoch 768: lr: 2.32e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0009, val/IIA: 93.30%, val/accuracy: 100.00%, val/strict_accuracy: 88.60%\n",
      "Epoch 769: lr: 2.31e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0006, val/iit_loss: 0.0002, val/IIA: 99.15%, val/accuracy: 100.00%, val/strict_accuracy: 96.61%\n",
      "Epoch 770: lr: 2.30e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.64%, val/accuracy: 100.00%, val/strict_accuracy: 98.00%\n",
      "Epoch 771: lr: 2.29e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.61%, val/accuracy: 100.00%, val/strict_accuracy: 98.29%\n",
      "Epoch 772: lr: 2.28e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.58%, val/accuracy: 100.00%, val/strict_accuracy: 98.44%\n",
      "Epoch 773: lr: 2.27e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.67%, val/accuracy: 100.00%, val/strict_accuracy: 98.49%\n",
      "Epoch 774: lr: 2.26e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.62%, val/accuracy: 100.00%, val/strict_accuracy: 98.48%\n",
      "Epoch 775: lr: 2.25e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.47%, val/accuracy: 100.00%, val/strict_accuracy: 98.47%\n",
      "Epoch 776: lr: 2.24e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.50%, val/accuracy: 100.00%, val/strict_accuracy: 98.52%\n",
      "Epoch 777: lr: 2.23e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.02%, val/accuracy: 100.00%, val/strict_accuracy: 98.48%\n",
      "Epoch 778: lr: 2.22e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.58%, val/accuracy: 100.00%, val/strict_accuracy: 98.45%\n",
      "Epoch 779: lr: 2.21e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.50%, val/accuracy: 100.00%, val/strict_accuracy: 98.33%\n",
      "Epoch 780: lr: 2.20e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.46%, val/accuracy: 100.00%, val/strict_accuracy: 98.26%\n",
      "Epoch 781: lr: 2.19e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 96.80%, val/accuracy: 100.00%, val/strict_accuracy: 98.46%\n",
      "Epoch 782: lr: 2.18e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.64%, val/accuracy: 100.00%, val/strict_accuracy: 98.56%\n",
      "Epoch 783: lr: 2.17e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.63%, val/accuracy: 100.00%, val/strict_accuracy: 98.66%\n",
      "Epoch 784: lr: 2.16e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.48%, val/accuracy: 100.00%, val/strict_accuracy: 98.49%\n",
      "Epoch 785: lr: 2.15e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.48%, val/accuracy: 100.00%, val/strict_accuracy: 98.66%\n",
      "Epoch 786: lr: 2.14e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.49%, val/accuracy: 100.00%, val/strict_accuracy: 97.75%\n",
      "Epoch 787: lr: 2.13e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.64%, val/accuracy: 100.00%, val/strict_accuracy: 98.59%\n",
      "Epoch 788: lr: 2.12e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.39%, val/accuracy: 100.00%, val/strict_accuracy: 98.50%\n",
      "Epoch 789: lr: 2.11e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.31%, val/accuracy: 100.00%, val/strict_accuracy: 98.65%\n",
      "Epoch 790: lr: 2.10e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.33%, val/accuracy: 100.00%, val/strict_accuracy: 98.22%\n",
      "Epoch 791: lr: 2.09e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.70%, val/accuracy: 100.00%, val/strict_accuracy: 98.56%\n",
      "Epoch 792: lr: 2.08e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.36%, val/accuracy: 100.00%, val/strict_accuracy: 98.63%\n",
      "Epoch 793: lr: 2.07e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.31%, val/accuracy: 100.00%, val/strict_accuracy: 97.81%\n",
      "Epoch 794: lr: 2.06e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 97.84%, val/accuracy: 100.00%, val/strict_accuracy: 95.69%\n",
      "Epoch 795: lr: 2.05e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0005, val/iit_loss: 0.0001, val/IIA: 99.23%, val/accuracy: 100.00%, val/strict_accuracy: 97.61%\n",
      "Epoch 796: lr: 2.04e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.59%, val/accuracy: 100.00%, val/strict_accuracy: 98.40%\n",
      "Epoch 797: lr: 2.03e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.63%, val/accuracy: 100.00%, val/strict_accuracy: 98.48%\n",
      "Epoch 798: lr: 2.02e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.65%, val/accuracy: 100.00%, val/strict_accuracy: 98.71%\n",
      "Epoch 799: lr: 2.01e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.53%, val/accuracy: 100.00%, val/strict_accuracy: 98.64%\n",
      "Epoch 800: lr: 2.00e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.65%, val/accuracy: 100.00%, val/strict_accuracy: 98.74%\n",
      "Epoch 801: lr: 1.99e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 98.67%\n",
      "Epoch 802: lr: 1.98e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.02%, val/accuracy: 100.00%, val/strict_accuracy: 98.70%\n",
      "Epoch 803: lr: 1.97e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.31%, val/accuracy: 100.00%, val/strict_accuracy: 98.64%\n",
      "Epoch 804: lr: 1.96e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.75%, val/accuracy: 100.00%, val/strict_accuracy: 98.65%\n",
      "Epoch 805: lr: 1.95e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.66%, val/accuracy: 100.00%, val/strict_accuracy: 98.74%\n",
      "Epoch 806: lr: 1.94e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.60%, val/accuracy: 100.00%, val/strict_accuracy: 98.74%\n",
      "Epoch 807: lr: 1.93e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.51%, val/accuracy: 100.00%, val/strict_accuracy: 98.72%\n",
      "Epoch 808: lr: 1.92e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 98.79%\n",
      "Epoch 809: lr: 1.91e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.50%, val/accuracy: 100.00%, val/strict_accuracy: 98.53%\n",
      "Epoch 810: lr: 1.90e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.68%, val/accuracy: 100.00%, val/strict_accuracy: 98.54%\n",
      "Epoch 811: lr: 1.89e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 97.07%, val/accuracy: 100.00%, val/strict_accuracy: 98.23%\n",
      "Epoch 812: lr: 1.88e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.82%, val/accuracy: 100.00%, val/strict_accuracy: 98.62%\n",
      "Epoch 813: lr: 1.87e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.67%, val/accuracy: 100.00%, val/strict_accuracy: 98.70%\n",
      "Epoch 814: lr: 1.86e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.41%, val/accuracy: 100.00%, val/strict_accuracy: 98.66%\n",
      "Epoch 815: lr: 1.85e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.46%, val/accuracy: 100.00%, val/strict_accuracy: 98.46%\n",
      "Epoch 816: lr: 1.84e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.60%, val/accuracy: 100.00%, val/strict_accuracy: 98.72%\n",
      "Epoch 817: lr: 1.83e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.76%, val/accuracy: 100.00%, val/strict_accuracy: 98.71%\n",
      "Epoch 818: lr: 1.82e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 98.80%\n",
      "Epoch 819: lr: 1.81e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 98.67%\n",
      "Epoch 820: lr: 1.80e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.39%, val/accuracy: 100.00%, val/strict_accuracy: 98.67%\n",
      "Epoch 821: lr: 1.79e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.47%, val/accuracy: 100.00%, val/strict_accuracy: 98.69%\n",
      "Epoch 822: lr: 1.78e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.56%, val/accuracy: 100.00%, val/strict_accuracy: 98.17%\n",
      "Epoch 823: lr: 1.77e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0003, val/IIA: 97.02%, val/accuracy: 100.00%, val/strict_accuracy: 96.94%\n",
      "Epoch 824: lr: 1.76e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0001, val/IIA: 99.69%, val/accuracy: 100.00%, val/strict_accuracy: 98.56%\n",
      "Epoch 825: lr: 1.75e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.58%, val/accuracy: 100.00%, val/strict_accuracy: 98.47%\n",
      "Epoch 826: lr: 1.74e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.69%, val/accuracy: 100.00%, val/strict_accuracy: 98.76%\n",
      "Epoch 827: lr: 1.73e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.72%, val/accuracy: 100.00%, val/strict_accuracy: 98.80%\n",
      "Epoch 828: lr: 1.72e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.71%, val/accuracy: 100.00%, val/strict_accuracy: 98.75%\n",
      "Epoch 829: lr: 1.71e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 98.78%\n",
      "Epoch 830: lr: 1.70e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.58%, val/accuracy: 100.00%, val/strict_accuracy: 98.87%\n",
      "Epoch 831: lr: 1.69e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.68%, val/accuracy: 100.00%, val/strict_accuracy: 98.72%\n",
      "Epoch 832: lr: 1.68e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.72%, val/accuracy: 100.00%, val/strict_accuracy: 98.81%\n",
      "Epoch 833: lr: 1.67e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.47%, val/accuracy: 100.00%, val/strict_accuracy: 98.83%\n",
      "Epoch 834: lr: 1.66e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 98.89%, val/accuracy: 100.00%, val/strict_accuracy: 98.00%\n",
      "Epoch 835: lr: 1.65e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.67%, val/accuracy: 100.00%, val/strict_accuracy: 98.82%\n",
      "Epoch 836: lr: 1.64e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.64%, val/accuracy: 100.00%, val/strict_accuracy: 98.86%\n",
      "Epoch 837: lr: 1.63e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.54%, val/accuracy: 100.00%, val/strict_accuracy: 98.75%\n",
      "Epoch 838: lr: 1.62e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.43%, val/accuracy: 100.00%, val/strict_accuracy: 98.67%\n",
      "Epoch 839: lr: 1.61e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.79%, val/accuracy: 100.00%, val/strict_accuracy: 98.81%\n",
      "Epoch 840: lr: 1.60e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 98.70%\n",
      "Epoch 841: lr: 1.59e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.51%, val/accuracy: 100.00%, val/strict_accuracy: 98.03%\n",
      "Epoch 842: lr: 1.58e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.67%, val/accuracy: 100.00%, val/strict_accuracy: 98.52%\n",
      "Epoch 843: lr: 1.57e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.66%, val/accuracy: 100.00%, val/strict_accuracy: 98.93%\n",
      "Epoch 844: lr: 1.56e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.42%, val/accuracy: 100.00%, val/strict_accuracy: 98.59%\n",
      "Epoch 845: lr: 1.55e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0001, val/IIA: 99.65%, val/accuracy: 100.00%, val/strict_accuracy: 98.14%\n",
      "Epoch 846: lr: 1.54e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 98.57%\n",
      "Epoch 847: lr: 1.53e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.66%, val/accuracy: 100.00%, val/strict_accuracy: 98.74%\n",
      "Epoch 848: lr: 1.52e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.59%, val/accuracy: 100.00%, val/strict_accuracy: 98.80%\n",
      "Epoch 849: lr: 1.51e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.75%, val/accuracy: 100.00%, val/strict_accuracy: 98.86%\n",
      "Epoch 850: lr: 1.50e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.66%, val/accuracy: 100.00%, val/strict_accuracy: 98.86%\n",
      "Epoch 851: lr: 1.49e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.73%, val/accuracy: 100.00%, val/strict_accuracy: 98.82%\n",
      "Epoch 852: lr: 1.48e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.55%, val/accuracy: 100.00%, val/strict_accuracy: 98.77%\n",
      "Epoch 853: lr: 1.47e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.18%, val/accuracy: 100.00%, val/strict_accuracy: 98.68%\n",
      "Epoch 854: lr: 1.46e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.75%, val/accuracy: 100.00%, val/strict_accuracy: 98.94%\n",
      "Epoch 855: lr: 1.45e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.68%, val/accuracy: 100.00%, val/strict_accuracy: 98.80%\n",
      "Epoch 856: lr: 1.44e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.74%, val/accuracy: 100.00%, val/strict_accuracy: 98.83%\n",
      "Epoch 857: lr: 1.43e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.70%, val/accuracy: 100.00%, val/strict_accuracy: 98.88%\n",
      "Epoch 858: lr: 1.42e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.50%, val/accuracy: 100.00%, val/strict_accuracy: 98.83%\n",
      "Epoch 859: lr: 1.41e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.64%, val/accuracy: 100.00%, val/strict_accuracy: 98.79%\n",
      "Epoch 860: lr: 1.40e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.66%, val/accuracy: 100.00%, val/strict_accuracy: 98.84%\n",
      "Epoch 861: lr: 1.39e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.69%, val/accuracy: 100.00%, val/strict_accuracy: 98.89%\n",
      "Epoch 862: lr: 1.38e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.56%, val/accuracy: 100.00%, val/strict_accuracy: 98.77%\n",
      "Epoch 863: lr: 1.37e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.34%, val/accuracy: 100.00%, val/strict_accuracy: 98.87%\n",
      "Epoch 864: lr: 1.36e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 99.11%, val/accuracy: 100.00%, val/strict_accuracy: 98.28%\n",
      "Epoch 865: lr: 1.35e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.61%, val/accuracy: 100.00%, val/strict_accuracy: 98.86%\n",
      "Epoch 866: lr: 1.34e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.74%, val/accuracy: 100.00%, val/strict_accuracy: 98.75%\n",
      "Epoch 867: lr: 1.33e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0002, val/IIA: 98.79%, val/accuracy: 100.00%, val/strict_accuracy: 98.83%\n",
      "Epoch 868: lr: 1.32e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.40%, val/accuracy: 100.00%, val/strict_accuracy: 96.01%\n",
      "Epoch 869: lr: 1.31e-04, train/iit_loss: 0.0002, train/behavior_loss: 0.0000, train/strict_loss: 0.0001, val/iit_loss: 0.0001, val/IIA: 99.55%, val/accuracy: 100.00%, val/strict_accuracy: 98.71%\n",
      "Epoch 870: lr: 1.30e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.68%, val/accuracy: 100.00%, val/strict_accuracy: 98.85%\n",
      "Epoch 871: lr: 1.29e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 98.90%\n",
      "Epoch 872: lr: 1.28e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 98.93%\n",
      "Epoch 873: lr: 1.27e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.73%, val/accuracy: 100.00%, val/strict_accuracy: 98.89%\n",
      "Epoch 874: lr: 1.26e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.69%, val/accuracy: 100.00%, val/strict_accuracy: 98.95%\n",
      "Epoch 875: lr: 1.25e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.72%, val/accuracy: 100.00%, val/strict_accuracy: 98.87%\n",
      "Epoch 876: lr: 1.24e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.69%, val/accuracy: 100.00%, val/strict_accuracy: 98.80%\n",
      "Epoch 877: lr: 1.23e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.61%, val/accuracy: 100.00%, val/strict_accuracy: 98.96%\n",
      "Epoch 878: lr: 1.22e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.59%, val/accuracy: 100.00%, val/strict_accuracy: 98.89%\n",
      "Epoch 879: lr: 1.21e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.69%, val/accuracy: 100.00%, val/strict_accuracy: 98.95%\n",
      "Epoch 880: lr: 1.20e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.66%, val/accuracy: 100.00%, val/strict_accuracy: 98.85%\n",
      "Epoch 881: lr: 1.19e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 98.87%\n",
      "Epoch 882: lr: 1.18e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.61%, val/accuracy: 100.00%, val/strict_accuracy: 98.90%\n",
      "Epoch 883: lr: 1.17e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.75%, val/accuracy: 100.00%, val/strict_accuracy: 98.91%\n",
      "Epoch 884: lr: 1.16e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.74%, val/accuracy: 100.00%, val/strict_accuracy: 98.77%\n",
      "Epoch 885: lr: 1.15e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.44%, val/accuracy: 100.00%, val/strict_accuracy: 98.81%\n",
      "Epoch 886: lr: 1.14e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.75%, val/accuracy: 100.00%, val/strict_accuracy: 99.05%\n",
      "Epoch 887: lr: 1.13e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.83%, val/accuracy: 100.00%, val/strict_accuracy: 98.98%\n",
      "Epoch 888: lr: 1.12e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.60%, val/accuracy: 100.00%, val/strict_accuracy: 98.84%\n",
      "Epoch 889: lr: 1.11e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.61%, val/accuracy: 100.00%, val/strict_accuracy: 98.90%\n",
      "Epoch 890: lr: 1.10e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.76%, val/accuracy: 100.00%, val/strict_accuracy: 98.93%\n",
      "Epoch 891: lr: 1.09e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.67%, val/accuracy: 100.00%, val/strict_accuracy: 98.83%\n",
      "Epoch 892: lr: 1.08e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.70%, val/accuracy: 100.00%, val/strict_accuracy: 98.94%\n",
      "Epoch 893: lr: 1.07e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.69%, val/accuracy: 100.00%, val/strict_accuracy: 98.94%\n",
      "Epoch 894: lr: 1.06e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.75%, val/accuracy: 100.00%, val/strict_accuracy: 98.88%\n",
      "Epoch 895: lr: 1.05e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.76%, val/accuracy: 100.00%, val/strict_accuracy: 98.98%\n",
      "Epoch 896: lr: 1.04e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.72%, val/accuracy: 100.00%, val/strict_accuracy: 98.98%\n",
      "Epoch 897: lr: 1.03e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 99.03%\n",
      "Epoch 898: lr: 1.02e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 98.92%\n",
      "Epoch 899: lr: 1.01e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 98.99%\n",
      "Epoch 900: lr: 1.00e-04, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.00%\n",
      "Epoch 901: lr: 9.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 99.01%\n",
      "Epoch 902: lr: 9.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.67%, val/accuracy: 100.00%, val/strict_accuracy: 98.91%\n",
      "Epoch 903: lr: 9.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 98.92%\n",
      "Epoch 904: lr: 9.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 98.88%\n",
      "Epoch 905: lr: 9.50e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.72%, val/accuracy: 100.00%, val/strict_accuracy: 99.02%\n",
      "Epoch 906: lr: 9.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.76%, val/accuracy: 100.00%, val/strict_accuracy: 98.73%\n",
      "Epoch 907: lr: 9.30e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 98.89%\n",
      "Epoch 908: lr: 9.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.22%, val/accuracy: 100.00%, val/strict_accuracy: 98.98%\n",
      "Epoch 909: lr: 9.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.73%, val/accuracy: 100.00%, val/strict_accuracy: 98.88%\n",
      "Epoch 910: lr: 9.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 98.90%\n",
      "Epoch 911: lr: 8.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.86%, val/accuracy: 100.00%, val/strict_accuracy: 98.92%\n",
      "Epoch 912: lr: 8.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.68%, val/accuracy: 100.00%, val/strict_accuracy: 98.74%\n",
      "Epoch 913: lr: 8.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.03%\n",
      "Epoch 914: lr: 8.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.63%, val/accuracy: 100.00%, val/strict_accuracy: 98.96%\n",
      "Epoch 915: lr: 8.50e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 916: lr: 8.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.79%, val/accuracy: 100.00%, val/strict_accuracy: 98.99%\n",
      "Epoch 917: lr: 8.30e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.76%, val/accuracy: 100.00%, val/strict_accuracy: 98.68%\n",
      "Epoch 918: lr: 8.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.01%\n",
      "Epoch 919: lr: 8.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 98.99%\n",
      "Epoch 920: lr: 8.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 98.97%\n",
      "Epoch 921: lr: 7.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 99.02%\n",
      "Epoch 922: lr: 7.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.86%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 923: lr: 7.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.75%, val/accuracy: 100.00%, val/strict_accuracy: 98.94%\n",
      "Epoch 924: lr: 7.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.73%, val/accuracy: 100.00%, val/strict_accuracy: 99.00%\n",
      "Epoch 925: lr: 7.50e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.86%, val/accuracy: 100.00%, val/strict_accuracy: 99.02%\n",
      "Epoch 926: lr: 7.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.01%\n",
      "Epoch 927: lr: 7.30e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 928: lr: 7.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 98.90%\n",
      "Epoch 929: lr: 7.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.73%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 930: lr: 7.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.74%, val/accuracy: 100.00%, val/strict_accuracy: 98.98%\n",
      "Epoch 931: lr: 6.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.01%\n",
      "Epoch 932: lr: 6.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.87%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 933: lr: 6.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.11%\n",
      "Epoch 934: lr: 6.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.62%, val/accuracy: 100.00%, val/strict_accuracy: 98.94%\n",
      "Epoch 935: lr: 6.50e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 98.94%\n",
      "Epoch 936: lr: 6.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.73%, val/accuracy: 100.00%, val/strict_accuracy: 99.08%\n",
      "Epoch 937: lr: 6.30e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.89%, val/accuracy: 100.00%, val/strict_accuracy: 98.82%\n",
      "Epoch 938: lr: 6.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.65%, val/accuracy: 100.00%, val/strict_accuracy: 99.08%\n",
      "Epoch 939: lr: 6.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 940: lr: 6.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.65%, val/accuracy: 100.00%, val/strict_accuracy: 98.67%\n",
      "Epoch 941: lr: 5.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.05%\n",
      "Epoch 942: lr: 5.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.71%, val/accuracy: 100.00%, val/strict_accuracy: 98.89%\n",
      "Epoch 943: lr: 5.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.83%, val/accuracy: 100.00%, val/strict_accuracy: 99.02%\n",
      "Epoch 944: lr: 5.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.03%\n",
      "Epoch 945: lr: 5.50e-05, train/iit_loss: 0.0000, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.71%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 946: lr: 5.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 947: lr: 5.30e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.67%, val/accuracy: 100.00%, val/strict_accuracy: 99.02%\n",
      "Epoch 948: lr: 5.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 99.10%\n",
      "Epoch 949: lr: 5.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.29%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 950: lr: 5.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.71%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 951: lr: 4.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.01%\n",
      "Epoch 952: lr: 4.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.03%\n",
      "Epoch 953: lr: 4.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.86%, val/accuracy: 100.00%, val/strict_accuracy: 98.87%\n",
      "Epoch 954: lr: 4.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.73%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 955: lr: 4.50e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.74%, val/accuracy: 100.00%, val/strict_accuracy: 99.09%\n",
      "Epoch 956: lr: 4.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.08%\n",
      "Epoch 957: lr: 4.30e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 958: lr: 4.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.79%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 959: lr: 4.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.71%, val/accuracy: 100.00%, val/strict_accuracy: 99.03%\n",
      "Epoch 960: lr: 4.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 961: lr: 3.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.83%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 962: lr: 3.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 99.05%\n",
      "Epoch 963: lr: 3.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.77%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 964: lr: 3.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.71%, val/accuracy: 100.00%, val/strict_accuracy: 99.11%\n",
      "Epoch 965: lr: 3.50e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.76%, val/accuracy: 100.00%, val/strict_accuracy: 99.08%\n",
      "Epoch 966: lr: 3.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.02%\n",
      "Epoch 967: lr: 3.30e-05, train/iit_loss: 0.0000, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.74%, val/accuracy: 100.00%, val/strict_accuracy: 99.11%\n",
      "Epoch 968: lr: 3.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.83%, val/accuracy: 100.00%, val/strict_accuracy: 99.00%\n",
      "Epoch 969: lr: 3.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.78%, val/accuracy: 100.00%, val/strict_accuracy: 99.09%\n",
      "Epoch 970: lr: 3.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.74%, val/accuracy: 100.00%, val/strict_accuracy: 99.08%\n",
      "Epoch 971: lr: 2.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.00%\n",
      "Epoch 972: lr: 2.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 973: lr: 2.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.83%, val/accuracy: 100.00%, val/strict_accuracy: 99.02%\n",
      "Epoch 974: lr: 2.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.79%, val/accuracy: 100.00%, val/strict_accuracy: 98.99%\n",
      "Epoch 975: lr: 2.50e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.88%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 976: lr: 2.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 977: lr: 2.30e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.08%\n",
      "Epoch 978: lr: 2.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.05%\n",
      "Epoch 979: lr: 2.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.70%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 980: lr: 2.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.02%\n",
      "Epoch 981: lr: 1.90e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.82%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 982: lr: 1.80e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.85%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 983: lr: 1.70e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.86%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 984: lr: 1.60e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.08%\n",
      "Epoch 985: lr: 1.50e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 986: lr: 1.40e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.75%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 987: lr: 1.30e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 99.01%\n",
      "Epoch 988: lr: 1.20e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.79%, val/accuracy: 100.00%, val/strict_accuracy: 99.05%\n",
      "Epoch 989: lr: 1.10e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.83%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 990: lr: 1.00e-05, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.74%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 991: lr: 9.00e-06, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.05%\n",
      "Epoch 992: lr: 8.00e-06, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.80%, val/accuracy: 100.00%, val/strict_accuracy: 99.04%\n",
      "Epoch 993: lr: 7.00e-06, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.85%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 994: lr: 6.00e-06, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.03%\n",
      "Epoch 995: lr: 5.00e-06, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.81%, val/accuracy: 100.00%, val/strict_accuracy: 99.03%\n",
      "Epoch 996: lr: 4.00e-06, train/iit_loss: 0.0000, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0000, val/IIA: 99.88%, val/accuracy: 100.00%, val/strict_accuracy: 99.05%\n",
      "Epoch 997: lr: 3.00e-06, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.07%\n",
      "Epoch 998: lr: 2.00e-06, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.89%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n",
      "Epoch 999: lr: 1.00e-06, train/iit_loss: 0.0000, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.85%, val/accuracy: 100.00%, val/strict_accuracy: 99.08%\n",
      "Epoch 1000: lr: 0.00e+00, train/iit_loss: 0.0001, train/behavior_loss: 0.0000, train/strict_loss: 0.0000, val/iit_loss: 0.0001, val/IIA: 99.84%, val/accuracy: 100.00%, val/strict_accuracy: 99.06%\n"
     ]
    }
   ],
   "source": [
    "model_pair.train(\n",
    "    train_set=train_set,\n",
    "    test_set=test_set,\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    epochs=n_epochs,\n",
    "    optimizer_kwargs=dict(weight_decay=1e-4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "from safetensors.torch import save_file\n",
    "state_dict = model_pair.ll_model.state_dict()\n",
    "tensors = {key: value.cpu() for key, value in state_dict.items()}\n",
    "save_file(tensors, \"cases_03_04_ll_model_IIA_??_strict_??.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load it in\n",
    "from safetensors.torch import load_file\n",
    "state_dict = load_file(\"cases_03_04_ll_model_IIA_99p84_strict_99p06.pt\")\n",
    "model_pair.ll_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with pure benchmark HL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "hl_model = hl_models[0]\n",
    "corr = corrs[0]\n",
    "\n",
    "ll_cfg = cases[0].get_ll_model_cfg()\n",
    "\n",
    "class SingleOutputHookedTransformer(tl.HookedTransformer):\n",
    "    def forward(self, x):\n",
    "        output = super().forward(x)\n",
    "        return output[:,:,:1]\n",
    "\n",
    "ll_model = cases[0].get_ll_model()#SingleOutputHookedTransformer(ll_cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 13]) torch.Size([5, 13])\n",
      "tensor([0, 0, 2, 3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000],\n",
       "          [0.0000],\n",
       "          [0.0000],\n",
       "          [0.0000],\n",
       "          [0.2500]]]),\n",
       " ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.hook_q_input', 'blocks.0.hook_k_input', 'blocks.0.hook_v_input', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.attn.hook_result', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.hook_mlp_in', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.hook_q_input', 'blocks.1.hook_k_input', 'blocks.1.hook_v_input', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.attn.hook_result', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.hook_mlp_in', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pair = StrictIITModelPair(hl_model=hl_model.hl_model, ll_model=ll_model, corr=corr, training_args=training_args)\n",
    "print(hl_model.W_E.shape, hl_model.W_pos.shape)\n",
    "print(inputs[0])\n",
    "hl_model.run_with_cache(inputs[0][None,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 5])\n",
      "torch.Size([320, 5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/qny1f95136l2lpppg_d78n6c0000gq/T/ipykernel_11783/2595782960.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(data).to(int)\n",
      "/var/folders/ry/qny1f95136l2lpppg_d78n6c0000gq/T/ipykernel_11783/2595782960.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.targets = torch.tensor(targets)\n"
     ]
    }
   ],
   "source": [
    "dset_list = clean_datasets\n",
    "inputs = torch.cat([dset.inputs for dset in dset_list], dim=0)\n",
    "targets = torch.cat([dset.targets for dset in dset_list], dim=0)\n",
    "\n",
    "# shuffle dataset contents, keeping inputs and targets in sync\n",
    "indices = torch.randperm(inputs.shape[0])\n",
    "inputs = inputs[indices][:,1:]\n",
    "targets = targets[indices][:,1:]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list or numpy array): List or array of input data.\n",
    "            targets (list or numpy array): List or array of target data.\n",
    "        \"\"\"\n",
    "        self.data = torch.tensor(data).to(int)\n",
    "        self.targets = torch.tensor(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index\n",
    "        Returns:\n",
    "            tuple: (input tensor, target tensor)\n",
    "        \"\"\"\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "decorated_dset = CustomDataset(\n",
    "    data = inputs,\n",
    "    targets = targets,\n",
    ")\n",
    "print(decorated_dset.data.shape)\n",
    "print(decorated_dset.targets.shape)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    decorated_dset, test_size=0.2, random_state=42\n",
    ")\n",
    "this_train_set = IITDataset(train_dataset, train_dataset, seed=0)\n",
    "this_test_set = IITDataset(test_dataset, test_dataset, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args={'batch_size': 256, 'lr': 0.001, 'num_workers': 0, 'early_stop': True, 'lr_scheduler': <class 'torch.optim.lr_scheduler.LinearLR'>, 'scheduler_val_metric': ['val/accuracy', 'val/IIA'], 'scheduler_mode': 'max', 'scheduler_kwargs': {'start_factor': 1, 'end_factor': 0, 'total_iters': 1000}, 'clip_grad_norm': False, 'seed': 0, 'detach_while_caching': True, 'atol': 0.05, 'use_single_loss': True, 'iit_weight': 0.5, 'behavior_weight': 0.25, 'strict_weight': 0.25}\n",
      "Epoch 1: lr: 9.99e-04, train/iit_loss: 0.1611, train/behavior_loss: 0.0867, train/strict_loss: 0.0218, val/iit_loss: 0.3348, val/IIA: 3.75%, val/accuracy: 5.00%, val/strict_accuracy: 5.20%\n",
      "Epoch 2: lr: 9.98e-04, train/iit_loss: 0.1472, train/behavior_loss: 0.0768, train/strict_loss: 0.0186, val/iit_loss: 0.3080, val/IIA: 5.62%, val/accuracy: 4.69%, val/strict_accuracy: 5.62%\n",
      "Epoch 3: lr: 9.97e-04, train/iit_loss: 0.1322, train/behavior_loss: 0.0690, train/strict_loss: 0.0170, val/iit_loss: 0.2797, val/IIA: 8.44%, val/accuracy: 8.44%, val/strict_accuracy: 8.79%\n",
      "Epoch 4: lr: 9.96e-04, train/iit_loss: 0.1124, train/behavior_loss: 0.0625, train/strict_loss: 0.0157, val/iit_loss: 0.2451, val/IIA: 12.19%, val/accuracy: 13.12%, val/strict_accuracy: 11.56%\n",
      "Epoch 5: lr: 9.95e-04, train/iit_loss: 0.1014, train/behavior_loss: 0.0569, train/strict_loss: 0.0144, val/iit_loss: 0.2239, val/IIA: 14.06%, val/accuracy: 12.19%, val/strict_accuracy: 10.82%\n",
      "Epoch 6: lr: 9.94e-04, train/iit_loss: 0.0920, train/behavior_loss: 0.0522, train/strict_loss: 0.0132, val/iit_loss: 0.2060, val/IIA: 11.87%, val/accuracy: 10.94%, val/strict_accuracy: 10.31%\n",
      "Epoch 7: lr: 9.93e-04, train/iit_loss: 0.0840, train/behavior_loss: 0.0482, train/strict_loss: 0.0118, val/iit_loss: 0.2058, val/IIA: 13.75%, val/accuracy: 6.56%, val/strict_accuracy: 8.87%\n",
      "Epoch 8: lr: 9.92e-04, train/iit_loss: 0.0772, train/behavior_loss: 0.0447, train/strict_loss: 0.0113, val/iit_loss: 0.1940, val/IIA: 11.56%, val/accuracy: 5.62%, val/strict_accuracy: 6.72%\n",
      "Epoch 9: lr: 9.91e-04, train/iit_loss: 0.0806, train/behavior_loss: 0.0417, train/strict_loss: 0.0107, val/iit_loss: 0.1678, val/IIA: 6.25%, val/accuracy: 6.56%, val/strict_accuracy: 6.64%\n",
      "Epoch 10: lr: 9.90e-04, train/iit_loss: 0.0755, train/behavior_loss: 0.0391, train/strict_loss: 0.0101, val/iit_loss: 0.1594, val/IIA: 5.00%, val/accuracy: 7.19%, val/strict_accuracy: 7.23%\n",
      "Epoch 11: lr: 9.89e-04, train/iit_loss: 0.0622, train/behavior_loss: 0.0369, train/strict_loss: 0.0096, val/iit_loss: 0.1665, val/IIA: 10.94%, val/accuracy: 7.81%, val/strict_accuracy: 7.73%\n",
      "Epoch 12: lr: 9.88e-04, train/iit_loss: 0.0586, train/behavior_loss: 0.0349, train/strict_loss: 0.0085, val/iit_loss: 0.1467, val/IIA: 7.81%, val/accuracy: 9.69%, val/strict_accuracy: 8.83%\n",
      "Epoch 13: lr: 9.87e-04, train/iit_loss: 0.0633, train/behavior_loss: 0.0332, train/strict_loss: 0.0076, val/iit_loss: 0.1541, val/IIA: 10.31%, val/accuracy: 9.38%, val/strict_accuracy: 8.24%\n",
      "Epoch 14: lr: 9.86e-04, train/iit_loss: 0.0602, train/behavior_loss: 0.0317, train/strict_loss: 0.0079, val/iit_loss: 0.1489, val/IIA: 9.38%, val/accuracy: 8.75%, val/strict_accuracy: 7.66%\n",
      "Epoch 15: lr: 9.85e-04, train/iit_loss: 0.0573, train/behavior_loss: 0.0304, train/strict_loss: 0.0074, val/iit_loss: 0.1443, val/IIA: 8.12%, val/accuracy: 5.00%, val/strict_accuracy: 6.25%\n",
      "Epoch 16: lr: 9.84e-04, train/iit_loss: 0.0493, train/behavior_loss: 0.0292, train/strict_loss: 0.0074, val/iit_loss: 0.1338, val/IIA: 6.88%, val/accuracy: 6.25%, val/strict_accuracy: 5.90%\n",
      "Epoch 17: lr: 9.83e-04, train/iit_loss: 0.0525, train/behavior_loss: 0.0283, train/strict_loss: 0.0071, val/iit_loss: 0.1320, val/IIA: 5.94%, val/accuracy: 5.62%, val/strict_accuracy: 5.00%\n",
      "Epoch 18: lr: 9.82e-04, train/iit_loss: 0.0505, train/behavior_loss: 0.0274, train/strict_loss: 0.0068, val/iit_loss: 0.1305, val/IIA: 4.37%, val/accuracy: 3.75%, val/strict_accuracy: 4.38%\n",
      "Epoch 19: lr: 9.81e-04, train/iit_loss: 0.0460, train/behavior_loss: 0.0266, train/strict_loss: 0.0077, val/iit_loss: 0.1293, val/IIA: 28.44%, val/accuracy: 23.75%, val/strict_accuracy: 23.87%\n",
      "Epoch 20: lr: 9.80e-04, train/iit_loss: 0.0452, train/behavior_loss: 0.0258, train/strict_loss: 0.0076, val/iit_loss: 0.1275, val/IIA: 22.50%, val/accuracy: 23.75%, val/strict_accuracy: 23.83%\n",
      "Epoch 21: lr: 9.79e-04, train/iit_loss: 0.0445, train/behavior_loss: 0.0251, train/strict_loss: 0.0064, val/iit_loss: 0.1260, val/IIA: 22.81%, val/accuracy: 23.75%, val/strict_accuracy: 23.55%\n",
      "Epoch 22: lr: 9.78e-04, train/iit_loss: 0.0438, train/behavior_loss: 0.0245, train/strict_loss: 0.0074, val/iit_loss: 0.1196, val/IIA: 27.19%, val/accuracy: 21.56%, val/strict_accuracy: 22.93%\n",
      "Epoch 23: lr: 9.77e-04, train/iit_loss: 0.0433, train/behavior_loss: 0.0238, train/strict_loss: 0.0060, val/iit_loss: 0.1232, val/IIA: 22.81%, val/accuracy: 21.56%, val/strict_accuracy: 22.42%\n",
      "Epoch 24: lr: 9.76e-04, train/iit_loss: 0.0428, train/behavior_loss: 0.0232, train/strict_loss: 0.0058, val/iit_loss: 0.1135, val/IIA: 27.19%, val/accuracy: 21.88%, val/strict_accuracy: 22.58%\n",
      "Epoch 25: lr: 9.75e-04, train/iit_loss: 0.0399, train/behavior_loss: 0.0226, train/strict_loss: 0.0057, val/iit_loss: 0.1207, val/IIA: 23.75%, val/accuracy: 23.44%, val/strict_accuracy: 23.20%\n",
      "Epoch 26: lr: 9.74e-04, train/iit_loss: 0.0422, train/behavior_loss: 0.0221, train/strict_loss: 0.0055, val/iit_loss: 0.1197, val/IIA: 4.69%, val/accuracy: 5.00%, val/strict_accuracy: 4.06%\n",
      "Epoch 27: lr: 9.73e-04, train/iit_loss: 0.0420, train/behavior_loss: 0.0215, train/strict_loss: 0.0048, val/iit_loss: 0.1189, val/IIA: 5.31%, val/accuracy: 5.94%, val/strict_accuracy: 5.70%\n",
      "Epoch 28: lr: 9.72e-04, train/iit_loss: 0.0419, train/behavior_loss: 0.0210, train/strict_loss: 0.0047, val/iit_loss: 0.1030, val/IIA: 7.19%, val/accuracy: 6.56%, val/strict_accuracy: 6.37%\n",
      "Epoch 29: lr: 9.71e-04, train/iit_loss: 0.0418, train/behavior_loss: 0.0205, train/strict_loss: 0.0051, val/iit_loss: 0.1175, val/IIA: 7.19%, val/accuracy: 7.81%, val/strict_accuracy: 6.91%\n",
      "Epoch 30: lr: 9.70e-04, train/iit_loss: 0.0353, train/behavior_loss: 0.0200, train/strict_loss: 0.0052, val/iit_loss: 0.0987, val/IIA: 5.94%, val/accuracy: 7.81%, val/strict_accuracy: 7.19%\n",
      "Epoch 31: lr: 9.69e-04, train/iit_loss: 0.0344, train/behavior_loss: 0.0195, train/strict_loss: 0.0068, val/iit_loss: 0.0964, val/IIA: 6.25%, val/accuracy: 8.44%, val/strict_accuracy: 7.27%\n",
      "Epoch 32: lr: 9.68e-04, train/iit_loss: 0.0335, train/behavior_loss: 0.0189, train/strict_loss: 0.0048, val/iit_loss: 0.1165, val/IIA: 7.50%, val/accuracy: 7.19%, val/strict_accuracy: 7.23%\n",
      "Epoch 33: lr: 9.67e-04, train/iit_loss: 0.0326, train/behavior_loss: 0.0184, train/strict_loss: 0.0047, val/iit_loss: 0.0916, val/IIA: 7.19%, val/accuracy: 4.69%, val/strict_accuracy: 6.05%\n",
      "Epoch 34: lr: 9.66e-04, train/iit_loss: 0.0420, train/behavior_loss: 0.0178, train/strict_loss: 0.0045, val/iit_loss: 0.1166, val/IIA: 6.56%, val/accuracy: 4.69%, val/strict_accuracy: 5.27%\n",
      "Epoch 35: lr: 9.65e-04, train/iit_loss: 0.0307, train/behavior_loss: 0.0173, train/strict_loss: 0.0045, val/iit_loss: 0.1167, val/IIA: 6.25%, val/accuracy: 5.00%, val/strict_accuracy: 5.23%\n",
      "Epoch 36: lr: 9.64e-04, train/iit_loss: 0.0423, train/behavior_loss: 0.0168, train/strict_loss: 0.0043, val/iit_loss: 0.0848, val/IIA: 27.50%, val/accuracy: 25.62%, val/strict_accuracy: 25.59%\n",
      "Epoch 37: lr: 9.63e-04, train/iit_loss: 0.0425, train/behavior_loss: 0.0163, train/strict_loss: 0.0037, val/iit_loss: 0.1166, val/IIA: 25.31%, val/accuracy: 25.62%, val/strict_accuracy: 25.59%\n",
      "Epoch 38: lr: 9.62e-04, train/iit_loss: 0.0283, train/behavior_loss: 0.0158, train/strict_loss: 0.0042, val/iit_loss: 0.0804, val/IIA: 28.12%, val/accuracy: 23.75%, val/strict_accuracy: 24.88%\n",
      "Epoch 39: lr: 9.61e-04, train/iit_loss: 0.0429, train/behavior_loss: 0.0153, train/strict_loss: 0.0041, val/iit_loss: 0.1162, val/IIA: 24.38%, val/accuracy: 23.44%, val/strict_accuracy: 24.57%\n",
      "Epoch 40: lr: 9.60e-04, train/iit_loss: 0.0268, train/behavior_loss: 0.0149, train/strict_loss: 0.0040, val/iit_loss: 0.0756, val/IIA: 27.81%, val/accuracy: 25.00%, val/strict_accuracy: 24.65%\n",
      "Epoch 41: lr: 9.59e-04, train/iit_loss: 0.0433, train/behavior_loss: 0.0144, train/strict_loss: 0.0037, val/iit_loss: 0.1153, val/IIA: 4.37%, val/accuracy: 5.31%, val/strict_accuracy: 4.88%\n",
      "Epoch 42: lr: 9.58e-04, train/iit_loss: 0.0253, train/behavior_loss: 0.0140, train/strict_loss: 0.0067, val/iit_loss: 0.0707, val/IIA: 8.44%, val/accuracy: 4.37%, val/strict_accuracy: 4.80%\n",
      "Epoch 43: lr: 9.57e-04, train/iit_loss: 0.0437, train/behavior_loss: 0.0136, train/strict_loss: 0.0033, val/iit_loss: 0.1142, val/IIA: 4.37%, val/accuracy: 4.37%, val/strict_accuracy: 4.65%\n",
      "Epoch 44: lr: 9.56e-04, train/iit_loss: 0.0239, train/behavior_loss: 0.0132, train/strict_loss: 0.0067, val/iit_loss: 0.1137, val/IIA: 4.06%, val/accuracy: 4.37%, val/strict_accuracy: 4.69%\n",
      "Epoch 45: lr: 9.55e-04, train/iit_loss: 0.0441, train/behavior_loss: 0.0128, train/strict_loss: 0.0035, val/iit_loss: 0.1132, val/IIA: 3.44%, val/accuracy: 4.37%, val/strict_accuracy: 4.80%\n",
      "Epoch 46: lr: 9.54e-04, train/iit_loss: 0.0443, train/behavior_loss: 0.0125, train/strict_loss: 0.0033, val/iit_loss: 0.0620, val/IIA: 8.44%, val/accuracy: 3.75%, val/strict_accuracy: 4.88%\n",
      "Epoch 47: lr: 9.53e-04, train/iit_loss: 0.0444, train/behavior_loss: 0.0122, train/strict_loss: 0.0032, val/iit_loss: 0.1120, val/IIA: 22.50%, val/accuracy: 23.75%, val/strict_accuracy: 24.96%\n",
      "Epoch 48: lr: 9.52e-04, train/iit_loss: 0.0443, train/behavior_loss: 0.0119, train/strict_loss: 0.0033, val/iit_loss: 0.1114, val/IIA: 22.50%, val/accuracy: 24.69%, val/strict_accuracy: 25.04%\n",
      "Epoch 49: lr: 9.51e-04, train/iit_loss: 0.0210, train/behavior_loss: 0.0117, train/strict_loss: 0.0030, val/iit_loss: 0.0577, val/IIA: 28.12%, val/accuracy: 24.69%, val/strict_accuracy: 25.20%\n",
      "Epoch 50: lr: 9.50e-04, train/iit_loss: 0.0442, train/behavior_loss: 0.0114, train/strict_loss: 0.0031, val/iit_loss: 0.0564, val/IIA: 28.75%, val/accuracy: 24.69%, val/strict_accuracy: 25.16%\n",
      "Epoch 51: lr: 9.49e-04, train/iit_loss: 0.0201, train/behavior_loss: 0.0111, train/strict_loss: 0.0029, val/iit_loss: 0.1099, val/IIA: 22.81%, val/accuracy: 24.69%, val/strict_accuracy: 25.23%\n",
      "Epoch 52: lr: 9.48e-04, train/iit_loss: 0.0441, train/behavior_loss: 0.0109, train/strict_loss: 0.0025, val/iit_loss: 0.1093, val/IIA: 22.81%, val/accuracy: 24.69%, val/strict_accuracy: 25.23%\n",
      "Epoch 53: lr: 9.47e-04, train/iit_loss: 0.0440, train/behavior_loss: 0.0106, train/strict_loss: 0.0066, val/iit_loss: 0.0523, val/IIA: 30.94%, val/accuracy: 24.69%, val/strict_accuracy: 25.12%\n",
      "Epoch 54: lr: 9.46e-04, train/iit_loss: 0.0187, train/behavior_loss: 0.0105, train/strict_loss: 0.0030, val/iit_loss: 0.1079, val/IIA: 22.50%, val/accuracy: 25.94%, val/strict_accuracy: 25.08%\n",
      "Epoch 55: lr: 9.45e-04, train/iit_loss: 0.0182, train/behavior_loss: 0.0102, train/strict_loss: 0.0027, val/iit_loss: 0.0490, val/IIA: 31.87%, val/accuracy: 26.56%, val/strict_accuracy: 25.23%\n",
      "Epoch 56: lr: 9.44e-04, train/iit_loss: 0.0438, train/behavior_loss: 0.0100, train/strict_loss: 0.0028, val/iit_loss: 0.1068, val/IIA: 22.81%, val/accuracy: 26.56%, val/strict_accuracy: 25.31%\n",
      "Epoch 57: lr: 9.43e-04, train/iit_loss: 0.0171, train/behavior_loss: 0.0097, train/strict_loss: 0.0026, val/iit_loss: 0.0459, val/IIA: 32.19%, val/accuracy: 26.88%, val/strict_accuracy: 25.47%\n",
      "Epoch 58: lr: 9.42e-04, train/iit_loss: 0.0438, train/behavior_loss: 0.0094, train/strict_loss: 0.0024, val/iit_loss: 0.1062, val/IIA: 24.06%, val/accuracy: 26.88%, val/strict_accuracy: 25.35%\n",
      "Epoch 59: lr: 9.41e-04, train/iit_loss: 0.0438, train/behavior_loss: 0.0092, train/strict_loss: 0.0066, val/iit_loss: 0.0436, val/IIA: 32.19%, val/accuracy: 27.19%, val/strict_accuracy: 25.63%\n",
      "Epoch 60: lr: 9.40e-04, train/iit_loss: 0.0156, train/behavior_loss: 0.0090, train/strict_loss: 0.0026, val/iit_loss: 0.1057, val/IIA: 24.38%, val/accuracy: 26.56%, val/strict_accuracy: 25.51%\n",
      "Epoch 61: lr: 9.39e-04, train/iit_loss: 0.0436, train/behavior_loss: 0.0088, train/strict_loss: 0.0066, val/iit_loss: 0.1055, val/IIA: 24.69%, val/accuracy: 27.81%, val/strict_accuracy: 25.78%\n",
      "Epoch 62: lr: 9.38e-04, train/iit_loss: 0.0434, train/behavior_loss: 0.0086, train/strict_loss: 0.0026, val/iit_loss: 0.0410, val/IIA: 33.13%, val/accuracy: 28.12%, val/strict_accuracy: 25.90%\n",
      "Epoch 63: lr: 9.37e-04, train/iit_loss: 0.0145, train/behavior_loss: 0.0085, train/strict_loss: 0.0026, val/iit_loss: 0.0400, val/IIA: 32.81%, val/accuracy: 28.12%, val/strict_accuracy: 26.33%\n",
      "Epoch 64: lr: 9.36e-04, train/iit_loss: 0.0430, train/behavior_loss: 0.0083, train/strict_loss: 0.0025, val/iit_loss: 0.1043, val/IIA: 24.69%, val/accuracy: 28.12%, val/strict_accuracy: 26.29%\n",
      "Epoch 65: lr: 9.35e-04, train/iit_loss: 0.0137, train/behavior_loss: 0.0081, train/strict_loss: 0.0023, val/iit_loss: 0.1038, val/IIA: 25.00%, val/accuracy: 28.12%, val/strict_accuracy: 26.41%\n",
      "Epoch 66: lr: 9.34e-04, train/iit_loss: 0.0427, train/behavior_loss: 0.0079, train/strict_loss: 0.0019, val/iit_loss: 0.1032, val/IIA: 25.31%, val/accuracy: 28.12%, val/strict_accuracy: 26.56%\n",
      "Epoch 67: lr: 9.33e-04, train/iit_loss: 0.0130, train/behavior_loss: 0.0077, train/strict_loss: 0.0024, val/iit_loss: 0.0358, val/IIA: 35.31%, val/accuracy: 28.12%, val/strict_accuracy: 26.87%\n",
      "Epoch 68: lr: 9.32e-04, train/iit_loss: 0.0126, train/behavior_loss: 0.0075, train/strict_loss: 0.0018, val/iit_loss: 0.0343, val/IIA: 35.62%, val/accuracy: 28.44%, val/strict_accuracy: 27.30%\n",
      "Epoch 69: lr: 9.31e-04, train/iit_loss: 0.0425, train/behavior_loss: 0.0072, train/strict_loss: 0.0023, val/iit_loss: 0.0332, val/IIA: 35.94%, val/accuracy: 28.44%, val/strict_accuracy: 27.54%\n",
      "Epoch 70: lr: 9.30e-04, train/iit_loss: 0.0424, train/behavior_loss: 0.0070, train/strict_loss: 0.0066, val/iit_loss: 0.0326, val/IIA: 35.94%, val/accuracy: 28.44%, val/strict_accuracy: 28.09%\n",
      "Epoch 71: lr: 9.29e-04, train/iit_loss: 0.0115, train/behavior_loss: 0.0068, train/strict_loss: 0.0018, val/iit_loss: 0.1009, val/IIA: 25.62%, val/accuracy: 28.44%, val/strict_accuracy: 28.59%\n",
      "Epoch 72: lr: 9.28e-04, train/iit_loss: 0.0111, train/behavior_loss: 0.0066, train/strict_loss: 0.0020, val/iit_loss: 0.1006, val/IIA: 25.94%, val/accuracy: 31.87%, val/strict_accuracy: 29.30%\n",
      "Epoch 73: lr: 9.27e-04, train/iit_loss: 0.0421, train/behavior_loss: 0.0063, train/strict_loss: 0.0018, val/iit_loss: 0.1000, val/IIA: 25.94%, val/accuracy: 32.50%, val/strict_accuracy: 29.45%\n",
      "Epoch 74: lr: 9.26e-04, train/iit_loss: 0.0419, train/behavior_loss: 0.0061, train/strict_loss: 0.0019, val/iit_loss: 0.0992, val/IIA: 25.94%, val/accuracy: 33.44%, val/strict_accuracy: 29.73%\n",
      "Epoch 75: lr: 9.25e-04, train/iit_loss: 0.0416, train/behavior_loss: 0.0060, train/strict_loss: 0.0018, val/iit_loss: 0.0982, val/IIA: 25.94%, val/accuracy: 33.44%, val/strict_accuracy: 29.84%\n",
      "Epoch 76: lr: 9.24e-04, train/iit_loss: 0.0412, train/behavior_loss: 0.0059, train/strict_loss: 0.0014, val/iit_loss: 0.0283, val/IIA: 37.50%, val/accuracy: 33.44%, val/strict_accuracy: 30.08%\n",
      "Epoch 77: lr: 9.23e-04, train/iit_loss: 0.0100, train/behavior_loss: 0.0058, train/strict_loss: 0.0020, val/iit_loss: 0.0277, val/IIA: 37.50%, val/accuracy: 34.38%, val/strict_accuracy: 30.66%\n",
      "Epoch 78: lr: 9.22e-04, train/iit_loss: 0.0403, train/behavior_loss: 0.0057, train/strict_loss: 0.0066, val/iit_loss: 0.0943, val/IIA: 27.19%, val/accuracy: 32.50%, val/strict_accuracy: 30.39%\n",
      "Epoch 79: lr: 9.21e-04, train/iit_loss: 0.0097, train/behavior_loss: 0.0056, train/strict_loss: 0.0017, val/iit_loss: 0.0267, val/IIA: 38.44%, val/accuracy: 31.56%, val/strict_accuracy: 30.59%\n",
      "Epoch 80: lr: 9.20e-04, train/iit_loss: 0.0394, train/behavior_loss: 0.0055, train/strict_loss: 0.0016, val/iit_loss: 0.0264, val/IIA: 38.12%, val/accuracy: 31.56%, val/strict_accuracy: 30.31%\n",
      "Epoch 81: lr: 9.19e-04, train/iit_loss: 0.0390, train/behavior_loss: 0.0054, train/strict_loss: 0.0013, val/iit_loss: 0.0264, val/IIA: 38.44%, val/accuracy: 32.19%, val/strict_accuracy: 30.55%\n",
      "Epoch 82: lr: 9.18e-04, train/iit_loss: 0.0093, train/behavior_loss: 0.0053, train/strict_loss: 0.0020, val/iit_loss: 0.0904, val/IIA: 27.50%, val/accuracy: 32.19%, val/strict_accuracy: 30.94%\n",
      "Epoch 83: lr: 9.17e-04, train/iit_loss: 0.0091, train/behavior_loss: 0.0052, train/strict_loss: 0.0015, val/iit_loss: 0.0897, val/IIA: 27.50%, val/accuracy: 36.56%, val/strict_accuracy: 31.64%\n",
      "Epoch 84: lr: 9.16e-04, train/iit_loss: 0.0089, train/behavior_loss: 0.0050, train/strict_loss: 0.0012, val/iit_loss: 0.0241, val/IIA: 39.06%, val/accuracy: 37.81%, val/strict_accuracy: 32.46%\n",
      "Epoch 85: lr: 9.15e-04, train/iit_loss: 0.0085, train/behavior_loss: 0.0047, train/strict_loss: 0.0066, val/iit_loss: 0.0228, val/IIA: 40.31%, val/accuracy: 38.12%, val/strict_accuracy: 33.32%\n",
      "Epoch 86: lr: 9.14e-04, train/iit_loss: 0.0082, train/behavior_loss: 0.0045, train/strict_loss: 0.0015, val/iit_loss: 0.0214, val/IIA: 42.19%, val/accuracy: 39.38%, val/strict_accuracy: 34.22%\n",
      "Epoch 87: lr: 9.13e-04, train/iit_loss: 0.0379, train/behavior_loss: 0.0042, train/strict_loss: 0.0011, val/iit_loss: 0.0875, val/IIA: 28.12%, val/accuracy: 40.94%, val/strict_accuracy: 35.43%\n",
      "Epoch 88: lr: 9.12e-04, train/iit_loss: 0.0378, train/behavior_loss: 0.0039, train/strict_loss: 0.0016, val/iit_loss: 0.0871, val/IIA: 28.12%, val/accuracy: 40.94%, val/strict_accuracy: 36.37%\n",
      "Epoch 89: lr: 9.11e-04, train/iit_loss: 0.0376, train/behavior_loss: 0.0037, train/strict_loss: 0.0016, val/iit_loss: 0.0198, val/IIA: 42.19%, val/accuracy: 40.94%, val/strict_accuracy: 37.58%\n",
      "Epoch 90: lr: 9.10e-04, train/iit_loss: 0.0372, train/behavior_loss: 0.0036, train/strict_loss: 0.0011, val/iit_loss: 0.0197, val/IIA: 41.25%, val/accuracy: 40.94%, val/strict_accuracy: 37.62%\n",
      "Epoch 91: lr: 9.09e-04, train/iit_loss: 0.0071, train/behavior_loss: 0.0036, train/strict_loss: 0.0010, val/iit_loss: 0.0191, val/IIA: 41.87%, val/accuracy: 41.87%, val/strict_accuracy: 37.30%\n",
      "Epoch 92: lr: 9.08e-04, train/iit_loss: 0.0361, train/behavior_loss: 0.0035, train/strict_loss: 0.0016, val/iit_loss: 0.0822, val/IIA: 28.44%, val/accuracy: 42.19%, val/strict_accuracy: 36.45%\n",
      "Epoch 93: lr: 9.07e-04, train/iit_loss: 0.0355, train/behavior_loss: 0.0034, train/strict_loss: 0.0068, val/iit_loss: 0.0805, val/IIA: 27.81%, val/accuracy: 42.81%, val/strict_accuracy: 35.66%\n",
      "Epoch 94: lr: 9.06e-04, train/iit_loss: 0.0068, train/behavior_loss: 0.0034, train/strict_loss: 0.0011, val/iit_loss: 0.0791, val/IIA: 28.12%, val/accuracy: 41.25%, val/strict_accuracy: 35.23%\n",
      "Epoch 95: lr: 9.05e-04, train/iit_loss: 0.0067, train/behavior_loss: 0.0034, train/strict_loss: 0.0011, val/iit_loss: 0.0179, val/IIA: 41.87%, val/accuracy: 41.87%, val/strict_accuracy: 35.27%\n",
      "Epoch 96: lr: 9.04e-04, train/iit_loss: 0.0338, train/behavior_loss: 0.0033, train/strict_loss: 0.0067, val/iit_loss: 0.0178, val/IIA: 41.56%, val/accuracy: 43.13%, val/strict_accuracy: 35.82%\n",
      "Epoch 97: lr: 9.03e-04, train/iit_loss: 0.0332, train/behavior_loss: 0.0032, train/strict_loss: 0.0010, val/iit_loss: 0.0180, val/IIA: 40.94%, val/accuracy: 42.81%, val/strict_accuracy: 36.37%\n",
      "Epoch 98: lr: 9.02e-04, train/iit_loss: 0.0325, train/behavior_loss: 0.0032, train/strict_loss: 0.0067, val/iit_loss: 0.0183, val/IIA: 39.06%, val/accuracy: 42.19%, val/strict_accuracy: 36.48%\n",
      "Epoch 99: lr: 9.01e-04, train/iit_loss: 0.0318, train/behavior_loss: 0.0032, train/strict_loss: 0.0015, val/iit_loss: 0.0737, val/IIA: 29.69%, val/accuracy: 40.94%, val/strict_accuracy: 35.94%\n",
      "Epoch 100: lr: 9.00e-04, train/iit_loss: 0.0066, train/behavior_loss: 0.0033, train/strict_loss: 0.0016, val/iit_loss: 0.0719, val/IIA: 29.06%, val/accuracy: 40.94%, val/strict_accuracy: 35.16%\n",
      "Epoch 101: lr: 8.99e-04, train/iit_loss: 0.0065, train/behavior_loss: 0.0033, train/strict_loss: 0.0011, val/iit_loss: 0.0175, val/IIA: 40.31%, val/accuracy: 40.94%, val/strict_accuracy: 33.91%\n",
      "Epoch 102: lr: 8.98e-04, train/iit_loss: 0.0063, train/behavior_loss: 0.0032, train/strict_loss: 0.0010, val/iit_loss: 0.0166, val/IIA: 40.94%, val/accuracy: 39.38%, val/strict_accuracy: 34.10%\n",
      "Epoch 103: lr: 8.97e-04, train/iit_loss: 0.0061, train/behavior_loss: 0.0030, train/strict_loss: 0.0008, val/iit_loss: 0.0680, val/IIA: 28.75%, val/accuracy: 40.00%, val/strict_accuracy: 35.78%\n",
      "Epoch 104: lr: 8.96e-04, train/iit_loss: 0.0058, train/behavior_loss: 0.0028, train/strict_loss: 0.0066, val/iit_loss: 0.0675, val/IIA: 30.31%, val/accuracy: 45.94%, val/strict_accuracy: 37.81%\n",
      "Epoch 105: lr: 8.95e-04, train/iit_loss: 0.0294, train/behavior_loss: 0.0025, train/strict_loss: 0.0067, val/iit_loss: 0.0672, val/IIA: 34.06%, val/accuracy: 50.00%, val/strict_accuracy: 42.42%\n",
      "Epoch 106: lr: 8.94e-04, train/iit_loss: 0.0053, train/behavior_loss: 0.0023, train/strict_loss: 0.0008, val/iit_loss: 0.0668, val/IIA: 35.00%, val/accuracy: 52.81%, val/strict_accuracy: 43.87%\n",
      "Epoch 107: lr: 8.93e-04, train/iit_loss: 0.0051, train/behavior_loss: 0.0021, train/strict_loss: 0.0008, val/iit_loss: 0.0121, val/IIA: 45.94%, val/accuracy: 52.81%, val/strict_accuracy: 44.65%\n",
      "Epoch 108: lr: 8.92e-04, train/iit_loss: 0.0047, train/behavior_loss: 0.0019, train/strict_loss: 0.0006, val/iit_loss: 0.0109, val/IIA: 45.31%, val/accuracy: 57.19%, val/strict_accuracy: 46.09%\n",
      "Epoch 109: lr: 8.91e-04, train/iit_loss: 0.0044, train/behavior_loss: 0.0017, train/strict_loss: 0.0005, val/iit_loss: 0.0099, val/IIA: 47.81%, val/accuracy: 58.13%, val/strict_accuracy: 46.45%\n",
      "Epoch 110: lr: 8.90e-04, train/iit_loss: 0.0292, train/behavior_loss: 0.0016, train/strict_loss: 0.0004, val/iit_loss: 0.0632, val/IIA: 36.25%, val/accuracy: 57.19%, val/strict_accuracy: 46.88%\n",
      "Epoch 111: lr: 8.89e-04, train/iit_loss: 0.0040, train/behavior_loss: 0.0014, train/strict_loss: 0.0005, val/iit_loss: 0.0628, val/IIA: 37.81%, val/accuracy: 62.50%, val/strict_accuracy: 48.75%\n",
      "Epoch 112: lr: 8.88e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0013, train/strict_loss: 0.0006, val/iit_loss: 0.0626, val/IIA: 37.50%, val/accuracy: 63.44%, val/strict_accuracy: 49.22%\n",
      "Epoch 113: lr: 8.87e-04, train/iit_loss: 0.0289, train/behavior_loss: 0.0012, train/strict_loss: 0.0072, val/iit_loss: 0.0079, val/IIA: 47.81%, val/accuracy: 64.38%, val/strict_accuracy: 49.14%\n",
      "Epoch 114: lr: 8.86e-04, train/iit_loss: 0.0285, train/behavior_loss: 0.0012, train/strict_loss: 0.0003, val/iit_loss: 0.0077, val/IIA: 49.69%, val/accuracy: 61.25%, val/strict_accuracy: 49.22%\n",
      "Epoch 115: lr: 8.85e-04, train/iit_loss: 0.0278, train/behavior_loss: 0.0011, train/strict_loss: 0.0071, val/iit_loss: 0.0580, val/IIA: 36.88%, val/accuracy: 61.56%, val/strict_accuracy: 48.40%\n",
      "Epoch 116: lr: 8.84e-04, train/iit_loss: 0.0034, train/behavior_loss: 0.0011, train/strict_loss: 0.0005, val/iit_loss: 0.0077, val/IIA: 51.56%, val/accuracy: 57.81%, val/strict_accuracy: 46.76%\n",
      "Epoch 117: lr: 8.83e-04, train/iit_loss: 0.0033, train/behavior_loss: 0.0012, train/strict_loss: 0.0070, val/iit_loss: 0.0076, val/IIA: 52.81%, val/accuracy: 58.13%, val/strict_accuracy: 45.98%\n",
      "Epoch 118: lr: 8.82e-04, train/iit_loss: 0.0254, train/behavior_loss: 0.0012, train/strict_loss: 0.0005, val/iit_loss: 0.0537, val/IIA: 35.94%, val/accuracy: 56.88%, val/strict_accuracy: 46.29%\n",
      "Epoch 119: lr: 8.81e-04, train/iit_loss: 0.0246, train/behavior_loss: 0.0012, train/strict_loss: 0.0069, val/iit_loss: 0.0528, val/IIA: 36.56%, val/accuracy: 55.00%, val/strict_accuracy: 46.56%\n",
      "Epoch 120: lr: 8.80e-04, train/iit_loss: 0.0237, train/behavior_loss: 0.0013, train/strict_loss: 0.0067, val/iit_loss: 0.0516, val/IIA: 36.56%, val/accuracy: 55.94%, val/strict_accuracy: 46.33%\n",
      "Epoch 121: lr: 8.79e-04, train/iit_loss: 0.0228, train/behavior_loss: 0.0014, train/strict_loss: 0.0010, val/iit_loss: 0.0500, val/IIA: 36.25%, val/accuracy: 56.56%, val/strict_accuracy: 44.92%\n",
      "Epoch 122: lr: 8.78e-04, train/iit_loss: 0.0037, train/behavior_loss: 0.0016, train/strict_loss: 0.0007, val/iit_loss: 0.0097, val/IIA: 51.25%, val/accuracy: 54.69%, val/strict_accuracy: 43.44%\n",
      "Epoch 123: lr: 8.77e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0017, train/strict_loss: 0.0004, val/iit_loss: 0.0098, val/IIA: 51.56%, val/accuracy: 47.50%, val/strict_accuracy: 40.27%\n",
      "Epoch 124: lr: 8.76e-04, train/iit_loss: 0.0206, train/behavior_loss: 0.0017, train/strict_loss: 0.0011, val/iit_loss: 0.0099, val/IIA: 51.56%, val/accuracy: 47.19%, val/strict_accuracy: 40.74%\n",
      "Epoch 125: lr: 8.75e-04, train/iit_loss: 0.0200, train/behavior_loss: 0.0018, train/strict_loss: 0.0006, val/iit_loss: 0.0454, val/IIA: 36.88%, val/accuracy: 53.44%, val/strict_accuracy: 43.63%\n",
      "Epoch 126: lr: 8.74e-04, train/iit_loss: 0.0193, train/behavior_loss: 0.0018, train/strict_loss: 0.0008, val/iit_loss: 0.0446, val/IIA: 35.62%, val/accuracy: 52.50%, val/strict_accuracy: 43.63%\n",
      "Epoch 127: lr: 8.73e-04, train/iit_loss: 0.0188, train/behavior_loss: 0.0019, train/strict_loss: 0.0008, val/iit_loss: 0.0433, val/IIA: 36.56%, val/accuracy: 51.25%, val/strict_accuracy: 43.83%\n",
      "Epoch 128: lr: 8.72e-04, train/iit_loss: 0.0042, train/behavior_loss: 0.0019, train/strict_loss: 0.0012, val/iit_loss: 0.0415, val/IIA: 36.88%, val/accuracy: 48.44%, val/strict_accuracy: 42.19%\n",
      "Epoch 129: lr: 8.71e-04, train/iit_loss: 0.0176, train/behavior_loss: 0.0019, train/strict_loss: 0.0008, val/iit_loss: 0.0101, val/IIA: 51.25%, val/accuracy: 47.50%, val/strict_accuracy: 40.86%\n",
      "Epoch 130: lr: 8.70e-04, train/iit_loss: 0.0040, train/behavior_loss: 0.0019, train/strict_loss: 0.0008, val/iit_loss: 0.0383, val/IIA: 36.56%, val/accuracy: 48.75%, val/strict_accuracy: 41.29%\n",
      "Epoch 131: lr: 8.69e-04, train/iit_loss: 0.0039, train/behavior_loss: 0.0018, train/strict_loss: 0.0008, val/iit_loss: 0.0089, val/IIA: 51.88%, val/accuracy: 53.75%, val/strict_accuracy: 43.63%\n",
      "Epoch 132: lr: 8.68e-04, train/iit_loss: 0.0037, train/behavior_loss: 0.0016, train/strict_loss: 0.0008, val/iit_loss: 0.0361, val/IIA: 36.88%, val/accuracy: 61.56%, val/strict_accuracy: 47.54%\n",
      "Epoch 133: lr: 8.67e-04, train/iit_loss: 0.0158, train/behavior_loss: 0.0014, train/strict_loss: 0.0061, val/iit_loss: 0.0351, val/IIA: 38.12%, val/accuracy: 60.62%, val/strict_accuracy: 48.40%\n",
      "Epoch 134: lr: 8.66e-04, train/iit_loss: 0.0155, train/behavior_loss: 0.0012, train/strict_loss: 0.0062, val/iit_loss: 0.0070, val/IIA: 55.00%, val/accuracy: 65.00%, val/strict_accuracy: 48.87%\n",
      "Epoch 135: lr: 8.65e-04, train/iit_loss: 0.0150, train/behavior_loss: 0.0011, train/strict_loss: 0.0006, val/iit_loss: 0.0065, val/IIA: 54.69%, val/accuracy: 59.69%, val/strict_accuracy: 47.77%\n",
      "Epoch 136: lr: 8.64e-04, train/iit_loss: 0.0145, train/behavior_loss: 0.0010, train/strict_loss: 0.0006, val/iit_loss: 0.0063, val/IIA: 54.37%, val/accuracy: 60.62%, val/strict_accuracy: 47.66%\n",
      "Epoch 137: lr: 8.63e-04, train/iit_loss: 0.0140, train/behavior_loss: 0.0010, train/strict_loss: 0.0006, val/iit_loss: 0.0290, val/IIA: 39.38%, val/accuracy: 60.31%, val/strict_accuracy: 47.58%\n",
      "Epoch 138: lr: 8.62e-04, train/iit_loss: 0.0134, train/behavior_loss: 0.0010, train/strict_loss: 0.0062, val/iit_loss: 0.0280, val/IIA: 40.62%, val/accuracy: 65.31%, val/strict_accuracy: 49.06%\n",
      "Epoch 139: lr: 8.61e-04, train/iit_loss: 0.0028, train/behavior_loss: 0.0010, train/strict_loss: 0.0004, val/iit_loss: 0.0269, val/IIA: 41.56%, val/accuracy: 65.00%, val/strict_accuracy: 48.59%\n",
      "Epoch 140: lr: 8.60e-04, train/iit_loss: 0.0027, train/behavior_loss: 0.0009, train/strict_loss: 0.0005, val/iit_loss: 0.0257, val/IIA: 42.50%, val/accuracy: 66.56%, val/strict_accuracy: 49.02%\n",
      "Epoch 141: lr: 8.59e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0009, train/strict_loss: 0.0004, val/iit_loss: 0.0246, val/IIA: 42.19%, val/accuracy: 65.62%, val/strict_accuracy: 48.98%\n",
      "Epoch 142: lr: 8.58e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0008, train/strict_loss: 0.0063, val/iit_loss: 0.0051, val/IIA: 55.31%, val/accuracy: 65.00%, val/strict_accuracy: 49.45%\n",
      "Epoch 143: lr: 8.57e-04, train/iit_loss: 0.0112, train/behavior_loss: 0.0007, train/strict_loss: 0.0002, val/iit_loss: 0.0049, val/IIA: 55.62%, val/accuracy: 66.25%, val/strict_accuracy: 51.17%\n",
      "Epoch 144: lr: 8.56e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0007, train/strict_loss: 0.0005, val/iit_loss: 0.0047, val/IIA: 56.56%, val/accuracy: 71.56%, val/strict_accuracy: 52.70%\n",
      "Epoch 145: lr: 8.55e-04, train/iit_loss: 0.0106, train/behavior_loss: 0.0006, train/strict_loss: 0.0006, val/iit_loss: 0.0045, val/IIA: 55.94%, val/accuracy: 70.94%, val/strict_accuracy: 52.93%\n",
      "Epoch 146: lr: 8.54e-04, train/iit_loss: 0.0022, train/behavior_loss: 0.0006, train/strict_loss: 0.0004, val/iit_loss: 0.0207, val/IIA: 42.81%, val/accuracy: 70.00%, val/strict_accuracy: 51.80%\n",
      "Epoch 147: lr: 8.53e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0006, train/strict_loss: 0.0006, val/iit_loss: 0.0200, val/IIA: 43.44%, val/accuracy: 69.38%, val/strict_accuracy: 51.64%\n",
      "Epoch 148: lr: 8.52e-04, train/iit_loss: 0.0097, train/behavior_loss: 0.0006, train/strict_loss: 0.0005, val/iit_loss: 0.0040, val/IIA: 58.44%, val/accuracy: 70.63%, val/strict_accuracy: 54.22%\n",
      "Epoch 149: lr: 8.51e-04, train/iit_loss: 0.0094, train/behavior_loss: 0.0005, train/strict_loss: 0.0002, val/iit_loss: 0.0041, val/IIA: 57.81%, val/accuracy: 71.88%, val/strict_accuracy: 55.47%\n",
      "Epoch 150: lr: 8.50e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0005, train/strict_loss: 0.0003, val/iit_loss: 0.0180, val/IIA: 44.69%, val/accuracy: 70.63%, val/strict_accuracy: 55.00%\n",
      "Epoch 151: lr: 8.49e-04, train/iit_loss: 0.0087, train/behavior_loss: 0.0005, train/strict_loss: 0.0001, val/iit_loss: 0.0171, val/IIA: 44.37%, val/accuracy: 69.69%, val/strict_accuracy: 53.55%\n",
      "Epoch 152: lr: 8.48e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0005, train/strict_loss: 0.0002, val/iit_loss: 0.0039, val/IIA: 60.94%, val/accuracy: 68.75%, val/strict_accuracy: 52.62%\n",
      "Epoch 153: lr: 8.47e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0005, train/strict_loss: 0.0001, val/iit_loss: 0.0038, val/IIA: 60.31%, val/accuracy: 68.75%, val/strict_accuracy: 53.28%\n",
      "Epoch 154: lr: 8.46e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0005, train/strict_loss: 0.0002, val/iit_loss: 0.0153, val/IIA: 45.94%, val/accuracy: 71.56%, val/strict_accuracy: 54.80%\n",
      "Epoch 155: lr: 8.45e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0005, train/strict_loss: 0.0001, val/iit_loss: 0.0149, val/IIA: 46.25%, val/accuracy: 75.00%, val/strict_accuracy: 55.62%\n",
      "Epoch 156: lr: 8.44e-04, train/iit_loss: 0.0073, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0144, val/IIA: 46.56%, val/accuracy: 75.00%, val/strict_accuracy: 55.94%\n",
      "Epoch 157: lr: 8.43e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0004, train/strict_loss: 0.0002, val/iit_loss: 0.0034, val/IIA: 65.00%, val/accuracy: 79.37%, val/strict_accuracy: 58.05%\n",
      "Epoch 158: lr: 8.42e-04, train/iit_loss: 0.0069, train/behavior_loss: 0.0004, train/strict_loss: 0.0005, val/iit_loss: 0.0034, val/IIA: 63.13%, val/accuracy: 73.75%, val/strict_accuracy: 55.16%\n",
      "Epoch 159: lr: 8.41e-04, train/iit_loss: 0.0067, train/behavior_loss: 0.0004, train/strict_loss: 0.0005, val/iit_loss: 0.0130, val/IIA: 47.81%, val/accuracy: 76.56%, val/strict_accuracy: 57.81%\n",
      "Epoch 160: lr: 8.40e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0004, train/strict_loss: 0.0004, val/iit_loss: 0.0126, val/IIA: 48.44%, val/accuracy: 77.19%, val/strict_accuracy: 57.89%\n",
      "Epoch 161: lr: 8.39e-04, train/iit_loss: 0.0062, train/behavior_loss: 0.0004, train/strict_loss: 0.0002, val/iit_loss: 0.0034, val/IIA: 65.00%, val/accuracy: 77.81%, val/strict_accuracy: 57.58%\n",
      "Epoch 162: lr: 8.38e-04, train/iit_loss: 0.0060, train/behavior_loss: 0.0004, train/strict_loss: 0.0002, val/iit_loss: 0.0118, val/IIA: 48.75%, val/accuracy: 77.19%, val/strict_accuracy: 57.03%\n",
      "Epoch 163: lr: 8.37e-04, train/iit_loss: 0.0057, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0113, val/IIA: 49.06%, val/accuracy: 76.56%, val/strict_accuracy: 57.19%\n",
      "Epoch 164: lr: 8.36e-04, train/iit_loss: 0.0055, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0109, val/IIA: 50.31%, val/accuracy: 77.81%, val/strict_accuracy: 58.20%\n",
      "Epoch 165: lr: 8.35e-04, train/iit_loss: 0.0052, train/behavior_loss: 0.0005, train/strict_loss: 0.0001, val/iit_loss: 0.0038, val/IIA: 63.13%, val/accuracy: 76.88%, val/strict_accuracy: 57.73%\n",
      "Epoch 166: lr: 8.34e-04, train/iit_loss: 0.0050, train/behavior_loss: 0.0005, train/strict_loss: 0.0060, val/iit_loss: 0.0039, val/IIA: 61.56%, val/accuracy: 76.56%, val/strict_accuracy: 56.37%\n",
      "Epoch 167: lr: 8.33e-04, train/iit_loss: 0.0047, train/behavior_loss: 0.0005, train/strict_loss: 0.0005, val/iit_loss: 0.0039, val/IIA: 60.94%, val/accuracy: 76.56%, val/strict_accuracy: 55.82%\n",
      "Epoch 168: lr: 8.32e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0005, train/strict_loss: 0.0004, val/iit_loss: 0.0039, val/IIA: 60.94%, val/accuracy: 74.06%, val/strict_accuracy: 53.83%\n",
      "Epoch 169: lr: 8.31e-04, train/iit_loss: 0.0043, train/behavior_loss: 0.0005, train/strict_loss: 0.0059, val/iit_loss: 0.0038, val/IIA: 61.25%, val/accuracy: 76.25%, val/strict_accuracy: 55.86%\n",
      "Epoch 170: lr: 8.30e-04, train/iit_loss: 0.0041, train/behavior_loss: 0.0005, train/strict_loss: 0.0002, val/iit_loss: 0.0038, val/IIA: 63.13%, val/accuracy: 80.00%, val/strict_accuracy: 59.10%\n",
      "Epoch 171: lr: 8.29e-04, train/iit_loss: 0.0019, train/behavior_loss: 0.0005, train/strict_loss: 0.0002, val/iit_loss: 0.0036, val/IIA: 64.38%, val/accuracy: 78.12%, val/strict_accuracy: 57.89%\n",
      "Epoch 172: lr: 8.28e-04, train/iit_loss: 0.0038, train/behavior_loss: 0.0005, train/strict_loss: 0.0059, val/iit_loss: 0.0035, val/IIA: 64.06%, val/accuracy: 75.94%, val/strict_accuracy: 57.58%\n",
      "Epoch 173: lr: 8.27e-04, train/iit_loss: 0.0036, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0034, val/IIA: 64.06%, val/accuracy: 79.37%, val/strict_accuracy: 59.18%\n",
      "Epoch 174: lr: 8.26e-04, train/iit_loss: 0.0035, train/behavior_loss: 0.0004, train/strict_loss: 0.0059, val/iit_loss: 0.0066, val/IIA: 52.81%, val/accuracy: 80.94%, val/strict_accuracy: 61.56%\n",
      "Epoch 175: lr: 8.25e-04, train/iit_loss: 0.0033, train/behavior_loss: 0.0004, train/strict_loss: 0.0005, val/iit_loss: 0.0063, val/IIA: 55.00%, val/accuracy: 86.56%, val/strict_accuracy: 64.22%\n",
      "Epoch 176: lr: 8.24e-04, train/iit_loss: 0.0032, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0032, val/IIA: 68.12%, val/accuracy: 85.31%, val/strict_accuracy: 63.75%\n",
      "Epoch 177: lr: 8.23e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0004, train/strict_loss: 0.0005, val/iit_loss: 0.0058, val/IIA: 55.31%, val/accuracy: 83.75%, val/strict_accuracy: 61.13%\n",
      "Epoch 178: lr: 8.22e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0056, val/IIA: 55.94%, val/accuracy: 84.06%, val/strict_accuracy: 61.84%\n",
      "Epoch 179: lr: 8.21e-04, train/iit_loss: 0.0029, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0054, val/IIA: 55.00%, val/accuracy: 85.62%, val/strict_accuracy: 65.47%\n",
      "Epoch 180: lr: 8.20e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0004, train/strict_loss: 0.0002, val/iit_loss: 0.0030, val/IIA: 70.94%, val/accuracy: 85.62%, val/strict_accuracy: 65.43%\n",
      "Epoch 181: lr: 8.19e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 64.06%, val/accuracy: 81.88%, val/strict_accuracy: 60.70%\n",
      "Epoch 182: lr: 8.18e-04, train/iit_loss: 0.0028, train/behavior_loss: 0.0004, train/strict_loss: 0.0002, val/iit_loss: 0.0029, val/IIA: 69.69%, val/accuracy: 84.38%, val/strict_accuracy: 63.79%\n",
      "Epoch 183: lr: 8.17e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0029, val/IIA: 71.88%, val/accuracy: 85.31%, val/strict_accuracy: 66.64%\n",
      "Epoch 184: lr: 8.16e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0049, val/IIA: 55.94%, val/accuracy: 88.44%, val/strict_accuracy: 67.30%\n",
      "Epoch 185: lr: 8.15e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0003, train/strict_loss: 0.0004, val/iit_loss: 0.0028, val/IIA: 71.25%, val/accuracy: 86.87%, val/strict_accuracy: 64.96%\n",
      "Epoch 186: lr: 8.14e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0005, val/iit_loss: 0.0052, val/IIA: 55.00%, val/accuracy: 80.62%, val/strict_accuracy: 61.17%\n",
      "Epoch 187: lr: 8.13e-04, train/iit_loss: 0.0026, train/behavior_loss: 0.0004, train/strict_loss: 0.0060, val/iit_loss: 0.0027, val/IIA: 72.50%, val/accuracy: 89.69%, val/strict_accuracy: 65.70%\n",
      "Epoch 188: lr: 8.12e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0003, train/strict_loss: 0.0003, val/iit_loss: 0.0047, val/IIA: 58.75%, val/accuracy: 91.87%, val/strict_accuracy: 68.95%\n",
      "Epoch 189: lr: 8.11e-04, train/iit_loss: 0.0024, train/behavior_loss: 0.0003, train/strict_loss: 0.0060, val/iit_loss: 0.0030, val/IIA: 71.25%, val/accuracy: 92.19%, val/strict_accuracy: 68.75%\n",
      "Epoch 190: lr: 8.10e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0047, val/IIA: 57.81%, val/accuracy: 84.38%, val/strict_accuracy: 62.38%\n",
      "Epoch 191: lr: 8.09e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0050, val/IIA: 57.19%, val/accuracy: 80.62%, val/strict_accuracy: 60.39%\n",
      "Epoch 192: lr: 8.08e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0004, train/strict_loss: 0.0006, val/iit_loss: 0.0042, val/IIA: 56.88%, val/accuracy: 91.87%, val/strict_accuracy: 65.00%\n",
      "Epoch 193: lr: 8.07e-04, train/iit_loss: 0.0022, train/behavior_loss: 0.0003, train/strict_loss: 0.0003, val/iit_loss: 0.0043, val/IIA: 60.31%, val/accuracy: 93.12%, val/strict_accuracy: 69.06%\n",
      "Epoch 194: lr: 8.06e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0004, train/strict_loss: 0.0006, val/iit_loss: 0.0029, val/IIA: 71.25%, val/accuracy: 93.75%, val/strict_accuracy: 68.98%\n",
      "Epoch 195: lr: 8.05e-04, train/iit_loss: 0.0021, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 70.94%, val/accuracy: 89.06%, val/strict_accuracy: 63.48%\n",
      "Epoch 196: lr: 8.04e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0029, val/IIA: 69.06%, val/accuracy: 85.00%, val/strict_accuracy: 61.60%\n",
      "Epoch 197: lr: 8.03e-04, train/iit_loss: 0.0023, train/behavior_loss: 0.0004, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 72.81%, val/accuracy: 95.31%, val/strict_accuracy: 67.34%\n",
      "Epoch 198: lr: 8.02e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0003, train/strict_loss: 0.0057, val/iit_loss: 0.0032, val/IIA: 68.75%, val/accuracy: 92.19%, val/strict_accuracy: 69.45%\n",
      "Epoch 199: lr: 8.01e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0003, train/strict_loss: 0.0057, val/iit_loss: 0.0029, val/IIA: 71.25%, val/accuracy: 93.12%, val/strict_accuracy: 69.14%\n",
      "Epoch 200: lr: 8.00e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0003, train/strict_loss: 0.0005, val/iit_loss: 0.0037, val/IIA: 61.25%, val/accuracy: 93.44%, val/strict_accuracy: 65.62%\n",
      "Epoch 201: lr: 7.99e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0003, train/strict_loss: 0.0057, val/iit_loss: 0.0038, val/IIA: 60.62%, val/accuracy: 90.62%, val/strict_accuracy: 63.67%\n",
      "Epoch 202: lr: 7.98e-04, train/iit_loss: 0.0020, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 73.12%, val/accuracy: 94.06%, val/strict_accuracy: 67.89%\n",
      "Epoch 203: lr: 7.97e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0035, val/IIA: 63.75%, val/accuracy: 93.44%, val/strict_accuracy: 69.49%\n",
      "Epoch 204: lr: 7.96e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0034, val/IIA: 64.06%, val/accuracy: 93.44%, val/strict_accuracy: 69.26%\n",
      "Epoch 205: lr: 7.95e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0027, val/IIA: 71.88%, val/accuracy: 94.06%, val/strict_accuracy: 67.93%\n",
      "Epoch 206: lr: 7.94e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0034, val/IIA: 62.19%, val/accuracy: 93.75%, val/strict_accuracy: 65.63%\n",
      "Epoch 207: lr: 7.93e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0005, val/iit_loss: 0.0035, val/IIA: 62.50%, val/accuracy: 93.12%, val/strict_accuracy: 64.96%\n",
      "Epoch 208: lr: 7.92e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0003, train/strict_loss: 0.0005, val/iit_loss: 0.0027, val/IIA: 75.63%, val/accuracy: 93.12%, val/strict_accuracy: 65.00%\n",
      "Epoch 209: lr: 7.91e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0003, train/strict_loss: 0.0003, val/iit_loss: 0.0032, val/IIA: 63.75%, val/accuracy: 93.44%, val/strict_accuracy: 68.87%\n",
      "Epoch 210: lr: 7.90e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0032, val/IIA: 64.38%, val/accuracy: 93.44%, val/strict_accuracy: 68.71%\n",
      "Epoch 211: lr: 7.89e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 73.75%, val/accuracy: 95.00%, val/strict_accuracy: 67.38%\n",
      "Epoch 212: lr: 7.88e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 73.75%, val/accuracy: 93.75%, val/strict_accuracy: 66.91%\n",
      "Epoch 213: lr: 7.87e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 64.69%, val/accuracy: 93.12%, val/strict_accuracy: 68.52%\n",
      "Epoch 214: lr: 7.86e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 68.12%, val/accuracy: 93.12%, val/strict_accuracy: 69.10%\n",
      "Epoch 215: lr: 7.85e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 65.94%, val/accuracy: 93.12%, val/strict_accuracy: 68.55%\n",
      "Epoch 216: lr: 7.84e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0055, val/iit_loss: 0.0026, val/IIA: 73.44%, val/accuracy: 93.12%, val/strict_accuracy: 68.79%\n",
      "Epoch 217: lr: 7.83e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 73.44%, val/accuracy: 92.19%, val/strict_accuracy: 68.32%\n",
      "Epoch 218: lr: 7.82e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0025, val/IIA: 74.37%, val/accuracy: 93.44%, val/strict_accuracy: 68.13%\n",
      "Epoch 219: lr: 7.81e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 71.56%, val/accuracy: 93.12%, val/strict_accuracy: 69.65%\n",
      "Epoch 220: lr: 7.80e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 72.19%, val/accuracy: 93.12%, val/strict_accuracy: 69.49%\n",
      "Epoch 221: lr: 7.79e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0055, val/iit_loss: 0.0025, val/IIA: 72.50%, val/accuracy: 93.12%, val/strict_accuracy: 69.49%\n",
      "Epoch 222: lr: 7.78e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0028, val/IIA: 69.69%, val/accuracy: 93.44%, val/strict_accuracy: 67.03%\n",
      "Epoch 223: lr: 7.77e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 70.63%, val/accuracy: 94.69%, val/strict_accuracy: 68.20%\n",
      "Epoch 224: lr: 7.76e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 70.63%, val/accuracy: 94.69%, val/strict_accuracy: 68.63%\n",
      "Epoch 225: lr: 7.75e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0024, val/IIA: 74.06%, val/accuracy: 94.69%, val/strict_accuracy: 68.28%\n",
      "Epoch 226: lr: 7.74e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 72.81%, val/accuracy: 95.00%, val/strict_accuracy: 69.10%\n",
      "Epoch 227: lr: 7.73e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0025, val/IIA: 72.19%, val/accuracy: 94.69%, val/strict_accuracy: 69.77%\n",
      "Epoch 228: lr: 7.72e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 70.94%, val/accuracy: 95.63%, val/strict_accuracy: 67.85%\n",
      "Epoch 229: lr: 7.71e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 70.63%, val/accuracy: 93.12%, val/strict_accuracy: 66.80%\n",
      "Epoch 230: lr: 7.70e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 69.06%, val/accuracy: 95.94%, val/strict_accuracy: 67.97%\n",
      "Epoch 231: lr: 7.69e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 70.63%, val/accuracy: 95.31%, val/strict_accuracy: 68.79%\n",
      "Epoch 232: lr: 7.68e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 70.94%, val/accuracy: 95.94%, val/strict_accuracy: 69.18%\n",
      "Epoch 233: lr: 7.67e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 70.94%, val/accuracy: 95.94%, val/strict_accuracy: 68.67%\n",
      "Epoch 234: lr: 7.66e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0025, val/IIA: 71.25%, val/accuracy: 95.94%, val/strict_accuracy: 69.41%\n",
      "Epoch 235: lr: 7.65e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 73.75%, val/accuracy: 95.94%, val/strict_accuracy: 68.05%\n",
      "Epoch 236: lr: 7.64e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 73.12%, val/accuracy: 96.25%, val/strict_accuracy: 68.63%\n",
      "Epoch 237: lr: 7.63e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 73.75%, val/accuracy: 95.31%, val/strict_accuracy: 69.37%\n",
      "Epoch 238: lr: 7.62e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 73.12%, val/accuracy: 96.25%, val/strict_accuracy: 68.75%\n",
      "Epoch 239: lr: 7.61e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0053, val/iit_loss: 0.0023, val/IIA: 74.37%, val/accuracy: 95.94%, val/strict_accuracy: 67.62%\n",
      "Epoch 240: lr: 7.60e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0022, val/IIA: 74.69%, val/accuracy: 96.56%, val/strict_accuracy: 67.50%\n",
      "Epoch 241: lr: 7.59e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0023, val/IIA: 72.50%, val/accuracy: 95.63%, val/strict_accuracy: 69.92%\n",
      "Epoch 242: lr: 7.58e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0053, val/iit_loss: 0.0025, val/IIA: 72.19%, val/accuracy: 93.75%, val/strict_accuracy: 70.35%\n",
      "Epoch 243: lr: 7.57e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0005, val/iit_loss: 0.0023, val/IIA: 74.06%, val/accuracy: 94.38%, val/strict_accuracy: 69.96%\n",
      "Epoch 244: lr: 7.56e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 76.56%, val/accuracy: 97.19%, val/strict_accuracy: 67.23%\n",
      "Epoch 245: lr: 7.55e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0023, val/IIA: 75.00%, val/accuracy: 93.12%, val/strict_accuracy: 65.98%\n",
      "Epoch 246: lr: 7.54e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0023, val/IIA: 73.12%, val/accuracy: 96.25%, val/strict_accuracy: 68.59%\n",
      "Epoch 247: lr: 7.53e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 74.69%, val/accuracy: 95.00%, val/strict_accuracy: 69.84%\n",
      "Epoch 248: lr: 7.52e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 74.06%, val/accuracy: 94.38%, val/strict_accuracy: 69.69%\n",
      "Epoch 249: lr: 7.51e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 71.88%, val/accuracy: 96.25%, val/strict_accuracy: 68.24%\n",
      "Epoch 250: lr: 7.50e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 71.88%, val/accuracy: 96.25%, val/strict_accuracy: 68.05%\n",
      "Epoch 251: lr: 7.49e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0021, val/IIA: 74.69%, val/accuracy: 96.25%, val/strict_accuracy: 68.95%\n",
      "Epoch 252: lr: 7.48e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0021, val/IIA: 75.00%, val/accuracy: 95.94%, val/strict_accuracy: 69.77%\n",
      "Epoch 253: lr: 7.47e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0021, val/IIA: 76.56%, val/accuracy: 95.31%, val/strict_accuracy: 70.08%\n",
      "Epoch 254: lr: 7.46e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0052, val/iit_loss: 0.0021, val/IIA: 75.63%, val/accuracy: 96.25%, val/strict_accuracy: 69.57%\n",
      "Epoch 255: lr: 7.45e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0021, val/IIA: 77.50%, val/accuracy: 97.50%, val/strict_accuracy: 68.24%\n",
      "Epoch 256: lr: 7.44e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 77.50%, val/accuracy: 97.81%, val/strict_accuracy: 69.22%\n",
      "Epoch 257: lr: 7.43e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0020, val/IIA: 74.37%, val/accuracy: 95.63%, val/strict_accuracy: 70.23%\n",
      "Epoch 258: lr: 7.42e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0021, val/IIA: 73.75%, val/accuracy: 94.69%, val/strict_accuracy: 70.98%\n",
      "Epoch 259: lr: 7.41e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 75.00%, val/accuracy: 96.25%, val/strict_accuracy: 69.92%\n",
      "Epoch 260: lr: 7.40e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0021, val/IIA: 74.37%, val/accuracy: 97.50%, val/strict_accuracy: 69.77%\n",
      "Epoch 261: lr: 7.39e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0020, val/IIA: 75.31%, val/accuracy: 96.25%, val/strict_accuracy: 69.57%\n",
      "Epoch 262: lr: 7.38e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0020, val/IIA: 74.69%, val/accuracy: 94.69%, val/strict_accuracy: 70.51%\n",
      "Epoch 263: lr: 7.37e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0052, val/iit_loss: 0.0020, val/IIA: 75.94%, val/accuracy: 94.38%, val/strict_accuracy: 70.00%\n",
      "Epoch 264: lr: 7.36e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 76.25%, val/accuracy: 95.00%, val/strict_accuracy: 68.95%\n",
      "Epoch 265: lr: 7.35e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 74.06%, val/accuracy: 95.00%, val/strict_accuracy: 68.95%\n",
      "Epoch 266: lr: 7.34e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0020, val/IIA: 75.94%, val/accuracy: 95.00%, val/strict_accuracy: 68.98%\n",
      "Epoch 267: lr: 7.33e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 75.31%, val/accuracy: 95.63%, val/strict_accuracy: 69.69%\n",
      "Epoch 268: lr: 7.32e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0020, val/IIA: 74.37%, val/accuracy: 96.56%, val/strict_accuracy: 69.38%\n",
      "Epoch 269: lr: 7.31e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0019, val/IIA: 76.88%, val/accuracy: 97.19%, val/strict_accuracy: 69.10%\n",
      "Epoch 270: lr: 7.30e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0051, val/iit_loss: 0.0019, val/IIA: 78.75%, val/accuracy: 98.44%, val/strict_accuracy: 69.10%\n",
      "Epoch 271: lr: 7.29e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0051, val/iit_loss: 0.0019, val/IIA: 76.56%, val/accuracy: 98.44%, val/strict_accuracy: 70.23%\n",
      "Epoch 272: lr: 7.28e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0019, val/IIA: 76.25%, val/accuracy: 98.44%, val/strict_accuracy: 69.84%\n",
      "Epoch 273: lr: 7.27e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0019, val/IIA: 75.31%, val/accuracy: 98.44%, val/strict_accuracy: 69.77%\n",
      "Epoch 274: lr: 7.26e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0018, val/IIA: 76.25%, val/accuracy: 98.44%, val/strict_accuracy: 70.31%\n",
      "Epoch 275: lr: 7.25e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 77.50%, val/accuracy: 96.88%, val/strict_accuracy: 72.07%\n",
      "Epoch 276: lr: 7.24e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 77.50%, val/accuracy: 95.00%, val/strict_accuracy: 71.64%\n",
      "Epoch 277: lr: 7.23e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0050, val/iit_loss: 0.0019, val/IIA: 75.31%, val/accuracy: 96.88%, val/strict_accuracy: 70.66%\n",
      "Epoch 278: lr: 7.22e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0019, val/IIA: 75.94%, val/accuracy: 97.19%, val/strict_accuracy: 69.53%\n",
      "Epoch 279: lr: 7.21e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0049, val/iit_loss: 0.0019, val/IIA: 75.94%, val/accuracy: 97.19%, val/strict_accuracy: 68.87%\n",
      "Epoch 280: lr: 7.20e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0019, val/IIA: 75.31%, val/accuracy: 95.31%, val/strict_accuracy: 70.16%\n",
      "Epoch 281: lr: 7.19e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 77.81%, val/accuracy: 96.88%, val/strict_accuracy: 71.17%\n",
      "Epoch 282: lr: 7.18e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0018, val/IIA: 74.69%, val/accuracy: 97.19%, val/strict_accuracy: 68.95%\n",
      "Epoch 283: lr: 7.17e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 78.12%, val/accuracy: 96.25%, val/strict_accuracy: 68.09%\n",
      "Epoch 284: lr: 7.16e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 75.63%, val/accuracy: 98.44%, val/strict_accuracy: 70.62%\n",
      "Epoch 285: lr: 7.15e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 75.94%, val/accuracy: 98.44%, val/strict_accuracy: 71.68%\n",
      "Epoch 286: lr: 7.14e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0018, val/IIA: 75.31%, val/accuracy: 98.44%, val/strict_accuracy: 71.68%\n",
      "Epoch 287: lr: 7.13e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 79.37%, val/accuracy: 98.44%, val/strict_accuracy: 70.04%\n",
      "Epoch 288: lr: 7.12e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0004, val/iit_loss: 0.0019, val/IIA: 76.25%, val/accuracy: 96.56%, val/strict_accuracy: 69.34%\n",
      "Epoch 289: lr: 7.11e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 76.88%, val/accuracy: 98.44%, val/strict_accuracy: 71.76%\n",
      "Epoch 290: lr: 7.10e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0017, val/IIA: 77.50%, val/accuracy: 95.31%, val/strict_accuracy: 72.11%\n",
      "Epoch 291: lr: 7.09e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 75.94%, val/accuracy: 95.31%, val/strict_accuracy: 71.87%\n",
      "Epoch 292: lr: 7.08e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 78.44%, val/accuracy: 97.19%, val/strict_accuracy: 69.10%\n",
      "Epoch 293: lr: 7.07e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 73.44%, val/accuracy: 96.25%, val/strict_accuracy: 68.05%\n",
      "Epoch 294: lr: 7.06e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0017, val/IIA: 77.50%, val/accuracy: 97.19%, val/strict_accuracy: 71.13%\n",
      "Epoch 295: lr: 7.05e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 77.50%, val/accuracy: 95.63%, val/strict_accuracy: 72.23%\n",
      "Epoch 296: lr: 7.04e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 78.44%, val/accuracy: 97.19%, val/strict_accuracy: 72.30%\n",
      "Epoch 297: lr: 7.03e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 79.37%, val/accuracy: 97.19%, val/strict_accuracy: 68.20%\n",
      "Epoch 298: lr: 7.02e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0016, val/IIA: 80.31%, val/accuracy: 97.19%, val/strict_accuracy: 68.28%\n",
      "Epoch 299: lr: 7.01e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 79.37%, val/accuracy: 97.19%, val/strict_accuracy: 72.15%\n",
      "Epoch 300: lr: 7.00e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0004, val/iit_loss: 0.0017, val/IIA: 79.37%, val/accuracy: 97.50%, val/strict_accuracy: 72.62%\n",
      "Epoch 301: lr: 6.99e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 77.50%, val/accuracy: 97.19%, val/strict_accuracy: 69.92%\n",
      "Epoch 302: lr: 6.98e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0018, val/IIA: 76.56%, val/accuracy: 97.19%, val/strict_accuracy: 69.30%\n",
      "Epoch 303: lr: 6.97e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0015, val/IIA: 80.94%, val/accuracy: 97.19%, val/strict_accuracy: 70.20%\n",
      "Epoch 304: lr: 6.96e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 79.69%, val/accuracy: 97.50%, val/strict_accuracy: 72.66%\n",
      "Epoch 305: lr: 6.95e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0047, val/iit_loss: 0.0017, val/IIA: 79.69%, val/accuracy: 97.50%, val/strict_accuracy: 72.77%\n",
      "Epoch 306: lr: 6.94e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 83.13%, val/accuracy: 98.44%, val/strict_accuracy: 70.39%\n",
      "Epoch 307: lr: 6.93e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0047, val/iit_loss: 0.0018, val/IIA: 78.44%, val/accuracy: 97.19%, val/strict_accuracy: 70.12%\n",
      "Epoch 308: lr: 6.92e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 79.06%, val/accuracy: 98.44%, val/strict_accuracy: 72.54%\n",
      "Epoch 309: lr: 6.91e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 79.06%, val/accuracy: 96.88%, val/strict_accuracy: 73.32%\n",
      "Epoch 310: lr: 6.90e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 77.81%, val/accuracy: 97.19%, val/strict_accuracy: 72.07%\n",
      "Epoch 311: lr: 6.89e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0019, val/IIA: 75.63%, val/accuracy: 97.19%, val/strict_accuracy: 69.26%\n",
      "Epoch 312: lr: 6.88e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 80.94%, val/accuracy: 96.25%, val/strict_accuracy: 68.87%\n",
      "Epoch 313: lr: 6.87e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 80.62%, val/accuracy: 97.19%, val/strict_accuracy: 72.07%\n",
      "Epoch 314: lr: 6.86e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 80.00%, val/accuracy: 96.56%, val/strict_accuracy: 72.34%\n",
      "Epoch 315: lr: 6.85e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 77.81%, val/accuracy: 97.19%, val/strict_accuracy: 70.78%\n",
      "Epoch 316: lr: 6.84e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 81.56%, val/accuracy: 96.25%, val/strict_accuracy: 68.24%\n",
      "Epoch 317: lr: 6.83e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0017, val/IIA: 76.88%, val/accuracy: 97.19%, val/strict_accuracy: 69.30%\n",
      "Epoch 318: lr: 6.82e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 79.69%, val/accuracy: 98.44%, val/strict_accuracy: 72.93%\n",
      "Epoch 319: lr: 6.81e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 78.75%, val/accuracy: 97.50%, val/strict_accuracy: 72.50%\n",
      "Epoch 320: lr: 6.80e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.06%, val/accuracy: 97.19%, val/strict_accuracy: 70.78%\n",
      "Epoch 321: lr: 6.79e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0018, val/IIA: 76.25%, val/accuracy: 97.19%, val/strict_accuracy: 69.41%\n",
      "Epoch 322: lr: 6.78e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 78.12%, val/accuracy: 98.44%, val/strict_accuracy: 71.52%\n",
      "Epoch 323: lr: 6.77e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0016, val/IIA: 80.94%, val/accuracy: 97.50%, val/strict_accuracy: 72.81%\n",
      "Epoch 324: lr: 6.76e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0015, val/IIA: 80.31%, val/accuracy: 98.44%, val/strict_accuracy: 72.85%\n",
      "Epoch 325: lr: 6.75e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.69%, val/accuracy: 97.19%, val/strict_accuracy: 70.86%\n",
      "Epoch 326: lr: 6.74e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0018, val/IIA: 75.94%, val/accuracy: 96.88%, val/strict_accuracy: 68.20%\n",
      "Epoch 327: lr: 6.73e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.38%, val/accuracy: 97.19%, val/strict_accuracy: 70.66%\n",
      "Epoch 328: lr: 6.72e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 81.56%, val/accuracy: 98.44%, val/strict_accuracy: 72.34%\n",
      "Epoch 329: lr: 6.71e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 79.37%, val/accuracy: 97.19%, val/strict_accuracy: 72.11%\n",
      "Epoch 330: lr: 6.70e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 80.00%, val/accuracy: 97.19%, val/strict_accuracy: 70.98%\n",
      "Epoch 331: lr: 6.69e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.00%, val/accuracy: 97.19%, val/strict_accuracy: 70.94%\n",
      "Epoch 332: lr: 6.68e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 79.69%, val/accuracy: 97.19%, val/strict_accuracy: 72.27%\n",
      "Epoch 333: lr: 6.67e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.62%, val/accuracy: 97.19%, val/strict_accuracy: 72.30%\n",
      "Epoch 334: lr: 6.66e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.94%, val/accuracy: 97.19%, val/strict_accuracy: 71.84%\n",
      "Epoch 335: lr: 6.65e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0046, val/iit_loss: 0.0012, val/IIA: 85.00%, val/accuracy: 97.19%, val/strict_accuracy: 71.29%\n",
      "Epoch 336: lr: 6.64e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 97.19%, val/strict_accuracy: 72.15%\n",
      "Epoch 337: lr: 6.63e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.31%, val/accuracy: 98.44%, val/strict_accuracy: 72.46%\n",
      "Epoch 338: lr: 6.62e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.00%, val/accuracy: 97.19%, val/strict_accuracy: 72.46%\n",
      "Epoch 339: lr: 6.61e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0046, val/iit_loss: 0.0014, val/IIA: 83.44%, val/accuracy: 97.19%, val/strict_accuracy: 71.88%\n",
      "Epoch 340: lr: 6.60e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 81.88%, val/accuracy: 98.44%, val/strict_accuracy: 72.89%\n",
      "Epoch 341: lr: 6.59e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.00%, val/accuracy: 98.44%, val/strict_accuracy: 72.93%\n",
      "Epoch 342: lr: 6.58e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0045, val/iit_loss: 0.0014, val/IIA: 82.19%, val/accuracy: 98.44%, val/strict_accuracy: 72.93%\n",
      "Epoch 343: lr: 6.57e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.62%, val/accuracy: 97.19%, val/strict_accuracy: 72.19%\n",
      "Epoch 344: lr: 6.56e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 81.88%, val/accuracy: 97.19%, val/strict_accuracy: 71.91%\n",
      "Epoch 345: lr: 6.55e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.62%, val/accuracy: 98.44%, val/strict_accuracy: 72.77%\n",
      "Epoch 346: lr: 6.54e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 82.19%, val/accuracy: 98.44%, val/strict_accuracy: 72.42%\n",
      "Epoch 347: lr: 6.53e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 81.88%, val/accuracy: 97.19%, val/strict_accuracy: 72.23%\n",
      "Epoch 348: lr: 6.52e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 82.19%, val/accuracy: 97.19%, val/strict_accuracy: 70.82%\n",
      "Epoch 349: lr: 6.51e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 97.19%, val/strict_accuracy: 71.17%\n",
      "Epoch 350: lr: 6.50e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 97.19%, val/strict_accuracy: 72.50%\n",
      "Epoch 351: lr: 6.49e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 97.19%, val/strict_accuracy: 73.01%\n",
      "Epoch 352: lr: 6.48e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.19%, val/accuracy: 97.19%, val/strict_accuracy: 72.54%\n",
      "Epoch 353: lr: 6.47e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 83.13%, val/accuracy: 97.19%, val/strict_accuracy: 72.23%\n",
      "Epoch 354: lr: 6.46e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 82.50%, val/accuracy: 97.19%, val/strict_accuracy: 72.15%\n",
      "Epoch 355: lr: 6.45e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 87.50%, val/accuracy: 97.19%, val/strict_accuracy: 73.01%\n",
      "Epoch 356: lr: 6.44e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 98.44%, val/strict_accuracy: 71.91%\n",
      "Epoch 357: lr: 6.43e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 81.88%, val/accuracy: 97.19%, val/strict_accuracy: 72.46%\n",
      "Epoch 358: lr: 6.42e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.87%, val/accuracy: 96.56%, val/strict_accuracy: 71.25%\n",
      "Epoch 359: lr: 6.41e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.50%, val/accuracy: 97.19%, val/strict_accuracy: 71.41%\n",
      "Epoch 360: lr: 6.40e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.69%, val/accuracy: 97.19%, val/strict_accuracy: 71.76%\n",
      "Epoch 361: lr: 6.39e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 87.81%, val/accuracy: 97.19%, val/strict_accuracy: 72.23%\n",
      "Epoch 362: lr: 6.38e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 82.81%, val/accuracy: 97.19%, val/strict_accuracy: 71.64%\n",
      "Epoch 363: lr: 6.37e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 87.81%, val/accuracy: 97.19%, val/strict_accuracy: 71.72%\n",
      "Epoch 364: lr: 6.36e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0043, val/iit_loss: 0.0010, val/IIA: 87.50%, val/accuracy: 98.44%, val/strict_accuracy: 72.89%\n",
      "Epoch 365: lr: 6.35e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0010, val/IIA: 87.81%, val/accuracy: 98.44%, val/strict_accuracy: 73.32%\n",
      "Epoch 366: lr: 6.34e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 83.75%, val/accuracy: 97.19%, val/strict_accuracy: 72.97%\n",
      "Epoch 367: lr: 6.33e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0043, val/iit_loss: 0.0012, val/IIA: 83.75%, val/accuracy: 97.19%, val/strict_accuracy: 72.97%\n",
      "Epoch 368: lr: 6.32e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 88.44%, val/accuracy: 97.19%, val/strict_accuracy: 72.93%\n",
      "Epoch 369: lr: 6.31e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 84.69%, val/accuracy: 97.19%, val/strict_accuracy: 73.01%\n",
      "Epoch 370: lr: 6.30e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.38%, val/accuracy: 97.19%, val/strict_accuracy: 72.97%\n",
      "Epoch 371: lr: 6.29e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0009, val/IIA: 88.44%, val/accuracy: 97.19%, val/strict_accuracy: 73.05%\n",
      "Epoch 372: lr: 6.28e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 88.44%, val/accuracy: 97.19%, val/strict_accuracy: 73.09%\n",
      "Epoch 373: lr: 6.27e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.62%, val/accuracy: 97.19%, val/strict_accuracy: 72.85%\n",
      "Epoch 374: lr: 6.26e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 85.00%, val/accuracy: 97.19%, val/strict_accuracy: 72.38%\n",
      "Epoch 375: lr: 6.25e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0041, val/iit_loss: 0.0011, val/IIA: 86.87%, val/accuracy: 97.19%, val/strict_accuracy: 72.11%\n",
      "Epoch 376: lr: 6.24e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0041, val/iit_loss: 0.0009, val/IIA: 88.44%, val/accuracy: 96.56%, val/strict_accuracy: 72.42%\n",
      "Epoch 377: lr: 6.23e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.19%, val/accuracy: 97.19%, val/strict_accuracy: 72.11%\n",
      "Epoch 378: lr: 6.22e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0041, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 97.19%, val/strict_accuracy: 72.34%\n",
      "Epoch 379: lr: 6.21e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 88.13%, val/accuracy: 97.19%, val/strict_accuracy: 72.62%\n",
      "Epoch 380: lr: 6.20e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 88.13%, val/accuracy: 97.19%, val/strict_accuracy: 72.42%\n",
      "Epoch 381: lr: 6.19e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 88.13%, val/accuracy: 97.19%, val/strict_accuracy: 73.05%\n",
      "Epoch 382: lr: 6.18e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 88.13%, val/accuracy: 97.19%, val/strict_accuracy: 72.66%\n",
      "Epoch 383: lr: 6.17e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 88.44%, val/accuracy: 98.44%, val/strict_accuracy: 73.67%\n",
      "Epoch 384: lr: 6.16e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0009, val/IIA: 88.75%, val/accuracy: 98.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 385: lr: 6.15e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0040, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 98.44%, val/strict_accuracy: 74.02%\n",
      "Epoch 386: lr: 6.14e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0009, val/IIA: 90.00%, val/accuracy: 98.44%, val/strict_accuracy: 73.40%\n",
      "Epoch 387: lr: 6.13e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.62%, val/accuracy: 98.44%, val/strict_accuracy: 73.40%\n",
      "Epoch 388: lr: 6.12e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 87.81%, val/accuracy: 98.44%, val/strict_accuracy: 73.52%\n",
      "Epoch 389: lr: 6.11e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 88.44%, val/accuracy: 97.19%, val/strict_accuracy: 73.09%\n",
      "Epoch 390: lr: 6.10e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0039, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 97.50%, val/strict_accuracy: 72.46%\n",
      "Epoch 391: lr: 6.09e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 96.88%, val/strict_accuracy: 72.03%\n",
      "Epoch 392: lr: 6.08e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0008, val/IIA: 88.44%, val/accuracy: 96.88%, val/strict_accuracy: 72.73%\n",
      "Epoch 393: lr: 6.07e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 96.56%, val/strict_accuracy: 71.64%\n",
      "Epoch 394: lr: 6.06e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 85.31%, val/accuracy: 96.88%, val/strict_accuracy: 70.98%\n",
      "Epoch 395: lr: 6.05e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.87%, val/accuracy: 96.56%, val/strict_accuracy: 72.34%\n",
      "Epoch 396: lr: 6.04e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 89.69%, val/accuracy: 96.88%, val/strict_accuracy: 72.73%\n",
      "Epoch 397: lr: 6.03e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 84.38%, val/accuracy: 96.88%, val/strict_accuracy: 71.60%\n",
      "Epoch 398: lr: 6.02e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 83.75%, val/accuracy: 96.88%, val/strict_accuracy: 71.48%\n",
      "Epoch 399: lr: 6.01e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0008, val/IIA: 89.69%, val/accuracy: 96.88%, val/strict_accuracy: 72.38%\n",
      "Epoch 400: lr: 6.00e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 90.62%, val/accuracy: 96.56%, val/strict_accuracy: 73.36%\n",
      "Epoch 401: lr: 5.99e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 90.31%, val/accuracy: 97.50%, val/strict_accuracy: 72.19%\n",
      "Epoch 402: lr: 5.98e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0037, val/iit_loss: 0.0011, val/IIA: 84.69%, val/accuracy: 96.88%, val/strict_accuracy: 71.99%\n",
      "Epoch 403: lr: 5.97e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 85.62%, val/accuracy: 97.50%, val/strict_accuracy: 71.76%\n",
      "Epoch 404: lr: 5.96e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 88.13%, val/accuracy: 96.56%, val/strict_accuracy: 73.44%\n",
      "Epoch 405: lr: 5.95e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 86.56%, val/accuracy: 97.50%, val/strict_accuracy: 72.19%\n",
      "Epoch 406: lr: 5.94e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0007, val/IIA: 91.25%, val/accuracy: 96.88%, val/strict_accuracy: 71.80%\n",
      "Epoch 407: lr: 5.93e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 85.94%, val/accuracy: 96.88%, val/strict_accuracy: 72.03%\n",
      "Epoch 408: lr: 5.92e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 90.62%, val/accuracy: 97.50%, val/strict_accuracy: 72.46%\n",
      "Epoch 409: lr: 5.91e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 90.31%, val/accuracy: 97.50%, val/strict_accuracy: 72.38%\n",
      "Epoch 410: lr: 5.90e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.62%, val/accuracy: 96.88%, val/strict_accuracy: 71.84%\n",
      "Epoch 411: lr: 5.89e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 96.88%, val/strict_accuracy: 72.30%\n",
      "Epoch 412: lr: 5.88e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 90.00%, val/accuracy: 97.50%, val/strict_accuracy: 73.12%\n",
      "Epoch 413: lr: 5.87e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 90.94%, val/accuracy: 96.88%, val/strict_accuracy: 72.38%\n",
      "Epoch 414: lr: 5.86e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 96.88%, val/strict_accuracy: 72.07%\n",
      "Epoch 415: lr: 5.85e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0007, val/IIA: 91.25%, val/accuracy: 96.88%, val/strict_accuracy: 72.58%\n",
      "Epoch 416: lr: 5.84e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 86.87%, val/accuracy: 96.88%, val/strict_accuracy: 72.70%\n",
      "Epoch 417: lr: 5.83e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 86.56%, val/accuracy: 96.88%, val/strict_accuracy: 72.38%\n",
      "Epoch 418: lr: 5.82e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 86.56%, val/accuracy: 97.50%, val/strict_accuracy: 72.70%\n",
      "Epoch 419: lr: 5.81e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 86.56%, val/accuracy: 97.50%, val/strict_accuracy: 72.85%\n",
      "Epoch 420: lr: 5.80e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0036, val/iit_loss: 0.0010, val/IIA: 87.50%, val/accuracy: 98.75%, val/strict_accuracy: 73.16%\n",
      "Epoch 421: lr: 5.79e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 87.50%, val/accuracy: 98.75%, val/strict_accuracy: 73.36%\n",
      "Epoch 422: lr: 5.78e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0007, val/IIA: 91.87%, val/accuracy: 98.75%, val/strict_accuracy: 73.48%\n",
      "Epoch 423: lr: 5.77e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 88.44%, val/accuracy: 98.44%, val/strict_accuracy: 73.16%\n",
      "Epoch 424: lr: 5.76e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 87.50%, val/accuracy: 98.75%, val/strict_accuracy: 73.20%\n",
      "Epoch 425: lr: 5.75e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 87.50%, val/accuracy: 97.50%, val/strict_accuracy: 73.16%\n",
      "Epoch 426: lr: 5.74e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0007, val/IIA: 91.87%, val/accuracy: 98.75%, val/strict_accuracy: 73.52%\n",
      "Epoch 427: lr: 5.73e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 88.13%, val/accuracy: 98.75%, val/strict_accuracy: 73.75%\n",
      "Epoch 428: lr: 5.72e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 91.87%, val/accuracy: 97.50%, val/strict_accuracy: 73.44%\n",
      "Epoch 429: lr: 5.71e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 86.87%, val/accuracy: 96.88%, val/strict_accuracy: 72.58%\n",
      "Epoch 430: lr: 5.70e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.50%, val/accuracy: 96.88%, val/strict_accuracy: 73.16%\n",
      "Epoch 431: lr: 5.69e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.50%, val/accuracy: 96.88%, val/strict_accuracy: 73.32%\n",
      "Epoch 432: lr: 5.68e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0007, val/IIA: 91.56%, val/accuracy: 96.88%, val/strict_accuracy: 73.01%\n",
      "Epoch 433: lr: 5.67e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 96.88%, val/strict_accuracy: 71.99%\n",
      "Epoch 434: lr: 5.66e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0034, val/iit_loss: 0.0007, val/IIA: 92.50%, val/accuracy: 96.88%, val/strict_accuracy: 72.85%\n",
      "Epoch 435: lr: 5.65e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0010, val/IIA: 87.81%, val/accuracy: 97.50%, val/strict_accuracy: 73.52%\n",
      "Epoch 436: lr: 5.64e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.19%, val/accuracy: 97.50%, val/strict_accuracy: 72.97%\n",
      "Epoch 437: lr: 5.63e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 96.88%, val/strict_accuracy: 72.58%\n",
      "Epoch 438: lr: 5.62e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0034, val/iit_loss: 0.0006, val/IIA: 92.19%, val/accuracy: 97.50%, val/strict_accuracy: 73.32%\n",
      "Epoch 439: lr: 5.61e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 93.12%, val/accuracy: 97.50%, val/strict_accuracy: 73.55%\n",
      "Epoch 440: lr: 5.60e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.19%, val/accuracy: 96.88%, val/strict_accuracy: 73.05%\n",
      "Epoch 441: lr: 5.59e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0033, val/iit_loss: 0.0010, val/IIA: 87.50%, val/accuracy: 96.56%, val/strict_accuracy: 73.01%\n",
      "Epoch 442: lr: 5.58e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 88.13%, val/accuracy: 97.50%, val/strict_accuracy: 73.32%\n",
      "Epoch 443: lr: 5.57e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0010, val/IIA: 88.13%, val/accuracy: 97.50%, val/strict_accuracy: 73.75%\n",
      "Epoch 444: lr: 5.56e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0007, val/IIA: 92.81%, val/accuracy: 97.50%, val/strict_accuracy: 73.52%\n",
      "Epoch 445: lr: 5.55e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0006, val/IIA: 92.50%, val/accuracy: 97.50%, val/strict_accuracy: 73.48%\n",
      "Epoch 446: lr: 5.54e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.81%, val/accuracy: 98.75%, val/strict_accuracy: 73.67%\n",
      "Epoch 447: lr: 5.53e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0006, val/IIA: 92.50%, val/accuracy: 98.75%, val/strict_accuracy: 73.59%\n",
      "Epoch 448: lr: 5.52e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.81%, val/accuracy: 97.50%, val/strict_accuracy: 73.40%\n",
      "Epoch 449: lr: 5.51e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0032, val/iit_loss: 0.0010, val/IIA: 86.87%, val/accuracy: 97.50%, val/strict_accuracy: 73.44%\n",
      "Epoch 450: lr: 5.50e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.50%, val/accuracy: 97.50%, val/strict_accuracy: 73.59%\n",
      "Epoch 451: lr: 5.49e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.19%, val/accuracy: 96.88%, val/strict_accuracy: 73.09%\n",
      "Epoch 452: lr: 5.48e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0006, val/IIA: 92.19%, val/accuracy: 96.88%, val/strict_accuracy: 73.32%\n",
      "Epoch 453: lr: 5.47e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 86.56%, val/accuracy: 96.88%, val/strict_accuracy: 73.48%\n",
      "Epoch 454: lr: 5.46e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0031, val/iit_loss: 0.0006, val/IIA: 92.81%, val/accuracy: 96.88%, val/strict_accuracy: 73.48%\n",
      "Epoch 455: lr: 5.45e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.19%, val/accuracy: 96.56%, val/strict_accuracy: 73.16%\n",
      "Epoch 456: lr: 5.44e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0006, val/IIA: 92.81%, val/accuracy: 96.88%, val/strict_accuracy: 73.28%\n",
      "Epoch 457: lr: 5.43e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 85.94%, val/accuracy: 97.50%, val/strict_accuracy: 73.59%\n",
      "Epoch 458: lr: 5.42e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 88.13%, val/accuracy: 97.50%, val/strict_accuracy: 73.40%\n",
      "Epoch 459: lr: 5.41e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 87.50%, val/accuracy: 97.50%, val/strict_accuracy: 73.71%\n",
      "Epoch 460: lr: 5.40e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 85.94%, val/accuracy: 98.75%, val/strict_accuracy: 73.91%\n",
      "Epoch 461: lr: 5.39e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 87.50%, val/accuracy: 97.50%, val/strict_accuracy: 73.75%\n",
      "Epoch 462: lr: 5.38e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.50%, val/accuracy: 96.88%, val/strict_accuracy: 73.71%\n",
      "Epoch 463: lr: 5.37e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0010, val/IIA: 87.19%, val/accuracy: 97.50%, val/strict_accuracy: 73.59%\n",
      "Epoch 464: lr: 5.36e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0006, val/IIA: 92.50%, val/accuracy: 97.50%, val/strict_accuracy: 73.87%\n",
      "Epoch 465: lr: 5.35e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0006, val/IIA: 92.19%, val/accuracy: 96.88%, val/strict_accuracy: 73.28%\n",
      "Epoch 466: lr: 5.34e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 96.88%, val/strict_accuracy: 73.36%\n",
      "Epoch 467: lr: 5.33e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 97.50%, val/strict_accuracy: 73.48%\n",
      "Epoch 468: lr: 5.32e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.50%, val/accuracy: 96.56%, val/strict_accuracy: 73.32%\n",
      "Epoch 469: lr: 5.31e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 96.88%, val/strict_accuracy: 73.09%\n",
      "Epoch 470: lr: 5.30e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.50%, val/accuracy: 97.50%, val/strict_accuracy: 73.36%\n",
      "Epoch 471: lr: 5.29e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 97.50%, val/strict_accuracy: 73.59%\n",
      "Epoch 472: lr: 5.28e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.19%, val/accuracy: 97.50%, val/strict_accuracy: 73.83%\n",
      "Epoch 473: lr: 5.27e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 98.75%, val/strict_accuracy: 74.14%\n",
      "Epoch 474: lr: 5.26e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.19%, val/accuracy: 98.75%, val/strict_accuracy: 74.30%\n",
      "Epoch 475: lr: 5.25e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 98.75%, val/strict_accuracy: 73.91%\n",
      "Epoch 476: lr: 5.24e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 98.75%, val/strict_accuracy: 74.30%\n",
      "Epoch 477: lr: 5.23e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0029, val/iit_loss: 0.0011, val/IIA: 86.87%, val/accuracy: 98.75%, val/strict_accuracy: 74.34%\n",
      "Epoch 478: lr: 5.22e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0011, val/IIA: 86.87%, val/accuracy: 98.75%, val/strict_accuracy: 74.26%\n",
      "Epoch 479: lr: 5.21e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0029, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 97.50%, val/strict_accuracy: 74.14%\n",
      "Epoch 480: lr: 5.20e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.81%, val/accuracy: 98.75%, val/strict_accuracy: 74.61%\n",
      "Epoch 481: lr: 5.19e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 98.75%, val/strict_accuracy: 74.69%\n",
      "Epoch 482: lr: 5.18e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0029, val/iit_loss: 0.0006, val/IIA: 92.81%, val/accuracy: 97.50%, val/strict_accuracy: 74.18%\n",
      "Epoch 483: lr: 5.17e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 98.75%, val/strict_accuracy: 74.34%\n",
      "Epoch 484: lr: 5.16e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 98.75%, val/strict_accuracy: 74.41%\n",
      "Epoch 485: lr: 5.15e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 85.31%, val/accuracy: 97.50%, val/strict_accuracy: 74.06%\n",
      "Epoch 486: lr: 5.14e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 85.62%, val/accuracy: 96.88%, val/strict_accuracy: 73.63%\n",
      "Epoch 487: lr: 5.13e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0006, val/IIA: 91.56%, val/accuracy: 96.88%, val/strict_accuracy: 73.63%\n",
      "Epoch 488: lr: 5.12e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 91.25%, val/accuracy: 96.88%, val/strict_accuracy: 73.63%\n",
      "Epoch 489: lr: 5.11e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 91.25%, val/accuracy: 96.88%, val/strict_accuracy: 73.52%\n",
      "Epoch 490: lr: 5.10e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 97.50%, val/strict_accuracy: 73.79%\n",
      "Epoch 491: lr: 5.09e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0000, val/iit_loss: 0.0006, val/IIA: 92.50%, val/accuracy: 98.75%, val/strict_accuracy: 73.95%\n",
      "Epoch 492: lr: 5.08e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.19%, val/accuracy: 98.75%, val/strict_accuracy: 74.45%\n",
      "Epoch 493: lr: 5.07e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.19%, val/accuracy: 98.75%, val/strict_accuracy: 74.92%\n",
      "Epoch 494: lr: 5.06e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0007, val/IIA: 92.81%, val/accuracy: 98.75%, val/strict_accuracy: 74.96%\n",
      "Epoch 495: lr: 5.05e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 88.44%, val/accuracy: 98.75%, val/strict_accuracy: 74.73%\n",
      "Epoch 496: lr: 5.04e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.81%, val/accuracy: 98.75%, val/strict_accuracy: 74.38%\n",
      "Epoch 497: lr: 5.03e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.81%, val/accuracy: 98.75%, val/strict_accuracy: 74.34%\n",
      "Epoch 498: lr: 5.02e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.19%, val/accuracy: 98.75%, val/strict_accuracy: 73.67%\n",
      "Epoch 499: lr: 5.01e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.50%, val/accuracy: 97.50%, val/strict_accuracy: 73.52%\n",
      "Epoch 500: lr: 5.00e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.19%, val/accuracy: 97.81%, val/strict_accuracy: 73.20%\n",
      "Epoch 501: lr: 4.99e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.19%, val/accuracy: 97.50%, val/strict_accuracy: 73.52%\n",
      "Epoch 502: lr: 4.98e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.62%, val/accuracy: 97.50%, val/strict_accuracy: 73.87%\n",
      "Epoch 503: lr: 4.97e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 85.62%, val/accuracy: 98.75%, val/strict_accuracy: 74.10%\n",
      "Epoch 504: lr: 4.96e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 98.75%, val/strict_accuracy: 73.83%\n",
      "Epoch 505: lr: 4.95e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.50%, val/accuracy: 97.19%, val/strict_accuracy: 73.91%\n",
      "Epoch 506: lr: 4.94e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.87%, val/accuracy: 98.75%, val/strict_accuracy: 74.06%\n",
      "Epoch 507: lr: 4.93e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 86.87%, val/accuracy: 98.75%, val/strict_accuracy: 73.79%\n",
      "Epoch 508: lr: 4.92e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0026, val/iit_loss: 0.0007, val/IIA: 92.19%, val/accuracy: 98.75%, val/strict_accuracy: 74.26%\n",
      "Epoch 509: lr: 4.91e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0026, val/iit_loss: 0.0012, val/IIA: 85.94%, val/accuracy: 98.75%, val/strict_accuracy: 73.95%\n",
      "Epoch 510: lr: 4.90e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 92.50%, val/accuracy: 98.75%, val/strict_accuracy: 73.95%\n",
      "Epoch 511: lr: 4.89e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0007, val/IIA: 93.44%, val/accuracy: 98.75%, val/strict_accuracy: 74.06%\n",
      "Epoch 512: lr: 4.88e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.00%, val/accuracy: 98.75%, val/strict_accuracy: 74.26%\n",
      "Epoch 513: lr: 4.87e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 84.06%, val/accuracy: 97.81%, val/strict_accuracy: 73.55%\n",
      "Epoch 514: lr: 4.86e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.81%, val/accuracy: 98.75%, val/strict_accuracy: 73.91%\n",
      "Epoch 515: lr: 4.85e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 92.81%, val/accuracy: 98.75%, val/strict_accuracy: 73.52%\n",
      "Epoch 516: lr: 4.84e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 83.75%, val/accuracy: 97.81%, val/strict_accuracy: 74.02%\n",
      "Epoch 517: lr: 4.83e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 93.44%, val/accuracy: 98.75%, val/strict_accuracy: 74.30%\n",
      "Epoch 518: lr: 4.82e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0024, val/iit_loss: 0.0014, val/IIA: 81.88%, val/accuracy: 98.75%, val/strict_accuracy: 74.34%\n",
      "Epoch 519: lr: 4.81e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 93.44%, val/accuracy: 99.06%, val/strict_accuracy: 73.91%\n",
      "Epoch 520: lr: 4.80e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.06%, val/accuracy: 98.75%, val/strict_accuracy: 74.77%\n",
      "Epoch 521: lr: 4.79e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 82.50%, val/accuracy: 98.75%, val/strict_accuracy: 74.30%\n",
      "Epoch 522: lr: 4.78e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 82.19%, val/accuracy: 98.75%, val/strict_accuracy: 74.92%\n",
      "Epoch 523: lr: 4.77e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 74.88%\n",
      "Epoch 524: lr: 4.76e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0024, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.31%\n",
      "Epoch 525: lr: 4.75e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0024, val/iit_loss: 0.0014, val/IIA: 84.06%, val/accuracy: 98.75%, val/strict_accuracy: 75.12%\n",
      "Epoch 526: lr: 4.74e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.20%\n",
      "Epoch 527: lr: 4.73e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.38%, val/accuracy: 98.75%, val/strict_accuracy: 75.16%\n",
      "Epoch 528: lr: 4.72e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.16%\n",
      "Epoch 529: lr: 4.71e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 81.25%, val/accuracy: 98.75%, val/strict_accuracy: 74.96%\n",
      "Epoch 530: lr: 4.70e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 80.94%, val/accuracy: 98.75%, val/strict_accuracy: 75.12%\n",
      "Epoch 531: lr: 4.69e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 81.88%, val/accuracy: 98.75%, val/strict_accuracy: 75.12%\n",
      "Epoch 532: lr: 4.68e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.04%\n",
      "Epoch 533: lr: 4.67e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.08%\n",
      "Epoch 534: lr: 4.66e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0022, val/iit_loss: 0.0015, val/IIA: 81.88%, val/accuracy: 98.75%, val/strict_accuracy: 74.96%\n",
      "Epoch 535: lr: 4.65e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.16%\n",
      "Epoch 536: lr: 4.64e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 84.06%, val/accuracy: 98.75%, val/strict_accuracy: 75.16%\n",
      "Epoch 537: lr: 4.63e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0006, val/IIA: 94.38%, val/accuracy: 98.75%, val/strict_accuracy: 74.88%\n",
      "Epoch 538: lr: 4.62e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.00%, val/accuracy: 98.75%, val/strict_accuracy: 75.16%\n",
      "Epoch 539: lr: 4.61e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.38%, val/accuracy: 97.50%, val/strict_accuracy: 74.73%\n",
      "Epoch 540: lr: 4.60e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.06%, val/accuracy: 97.50%, val/strict_accuracy: 74.84%\n",
      "Epoch 541: lr: 4.59e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.31%, val/accuracy: 98.75%, val/strict_accuracy: 75.27%\n",
      "Epoch 542: lr: 4.58e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 82.81%, val/accuracy: 96.88%, val/strict_accuracy: 74.30%\n",
      "Epoch 543: lr: 4.57e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.06%, val/accuracy: 98.75%, val/strict_accuracy: 75.23%\n",
      "Epoch 544: lr: 4.56e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0021, val/iit_loss: 0.0006, val/IIA: 95.00%, val/accuracy: 97.50%, val/strict_accuracy: 74.53%\n",
      "Epoch 545: lr: 4.55e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 93.75%, val/accuracy: 97.19%, val/strict_accuracy: 74.38%\n",
      "Epoch 546: lr: 4.54e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.63%, val/accuracy: 98.75%, val/strict_accuracy: 75.27%\n",
      "Epoch 547: lr: 4.53e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0021, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.51%\n",
      "Epoch 548: lr: 4.52e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.06%, val/accuracy: 97.19%, val/strict_accuracy: 75.23%\n",
      "Epoch 549: lr: 4.51e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.06%, val/accuracy: 99.06%, val/strict_accuracy: 76.41%\n",
      "Epoch 550: lr: 4.50e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0001, train/strict_loss: 0.0021, val/iit_loss: 0.0014, val/IIA: 81.56%, val/accuracy: 98.44%, val/strict_accuracy: 75.66%\n",
      "Epoch 551: lr: 4.49e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 82.81%, val/accuracy: 98.44%, val/strict_accuracy: 76.05%\n",
      "Epoch 552: lr: 4.48e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 93.12%, val/accuracy: 98.44%, val/strict_accuracy: 76.45%\n",
      "Epoch 553: lr: 4.47e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0020, val/iit_loss: 0.0008, val/IIA: 91.56%, val/accuracy: 98.75%, val/strict_accuracy: 75.51%\n",
      "Epoch 554: lr: 4.46e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 84.06%, val/accuracy: 98.75%, val/strict_accuracy: 76.05%\n",
      "Epoch 555: lr: 4.45e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0020, val/iit_loss: 0.0006, val/IIA: 94.38%, val/accuracy: 98.75%, val/strict_accuracy: 75.98%\n",
      "Epoch 556: lr: 4.44e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 94.38%, val/accuracy: 97.50%, val/strict_accuracy: 74.45%\n",
      "Epoch 557: lr: 4.43e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.31%, val/accuracy: 99.06%, val/strict_accuracy: 76.72%\n",
      "Epoch 558: lr: 4.42e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.44%, val/strict_accuracy: 75.47%\n",
      "Epoch 559: lr: 4.41e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.38%, val/accuracy: 97.50%, val/strict_accuracy: 75.04%\n",
      "Epoch 560: lr: 4.40e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0019, val/iit_loss: 0.0018, val/IIA: 80.94%, val/accuracy: 98.75%, val/strict_accuracy: 76.25%\n",
      "Epoch 561: lr: 4.39e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.38%, val/accuracy: 97.50%, val/strict_accuracy: 75.16%\n",
      "Epoch 562: lr: 4.38e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0006, val/IIA: 94.38%, val/accuracy: 97.50%, val/strict_accuracy: 75.20%\n",
      "Epoch 563: lr: 4.37e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 78.75%, val/accuracy: 99.06%, val/strict_accuracy: 76.80%\n",
      "Epoch 564: lr: 4.36e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 78.75%, val/accuracy: 97.50%, val/strict_accuracy: 75.20%\n",
      "Epoch 565: lr: 4.35e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0018, val/iit_loss: 0.0020, val/IIA: 79.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.78%\n",
      "Epoch 566: lr: 4.34e-04, train/iit_loss: 0.0009, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.06%, val/accuracy: 99.06%, val/strict_accuracy: 76.80%\n",
      "Epoch 567: lr: 4.33e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0020, val/IIA: 79.37%, val/accuracy: 97.19%, val/strict_accuracy: 75.78%\n",
      "Epoch 568: lr: 4.32e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 79.69%, val/accuracy: 99.06%, val/strict_accuracy: 76.76%\n",
      "Epoch 569: lr: 4.31e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0020, val/IIA: 80.31%, val/accuracy: 98.75%, val/strict_accuracy: 75.94%\n",
      "Epoch 570: lr: 4.30e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0021, val/IIA: 79.37%, val/accuracy: 97.50%, val/strict_accuracy: 75.78%\n",
      "Epoch 571: lr: 4.29e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 95.00%, val/accuracy: 99.06%, val/strict_accuracy: 76.25%\n",
      "Epoch 572: lr: 4.28e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.78%\n",
      "Epoch 573: lr: 4.27e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.38%, val/accuracy: 97.19%, val/strict_accuracy: 75.98%\n",
      "Epoch 574: lr: 4.26e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.38%, val/accuracy: 99.06%, val/strict_accuracy: 76.13%\n",
      "Epoch 575: lr: 4.25e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 78.75%, val/accuracy: 97.19%, val/strict_accuracy: 75.86%\n",
      "Epoch 576: lr: 4.24e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 98.75%, val/strict_accuracy: 75.70%\n",
      "Epoch 577: lr: 4.23e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0016, val/iit_loss: 0.0019, val/IIA: 81.56%, val/accuracy: 98.75%, val/strict_accuracy: 75.27%\n",
      "Epoch 578: lr: 4.22e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 79.06%, val/accuracy: 97.19%, val/strict_accuracy: 76.09%\n",
      "Epoch 579: lr: 4.21e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 80.31%, val/accuracy: 99.06%, val/strict_accuracy: 76.21%\n",
      "Epoch 580: lr: 4.20e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 80.00%, val/accuracy: 97.19%, val/strict_accuracy: 75.78%\n",
      "Epoch 581: lr: 4.19e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0006, val/IIA: 94.69%, val/accuracy: 97.50%, val/strict_accuracy: 75.20%\n",
      "Epoch 582: lr: 4.18e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0006, val/IIA: 95.00%, val/accuracy: 97.50%, val/strict_accuracy: 75.43%\n",
      "Epoch 583: lr: 4.17e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 79.37%, val/accuracy: 97.19%, val/strict_accuracy: 75.12%\n",
      "Epoch 584: lr: 4.16e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0007, val/IIA: 94.69%, val/accuracy: 97.50%, val/strict_accuracy: 75.20%\n",
      "Epoch 585: lr: 4.15e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0015, val/iit_loss: 0.0007, val/IIA: 94.38%, val/accuracy: 97.19%, val/strict_accuracy: 75.27%\n",
      "Epoch 586: lr: 4.14e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0015, val/iit_loss: 0.0019, val/IIA: 81.88%, val/accuracy: 97.19%, val/strict_accuracy: 75.78%\n",
      "Epoch 587: lr: 4.13e-04, train/iit_loss: 0.0003, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 77.81%, val/accuracy: 98.75%, val/strict_accuracy: 75.86%\n",
      "Epoch 588: lr: 4.12e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0015, val/iit_loss: 0.0020, val/IIA: 80.00%, val/accuracy: 95.94%, val/strict_accuracy: 74.77%\n",
      "Epoch 589: lr: 4.11e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0019, val/IIA: 78.12%, val/accuracy: 98.44%, val/strict_accuracy: 76.60%\n",
      "Epoch 590: lr: 4.10e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0015, val/iit_loss: 0.0019, val/IIA: 77.81%, val/accuracy: 97.81%, val/strict_accuracy: 76.37%\n",
      "Epoch 591: lr: 4.09e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0019, val/IIA: 82.19%, val/accuracy: 97.81%, val/strict_accuracy: 75.90%\n",
      "Epoch 592: lr: 4.08e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0015, val/iit_loss: 0.0020, val/IIA: 78.12%, val/accuracy: 97.81%, val/strict_accuracy: 76.02%\n",
      "Epoch 593: lr: 4.07e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 79.37%, val/accuracy: 97.81%, val/strict_accuracy: 76.37%\n",
      "Epoch 594: lr: 4.06e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0020, val/IIA: 78.75%, val/accuracy: 97.81%, val/strict_accuracy: 76.05%\n",
      "Epoch 595: lr: 4.05e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 91.56%, val/accuracy: 97.81%, val/strict_accuracy: 75.86%\n",
      "Epoch 596: lr: 4.04e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 91.56%, val/accuracy: 97.81%, val/strict_accuracy: 76.17%\n",
      "Epoch 597: lr: 4.03e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 95.00%, val/strict_accuracy: 73.05%\n",
      "Epoch 598: lr: 4.02e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 88.13%, val/accuracy: 97.81%, val/strict_accuracy: 75.82%\n",
      "Epoch 599: lr: 4.01e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 85.94%, val/accuracy: 95.63%, val/strict_accuracy: 74.61%\n",
      "Epoch 600: lr: 4.00e-04, train/iit_loss: 0.0010, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0021, val/IIA: 79.37%, val/accuracy: 97.19%, val/strict_accuracy: 75.27%\n",
      "Epoch 601: lr: 3.99e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0013, val/iit_loss: 0.0024, val/IIA: 75.63%, val/accuracy: 98.12%, val/strict_accuracy: 75.94%\n",
      "Epoch 602: lr: 3.98e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 77.19%, val/accuracy: 92.19%, val/strict_accuracy: 69.30%\n",
      "Epoch 603: lr: 3.97e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 79.06%, val/accuracy: 96.56%, val/strict_accuracy: 74.84%\n",
      "Epoch 604: lr: 3.96e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0024, val/IIA: 74.69%, val/accuracy: 95.63%, val/strict_accuracy: 74.69%\n",
      "Epoch 605: lr: 3.95e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 95.63%, val/strict_accuracy: 74.96%\n",
      "Epoch 606: lr: 3.94e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 79.69%, val/accuracy: 98.44%, val/strict_accuracy: 75.35%\n",
      "Epoch 607: lr: 3.93e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0033, val/IIA: 73.12%, val/accuracy: 92.19%, val/strict_accuracy: 70.74%\n",
      "Epoch 608: lr: 3.92e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 75.63%, val/accuracy: 97.81%, val/strict_accuracy: 76.02%\n",
      "Epoch 609: lr: 3.91e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0012, val/iit_loss: 0.0034, val/IIA: 70.00%, val/accuracy: 97.81%, val/strict_accuracy: 75.74%\n",
      "Epoch 610: lr: 3.90e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 80.31%, val/accuracy: 92.81%, val/strict_accuracy: 70.62%\n",
      "Epoch 611: lr: 3.89e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0031, val/IIA: 75.31%, val/accuracy: 97.50%, val/strict_accuracy: 76.09%\n",
      "Epoch 612: lr: 3.88e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 79.06%, val/accuracy: 97.81%, val/strict_accuracy: 76.29%\n",
      "Epoch 613: lr: 3.87e-04, train/iit_loss: 0.0018, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0034, val/IIA: 74.37%, val/accuracy: 92.19%, val/strict_accuracy: 70.90%\n",
      "Epoch 614: lr: 3.86e-04, train/iit_loss: 0.0008, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 80.31%, val/accuracy: 97.81%, val/strict_accuracy: 76.09%\n",
      "Epoch 615: lr: 3.85e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0031, val/IIA: 74.69%, val/accuracy: 96.25%, val/strict_accuracy: 75.74%\n",
      "Epoch 616: lr: 3.84e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0032, val/IIA: 73.44%, val/accuracy: 94.69%, val/strict_accuracy: 73.55%\n",
      "Epoch 617: lr: 3.83e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0031, val/IIA: 74.69%, val/accuracy: 96.25%, val/strict_accuracy: 75.39%\n",
      "Epoch 618: lr: 3.82e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0031, val/IIA: 72.50%, val/accuracy: 96.25%, val/strict_accuracy: 76.13%\n",
      "Epoch 619: lr: 3.81e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0032, val/IIA: 72.19%, val/accuracy: 93.44%, val/strict_accuracy: 72.93%\n",
      "Epoch 620: lr: 3.80e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0033, val/IIA: 70.94%, val/accuracy: 98.44%, val/strict_accuracy: 76.41%\n",
      "Epoch 621: lr: 3.79e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.81%, val/accuracy: 96.25%, val/strict_accuracy: 75.12%\n",
      "Epoch 622: lr: 3.78e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0012, val/iit_loss: 0.0032, val/IIA: 72.19%, val/accuracy: 93.12%, val/strict_accuracy: 72.23%\n",
      "Epoch 623: lr: 3.77e-04, train/iit_loss: 0.0017, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0031, val/IIA: 72.19%, val/accuracy: 98.44%, val/strict_accuracy: 76.29%\n",
      "Epoch 624: lr: 3.76e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0011, val/IIA: 87.50%, val/accuracy: 96.25%, val/strict_accuracy: 75.51%\n",
      "Epoch 625: lr: 3.75e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 81.25%, val/accuracy: 93.44%, val/strict_accuracy: 72.23%\n",
      "Epoch 626: lr: 3.74e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0033, val/IIA: 72.19%, val/accuracy: 98.75%, val/strict_accuracy: 76.05%\n",
      "Epoch 627: lr: 3.73e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 83.13%, val/accuracy: 96.25%, val/strict_accuracy: 74.53%\n",
      "Epoch 628: lr: 3.72e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0031, val/IIA: 71.25%, val/accuracy: 93.12%, val/strict_accuracy: 70.12%\n",
      "Epoch 629: lr: 3.71e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0030, val/IIA: 75.00%, val/accuracy: 98.44%, val/strict_accuracy: 76.09%\n",
      "Epoch 630: lr: 3.70e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0012, val/iit_loss: 0.0030, val/IIA: 75.63%, val/accuracy: 98.44%, val/strict_accuracy: 75.90%\n",
      "Epoch 631: lr: 3.69e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 71.25%, val/accuracy: 92.50%, val/strict_accuracy: 68.91%\n",
      "Epoch 632: lr: 3.68e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0027, val/IIA: 74.37%, val/accuracy: 96.56%, val/strict_accuracy: 74.77%\n",
      "Epoch 633: lr: 3.67e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 83.13%, val/accuracy: 95.94%, val/strict_accuracy: 75.66%\n",
      "Epoch 634: lr: 3.66e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0026, val/IIA: 71.25%, val/accuracy: 95.31%, val/strict_accuracy: 72.19%\n",
      "Epoch 635: lr: 3.65e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 71.25%, val/accuracy: 95.31%, val/strict_accuracy: 71.72%\n",
      "Epoch 636: lr: 3.64e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.62%, val/accuracy: 97.81%, val/strict_accuracy: 76.33%\n",
      "Epoch 637: lr: 3.63e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 83.44%, val/accuracy: 96.56%, val/strict_accuracy: 75.35%\n",
      "Epoch 638: lr: 3.62e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0012, val/iit_loss: 0.0014, val/IIA: 81.25%, val/accuracy: 94.06%, val/strict_accuracy: 70.43%\n",
      "Epoch 639: lr: 3.61e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.38%, val/accuracy: 95.94%, val/strict_accuracy: 75.08%\n",
      "Epoch 640: lr: 3.60e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 78.44%, val/accuracy: 97.81%, val/strict_accuracy: 76.13%\n",
      "Epoch 641: lr: 3.59e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 80.94%, val/accuracy: 95.94%, val/strict_accuracy: 74.06%\n",
      "Epoch 642: lr: 3.58e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0011, val/iit_loss: 0.0024, val/IIA: 75.31%, val/accuracy: 95.94%, val/strict_accuracy: 72.50%\n",
      "Epoch 643: lr: 3.57e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.31%, val/accuracy: 97.81%, val/strict_accuracy: 75.94%\n",
      "Epoch 644: lr: 3.56e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 80.00%, val/accuracy: 97.19%, val/strict_accuracy: 76.13%\n",
      "Epoch 645: lr: 3.55e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 81.88%, val/accuracy: 95.94%, val/strict_accuracy: 72.30%\n",
      "Epoch 646: lr: 3.54e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 77.19%, val/accuracy: 95.94%, val/strict_accuracy: 73.75%\n",
      "Epoch 647: lr: 3.53e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0011, val/iit_loss: 0.0028, val/IIA: 80.31%, val/accuracy: 96.56%, val/strict_accuracy: 75.66%\n",
      "Epoch 648: lr: 3.52e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0011, val/IIA: 84.69%, val/accuracy: 97.19%, val/strict_accuracy: 74.73%\n",
      "Epoch 649: lr: 3.51e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 73.12%, val/accuracy: 95.31%, val/strict_accuracy: 69.30%\n",
      "Epoch 650: lr: 3.50e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 78.75%, val/accuracy: 97.81%, val/strict_accuracy: 76.41%\n",
      "Epoch 651: lr: 3.49e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 98.44%, val/strict_accuracy: 76.02%\n",
      "Epoch 652: lr: 3.48e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 74.06%, val/accuracy: 95.94%, val/strict_accuracy: 72.11%\n",
      "Epoch 653: lr: 3.47e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.62%, val/accuracy: 97.81%, val/strict_accuracy: 75.66%\n",
      "Epoch 654: lr: 3.46e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 79.06%, val/accuracy: 98.44%, val/strict_accuracy: 76.37%\n",
      "Epoch 655: lr: 3.45e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 83.75%, val/accuracy: 95.94%, val/strict_accuracy: 73.83%\n",
      "Epoch 656: lr: 3.44e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 84.38%, val/accuracy: 95.94%, val/strict_accuracy: 71.88%\n",
      "Epoch 657: lr: 3.43e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 83.75%, val/accuracy: 98.12%, val/strict_accuracy: 76.29%\n",
      "Epoch 658: lr: 3.42e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.56%, val/accuracy: 98.44%, val/strict_accuracy: 75.66%\n",
      "Epoch 659: lr: 3.41e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0016, val/IIA: 81.88%, val/accuracy: 95.00%, val/strict_accuracy: 68.75%\n",
      "Epoch 660: lr: 3.40e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 77.81%, val/accuracy: 98.44%, val/strict_accuracy: 76.17%\n",
      "Epoch 661: lr: 3.39e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 98.44%, val/strict_accuracy: 76.37%\n",
      "Epoch 662: lr: 3.38e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 74.06%, val/accuracy: 95.63%, val/strict_accuracy: 74.10%\n",
      "Epoch 663: lr: 3.37e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 83.75%, val/accuracy: 95.00%, val/strict_accuracy: 71.21%\n",
      "Epoch 664: lr: 3.36e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 79.06%, val/accuracy: 98.44%, val/strict_accuracy: 76.29%\n",
      "Epoch 665: lr: 3.35e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.19%, val/accuracy: 98.44%, val/strict_accuracy: 76.25%\n",
      "Epoch 666: lr: 3.34e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 83.75%, val/accuracy: 95.31%, val/strict_accuracy: 73.36%\n",
      "Epoch 667: lr: 3.33e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 77.19%, val/accuracy: 96.88%, val/strict_accuracy: 76.21%\n",
      "Epoch 668: lr: 3.32e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 78.75%, val/accuracy: 98.44%, val/strict_accuracy: 76.33%\n",
      "Epoch 669: lr: 3.31e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 83.44%, val/accuracy: 95.31%, val/strict_accuracy: 73.44%\n",
      "Epoch 670: lr: 3.30e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 77.19%, val/accuracy: 96.88%, val/strict_accuracy: 76.17%\n",
      "Epoch 671: lr: 3.29e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 78.44%, val/accuracy: 96.88%, val/strict_accuracy: 76.33%\n",
      "Epoch 672: lr: 3.28e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 96.88%, val/strict_accuracy: 76.17%\n",
      "Epoch 673: lr: 3.27e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 96.88%, val/strict_accuracy: 76.33%\n",
      "Epoch 674: lr: 3.26e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 89.06%, val/accuracy: 96.88%, val/strict_accuracy: 76.13%\n",
      "Epoch 675: lr: 3.25e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 89.38%, val/accuracy: 96.88%, val/strict_accuracy: 76.17%\n",
      "Epoch 676: lr: 3.24e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.38%, val/accuracy: 95.31%, val/strict_accuracy: 73.40%\n",
      "Epoch 677: lr: 3.23e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.31%, val/accuracy: 96.88%, val/strict_accuracy: 75.78%\n",
      "Epoch 678: lr: 3.22e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0010, val/iit_loss: 0.0010, val/IIA: 89.38%, val/accuracy: 96.88%, val/strict_accuracy: 76.25%\n",
      "Epoch 679: lr: 3.21e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 84.69%, val/accuracy: 96.56%, val/strict_accuracy: 75.31%\n",
      "Epoch 680: lr: 3.20e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 73.75%, val/accuracy: 95.31%, val/strict_accuracy: 74.49%\n",
      "Epoch 681: lr: 3.19e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.50%, val/accuracy: 96.88%, val/strict_accuracy: 75.86%\n",
      "Epoch 682: lr: 3.18e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.19%, val/accuracy: 96.88%, val/strict_accuracy: 75.82%\n",
      "Epoch 683: lr: 3.17e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 75.00%, val/accuracy: 95.31%, val/strict_accuracy: 74.73%\n",
      "Epoch 684: lr: 3.16e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.81%, val/accuracy: 96.88%, val/strict_accuracy: 75.90%\n",
      "Epoch 685: lr: 3.15e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.88%, val/accuracy: 96.56%, val/strict_accuracy: 75.55%\n",
      "Epoch 686: lr: 3.14e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.56%, val/accuracy: 96.56%, val/strict_accuracy: 75.43%\n",
      "Epoch 687: lr: 3.13e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.62%, val/accuracy: 96.25%, val/strict_accuracy: 75.59%\n",
      "Epoch 688: lr: 3.12e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 77.50%, val/accuracy: 96.56%, val/strict_accuracy: 75.39%\n",
      "Epoch 689: lr: 3.11e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0010, val/iit_loss: 0.0027, val/IIA: 74.37%, val/accuracy: 96.56%, val/strict_accuracy: 74.10%\n",
      "Epoch 690: lr: 3.10e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.69%, val/accuracy: 95.94%, val/strict_accuracy: 75.78%\n",
      "Epoch 691: lr: 3.09e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 95.94%, val/strict_accuracy: 75.35%\n",
      "Epoch 692: lr: 3.08e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 77.81%, val/accuracy: 95.94%, val/strict_accuracy: 75.31%\n",
      "Epoch 693: lr: 3.07e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 84.06%, val/accuracy: 95.94%, val/strict_accuracy: 75.27%\n",
      "Epoch 694: lr: 3.06e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.06%, val/accuracy: 95.94%, val/strict_accuracy: 75.23%\n",
      "Epoch 695: lr: 3.05e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 77.50%, val/accuracy: 95.94%, val/strict_accuracy: 75.00%\n",
      "Epoch 696: lr: 3.04e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.06%, val/accuracy: 95.94%, val/strict_accuracy: 75.23%\n",
      "Epoch 697: lr: 3.03e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 76.56%, val/accuracy: 96.56%, val/strict_accuracy: 74.37%\n",
      "Epoch 698: lr: 3.02e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 76.56%, val/accuracy: 96.56%, val/strict_accuracy: 74.88%\n",
      "Epoch 699: lr: 3.01e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 75.94%, val/accuracy: 95.63%, val/strict_accuracy: 75.27%\n",
      "Epoch 700: lr: 3.00e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 96.56%, val/strict_accuracy: 74.84%\n",
      "Epoch 701: lr: 2.99e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0029, val/IIA: 72.19%, val/accuracy: 95.31%, val/strict_accuracy: 71.64%\n",
      "Epoch 702: lr: 2.98e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 84.69%, val/accuracy: 95.00%, val/strict_accuracy: 75.39%\n",
      "Epoch 703: lr: 2.97e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0028, val/IIA: 76.88%, val/accuracy: 95.94%, val/strict_accuracy: 74.92%\n",
      "Epoch 704: lr: 2.96e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0029, val/IIA: 74.37%, val/accuracy: 96.25%, val/strict_accuracy: 72.46%\n",
      "Epoch 705: lr: 2.95e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.62%, val/accuracy: 96.56%, val/strict_accuracy: 74.45%\n",
      "Epoch 706: lr: 2.94e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 95.31%, val/strict_accuracy: 75.78%\n",
      "Epoch 707: lr: 2.93e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 95.94%, val/strict_accuracy: 74.88%\n",
      "Epoch 708: lr: 2.92e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.06%, val/accuracy: 96.56%, val/strict_accuracy: 73.83%\n",
      "Epoch 709: lr: 2.91e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 95.31%, val/strict_accuracy: 75.12%\n",
      "Epoch 710: lr: 2.90e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0010, val/iit_loss: 0.0028, val/IIA: 76.88%, val/accuracy: 95.31%, val/strict_accuracy: 75.00%\n",
      "Epoch 711: lr: 2.89e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0010, val/iit_loss: 0.0012, val/IIA: 85.00%, val/accuracy: 96.56%, val/strict_accuracy: 73.95%\n",
      "Epoch 712: lr: 2.88e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 84.06%, val/accuracy: 96.56%, val/strict_accuracy: 73.79%\n",
      "Epoch 713: lr: 2.87e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0029, val/IIA: 76.25%, val/accuracy: 95.94%, val/strict_accuracy: 75.55%\n",
      "Epoch 714: lr: 2.86e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 88.75%, val/accuracy: 96.56%, val/strict_accuracy: 75.27%\n",
      "Epoch 715: lr: 2.85e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0027, val/IIA: 74.37%, val/accuracy: 96.56%, val/strict_accuracy: 72.77%\n",
      "Epoch 716: lr: 2.84e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 96.56%, val/strict_accuracy: 74.65%\n",
      "Epoch 717: lr: 2.83e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0027, val/IIA: 76.88%, val/accuracy: 96.25%, val/strict_accuracy: 75.70%\n",
      "Epoch 718: lr: 2.82e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 75.94%, val/accuracy: 96.56%, val/strict_accuracy: 73.91%\n",
      "Epoch 719: lr: 2.81e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 74.37%, val/accuracy: 96.56%, val/strict_accuracy: 72.54%\n",
      "Epoch 720: lr: 2.80e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0028, val/IIA: 76.56%, val/accuracy: 95.94%, val/strict_accuracy: 75.98%\n",
      "Epoch 721: lr: 2.79e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 88.44%, val/accuracy: 96.25%, val/strict_accuracy: 75.47%\n",
      "Epoch 722: lr: 2.78e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 85.31%, val/accuracy: 96.25%, val/strict_accuracy: 71.41%\n",
      "Epoch 723: lr: 2.77e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0026, val/IIA: 77.19%, val/accuracy: 95.63%, val/strict_accuracy: 75.78%\n",
      "Epoch 724: lr: 2.76e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 88.75%, val/accuracy: 95.94%, val/strict_accuracy: 76.02%\n",
      "Epoch 725: lr: 2.75e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.31%, val/accuracy: 96.25%, val/strict_accuracy: 72.19%\n",
      "Epoch 726: lr: 2.74e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0009, val/iit_loss: 0.0012, val/IIA: 85.31%, val/accuracy: 96.25%, val/strict_accuracy: 71.84%\n",
      "Epoch 727: lr: 2.73e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0009, val/iit_loss: 0.0027, val/IIA: 76.88%, val/accuracy: 96.88%, val/strict_accuracy: 75.86%\n",
      "Epoch 728: lr: 2.72e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 77.81%, val/accuracy: 96.88%, val/strict_accuracy: 75.78%\n",
      "Epoch 729: lr: 2.71e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.00%, val/accuracy: 96.25%, val/strict_accuracy: 70.55%\n",
      "Epoch 730: lr: 2.70e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 87.19%, val/accuracy: 96.88%, val/strict_accuracy: 74.10%\n",
      "Epoch 731: lr: 2.69e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 77.81%, val/accuracy: 95.63%, val/strict_accuracy: 75.51%\n",
      "Epoch 732: lr: 2.68e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 76.56%, val/accuracy: 96.88%, val/strict_accuracy: 75.90%\n",
      "Epoch 733: lr: 2.67e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 85.31%, val/accuracy: 96.25%, val/strict_accuracy: 71.09%\n",
      "Epoch 734: lr: 2.66e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0009, val/IIA: 90.62%, val/accuracy: 98.75%, val/strict_accuracy: 76.72%\n",
      "Epoch 735: lr: 2.65e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 78.12%, val/accuracy: 98.75%, val/strict_accuracy: 76.87%\n",
      "Epoch 736: lr: 2.64e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0009, val/iit_loss: 0.0009, val/IIA: 87.81%, val/accuracy: 96.88%, val/strict_accuracy: 74.18%\n",
      "Epoch 737: lr: 2.63e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0023, val/IIA: 76.56%, val/accuracy: 96.88%, val/strict_accuracy: 74.69%\n",
      "Epoch 738: lr: 2.62e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0009, val/iit_loss: 0.0008, val/IIA: 90.62%, val/accuracy: 98.75%, val/strict_accuracy: 77.07%\n",
      "Epoch 739: lr: 2.61e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 77.81%, val/accuracy: 98.44%, val/strict_accuracy: 76.33%\n",
      "Epoch 740: lr: 2.60e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 88.44%, val/accuracy: 96.88%, val/strict_accuracy: 73.87%\n",
      "Epoch 741: lr: 2.59e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0023, val/IIA: 77.19%, val/accuracy: 98.44%, val/strict_accuracy: 76.29%\n",
      "Epoch 742: lr: 2.58e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 77.81%, val/accuracy: 98.75%, val/strict_accuracy: 76.99%\n",
      "Epoch 743: lr: 2.57e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 91.56%, val/accuracy: 98.12%, val/strict_accuracy: 75.98%\n",
      "Epoch 744: lr: 2.56e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 90.94%, val/accuracy: 98.12%, val/strict_accuracy: 75.43%\n",
      "Epoch 745: lr: 2.55e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 77.81%, val/accuracy: 98.44%, val/strict_accuracy: 76.48%\n",
      "Epoch 746: lr: 2.54e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 90.94%, val/accuracy: 98.44%, val/strict_accuracy: 76.99%\n",
      "Epoch 747: lr: 2.53e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0024, val/IIA: 77.19%, val/accuracy: 98.12%, val/strict_accuracy: 75.78%\n",
      "Epoch 748: lr: 2.52e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0024, val/IIA: 77.50%, val/accuracy: 98.12%, val/strict_accuracy: 76.05%\n",
      "Epoch 749: lr: 2.51e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0008, val/IIA: 91.56%, val/accuracy: 98.44%, val/strict_accuracy: 76.05%\n",
      "Epoch 750: lr: 2.50e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 77.50%, val/accuracy: 98.12%, val/strict_accuracy: 75.90%\n",
      "Epoch 751: lr: 2.49e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 78.12%, val/accuracy: 98.44%, val/strict_accuracy: 76.72%\n",
      "Epoch 752: lr: 2.48e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0003, val/iit_loss: 0.0008, val/IIA: 92.19%, val/accuracy: 98.12%, val/strict_accuracy: 76.21%\n",
      "Epoch 753: lr: 2.47e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0024, val/IIA: 75.00%, val/accuracy: 96.88%, val/strict_accuracy: 74.22%\n",
      "Epoch 754: lr: 2.46e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 90.62%, val/accuracy: 96.88%, val/strict_accuracy: 75.35%\n",
      "Epoch 755: lr: 2.45e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 78.75%, val/accuracy: 98.44%, val/strict_accuracy: 77.03%\n",
      "Epoch 756: lr: 2.44e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0023, val/IIA: 78.12%, val/accuracy: 96.88%, val/strict_accuracy: 75.59%\n",
      "Epoch 757: lr: 2.43e-04, train/iit_loss: 0.0011, train/behavior_loss: 0.0001, train/strict_loss: 0.0009, val/iit_loss: 0.0023, val/IIA: 76.88%, val/accuracy: 96.56%, val/strict_accuracy: 74.49%\n",
      "Epoch 758: lr: 2.42e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0024, val/IIA: 78.12%, val/accuracy: 96.56%, val/strict_accuracy: 76.45%\n",
      "Epoch 759: lr: 2.41e-04, train/iit_loss: 0.0004, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0024, val/IIA: 78.12%, val/accuracy: 96.56%, val/strict_accuracy: 76.25%\n",
      "Epoch 760: lr: 2.40e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0024, val/IIA: 75.94%, val/accuracy: 96.56%, val/strict_accuracy: 74.14%\n",
      "Epoch 761: lr: 2.39e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0024, val/IIA: 75.94%, val/accuracy: 96.56%, val/strict_accuracy: 73.98%\n",
      "Epoch 762: lr: 2.38e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 95.31%, val/strict_accuracy: 76.87%\n",
      "Epoch 763: lr: 2.37e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0009, val/IIA: 88.13%, val/accuracy: 96.56%, val/strict_accuracy: 76.45%\n",
      "Epoch 764: lr: 2.36e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0001, train/strict_loss: 0.0002, val/iit_loss: 0.0025, val/IIA: 75.00%, val/accuracy: 96.56%, val/strict_accuracy: 72.54%\n",
      "Epoch 765: lr: 2.35e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 85.94%, val/accuracy: 95.94%, val/strict_accuracy: 73.36%\n",
      "Epoch 766: lr: 2.34e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0010, val/IIA: 88.13%, val/accuracy: 94.69%, val/strict_accuracy: 76.41%\n",
      "Epoch 767: lr: 2.33e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0010, val/IIA: 87.81%, val/accuracy: 94.69%, val/strict_accuracy: 76.13%\n",
      "Epoch 768: lr: 2.32e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0009, val/iit_loss: 0.0011, val/IIA: 86.25%, val/accuracy: 95.94%, val/strict_accuracy: 74.49%\n",
      "Epoch 769: lr: 2.31e-04, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 75.63%, val/accuracy: 95.31%, val/strict_accuracy: 73.05%\n",
      "Epoch 770: lr: 2.30e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 77.19%, val/accuracy: 95.00%, val/strict_accuracy: 75.98%\n",
      "Epoch 771: lr: 2.29e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0026, val/IIA: 78.44%, val/accuracy: 94.69%, val/strict_accuracy: 76.13%\n",
      "Epoch 772: lr: 2.28e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0025, val/IIA: 75.63%, val/accuracy: 95.63%, val/strict_accuracy: 73.52%\n",
      "Epoch 773: lr: 2.27e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.81%, val/accuracy: 95.94%, val/strict_accuracy: 74.73%\n",
      "Epoch 774: lr: 2.26e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0026, val/IIA: 78.12%, val/accuracy: 95.00%, val/strict_accuracy: 76.29%\n",
      "Epoch 775: lr: 2.25e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0010, val/IIA: 88.44%, val/accuracy: 95.94%, val/strict_accuracy: 74.65%\n",
      "Epoch 776: lr: 2.24e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0001, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 75.63%, val/accuracy: 95.94%, val/strict_accuracy: 74.06%\n",
      "Epoch 777: lr: 2.23e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0026, val/IIA: 78.12%, val/accuracy: 95.94%, val/strict_accuracy: 74.77%\n",
      "Epoch 778: lr: 2.22e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0008, val/iit_loss: 0.0011, val/IIA: 88.75%, val/accuracy: 95.94%, val/strict_accuracy: 75.12%\n",
      "Epoch 779: lr: 2.21e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0001, train/strict_loss: 0.0008, val/iit_loss: 0.0026, val/IIA: 76.56%, val/accuracy: 95.94%, val/strict_accuracy: 74.06%\n",
      "Epoch 780: lr: 2.20e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 77.50%, val/accuracy: 95.94%, val/strict_accuracy: 74.49%\n",
      "Epoch 781: lr: 2.19e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 88.75%, val/accuracy: 94.69%, val/strict_accuracy: 75.86%\n",
      "Epoch 782: lr: 2.18e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.56%, val/accuracy: 95.94%, val/strict_accuracy: 74.22%\n",
      "Epoch 783: lr: 2.17e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0027, val/IIA: 76.88%, val/accuracy: 95.94%, val/strict_accuracy: 74.34%\n",
      "Epoch 784: lr: 2.16e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 88.44%, val/accuracy: 95.94%, val/strict_accuracy: 74.41%\n",
      "Epoch 785: lr: 2.15e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0027, val/IIA: 77.19%, val/accuracy: 94.69%, val/strict_accuracy: 75.63%\n",
      "Epoch 786: lr: 2.14e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 88.44%, val/accuracy: 95.94%, val/strict_accuracy: 74.49%\n",
      "Epoch 787: lr: 2.13e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 76.56%, val/accuracy: 95.94%, val/strict_accuracy: 74.69%\n",
      "Epoch 788: lr: 2.12e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 95.63%, val/strict_accuracy: 74.06%\n",
      "Epoch 789: lr: 2.11e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 76.25%, val/accuracy: 95.94%, val/strict_accuracy: 74.45%\n",
      "Epoch 790: lr: 2.10e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0028, val/IIA: 76.25%, val/accuracy: 94.69%, val/strict_accuracy: 76.09%\n",
      "Epoch 791: lr: 2.09e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0011, val/IIA: 88.44%, val/accuracy: 95.94%, val/strict_accuracy: 75.00%\n",
      "Epoch 792: lr: 2.08e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.31%, val/accuracy: 94.69%, val/strict_accuracy: 72.23%\n",
      "Epoch 793: lr: 2.07e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.87%, val/accuracy: 95.31%, val/strict_accuracy: 74.18%\n",
      "Epoch 794: lr: 2.06e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0011, val/IIA: 87.50%, val/accuracy: 94.69%, val/strict_accuracy: 75.74%\n",
      "Epoch 795: lr: 2.05e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 76.56%, val/accuracy: 94.38%, val/strict_accuracy: 74.69%\n",
      "Epoch 796: lr: 2.04e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0027, val/IIA: 76.56%, val/accuracy: 95.00%, val/strict_accuracy: 74.26%\n",
      "Epoch 797: lr: 2.03e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0011, val/IIA: 88.44%, val/accuracy: 94.38%, val/strict_accuracy: 74.65%\n",
      "Epoch 798: lr: 2.02e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 77.19%, val/accuracy: 94.69%, val/strict_accuracy: 75.86%\n",
      "Epoch 799: lr: 2.01e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 75.94%, val/accuracy: 95.00%, val/strict_accuracy: 73.20%\n",
      "Epoch 800: lr: 2.00e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 88.13%, val/accuracy: 94.38%, val/strict_accuracy: 74.10%\n",
      "Epoch 801: lr: 1.99e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.81%, val/accuracy: 94.38%, val/strict_accuracy: 75.43%\n",
      "Epoch 802: lr: 1.98e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0028, val/IIA: 76.25%, val/accuracy: 94.38%, val/strict_accuracy: 74.96%\n",
      "Epoch 803: lr: 1.97e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 75.94%, val/accuracy: 94.69%, val/strict_accuracy: 71.80%\n",
      "Epoch 804: lr: 1.96e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 75.94%, val/accuracy: 95.00%, val/strict_accuracy: 72.07%\n",
      "Epoch 805: lr: 1.95e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0028, val/IIA: 77.19%, val/accuracy: 94.38%, val/strict_accuracy: 75.04%\n",
      "Epoch 806: lr: 1.94e-04, train/iit_loss: 0.0005, train/behavior_loss: 0.0002, train/strict_loss: 0.0008, val/iit_loss: 0.0011, val/IIA: 87.81%, val/accuracy: 94.69%, val/strict_accuracy: 75.55%\n",
      "Epoch 807: lr: 1.93e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 95.00%, val/strict_accuracy: 72.58%\n",
      "Epoch 808: lr: 1.92e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 73.75%, val/accuracy: 94.69%, val/strict_accuracy: 70.90%\n",
      "Epoch 809: lr: 1.91e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 76.88%, val/accuracy: 94.38%, val/strict_accuracy: 73.98%\n",
      "Epoch 810: lr: 1.90e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0011, val/IIA: 87.81%, val/accuracy: 94.38%, val/strict_accuracy: 75.55%\n",
      "Epoch 811: lr: 1.89e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 76.56%, val/accuracy: 94.38%, val/strict_accuracy: 73.75%\n",
      "Epoch 812: lr: 1.88e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 94.38%, val/strict_accuracy: 73.16%\n",
      "Epoch 813: lr: 1.87e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0012, val/IIA: 86.87%, val/accuracy: 94.38%, val/strict_accuracy: 73.32%\n",
      "Epoch 814: lr: 1.86e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 87.50%, val/accuracy: 94.38%, val/strict_accuracy: 75.04%\n",
      "Epoch 815: lr: 1.85e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 88.75%, val/accuracy: 94.38%, val/strict_accuracy: 74.49%\n",
      "Epoch 816: lr: 1.84e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 84.38%, val/accuracy: 95.00%, val/strict_accuracy: 70.94%\n",
      "Epoch 817: lr: 1.83e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 87.81%, val/accuracy: 94.06%, val/strict_accuracy: 73.91%\n",
      "Epoch 818: lr: 1.82e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0012, val/IIA: 88.44%, val/accuracy: 94.38%, val/strict_accuracy: 74.77%\n",
      "Epoch 819: lr: 1.81e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 87.81%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 820: lr: 1.80e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 77.19%, val/accuracy: 93.75%, val/strict_accuracy: 73.20%\n",
      "Epoch 821: lr: 1.79e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 93.75%, val/strict_accuracy: 73.16%\n",
      "Epoch 822: lr: 1.78e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 87.50%, val/accuracy: 94.06%, val/strict_accuracy: 74.88%\n",
      "Epoch 823: lr: 1.77e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 87.50%, val/accuracy: 94.06%, val/strict_accuracy: 74.92%\n",
      "Epoch 824: lr: 1.76e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0028, val/IIA: 75.00%, val/accuracy: 94.38%, val/strict_accuracy: 72.07%\n",
      "Epoch 825: lr: 1.75e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 85.00%, val/accuracy: 94.69%, val/strict_accuracy: 71.13%\n",
      "Epoch 826: lr: 1.74e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 87.81%, val/accuracy: 94.06%, val/strict_accuracy: 74.80%\n",
      "Epoch 827: lr: 1.73e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0028, val/IIA: 77.50%, val/accuracy: 94.06%, val/strict_accuracy: 74.73%\n",
      "Epoch 828: lr: 1.72e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.69%, val/accuracy: 94.69%, val/strict_accuracy: 71.13%\n",
      "Epoch 829: lr: 1.71e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0029, val/IIA: 73.44%, val/accuracy: 94.38%, val/strict_accuracy: 70.66%\n",
      "Epoch 830: lr: 1.70e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 87.50%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 831: lr: 1.69e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 87.19%, val/accuracy: 94.06%, val/strict_accuracy: 75.00%\n",
      "Epoch 832: lr: 1.68e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 74.10%\n",
      "Epoch 833: lr: 1.67e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0028, val/IIA: 74.06%, val/accuracy: 93.44%, val/strict_accuracy: 71.09%\n",
      "Epoch 834: lr: 1.66e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 87.19%, val/accuracy: 93.75%, val/strict_accuracy: 74.02%\n",
      "Epoch 835: lr: 1.65e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 87.19%, val/accuracy: 94.06%, val/strict_accuracy: 74.53%\n",
      "Epoch 836: lr: 1.64e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0013, val/IIA: 85.62%, val/accuracy: 93.44%, val/strict_accuracy: 72.81%\n",
      "Epoch 837: lr: 1.63e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 83.44%, val/accuracy: 93.12%, val/strict_accuracy: 70.74%\n",
      "Epoch 838: lr: 1.62e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0029, val/IIA: 76.56%, val/accuracy: 94.06%, val/strict_accuracy: 74.49%\n",
      "Epoch 839: lr: 1.61e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 86.25%, val/accuracy: 94.06%, val/strict_accuracy: 74.96%\n",
      "Epoch 840: lr: 1.60e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0029, val/IIA: 75.94%, val/accuracy: 92.81%, val/strict_accuracy: 72.03%\n",
      "Epoch 841: lr: 1.59e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0030, val/IIA: 73.44%, val/accuracy: 93.12%, val/strict_accuracy: 69.73%\n",
      "Epoch 842: lr: 1.58e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 85.00%, val/accuracy: 94.06%, val/strict_accuracy: 74.34%\n",
      "Epoch 843: lr: 1.57e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 86.25%, val/accuracy: 93.44%, val/strict_accuracy: 74.53%\n",
      "Epoch 844: lr: 1.56e-04, train/iit_loss: 0.0016, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0029, val/IIA: 75.94%, val/accuracy: 92.81%, val/strict_accuracy: 72.93%\n",
      "Epoch 845: lr: 1.55e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0017, val/IIA: 80.94%, val/accuracy: 93.12%, val/strict_accuracy: 68.87%\n",
      "Epoch 846: lr: 1.54e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0003, train/strict_loss: 0.0001, val/iit_loss: 0.0015, val/IIA: 84.69%, val/accuracy: 91.87%, val/strict_accuracy: 69.84%\n",
      "Epoch 847: lr: 1.53e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 76.25%, val/accuracy: 93.75%, val/strict_accuracy: 74.18%\n",
      "Epoch 848: lr: 1.52e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 85.31%, val/accuracy: 94.06%, val/strict_accuracy: 74.22%\n",
      "Epoch 849: lr: 1.51e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0015, val/IIA: 84.69%, val/accuracy: 92.19%, val/strict_accuracy: 70.82%\n",
      "Epoch 850: lr: 1.50e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0030, val/IIA: 74.69%, val/accuracy: 91.87%, val/strict_accuracy: 69.69%\n",
      "Epoch 851: lr: 1.49e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0030, val/IIA: 76.56%, val/accuracy: 93.12%, val/strict_accuracy: 73.59%\n",
      "Epoch 852: lr: 1.48e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0031, val/IIA: 76.56%, val/accuracy: 93.75%, val/strict_accuracy: 74.06%\n",
      "Epoch 853: lr: 1.47e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 84.69%, val/accuracy: 92.81%, val/strict_accuracy: 71.88%\n",
      "Epoch 854: lr: 1.46e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0031, val/IIA: 72.50%, val/accuracy: 92.50%, val/strict_accuracy: 68.98%\n",
      "Epoch 855: lr: 1.45e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0030, val/IIA: 74.37%, val/accuracy: 91.87%, val/strict_accuracy: 69.84%\n",
      "Epoch 856: lr: 1.44e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0030, val/IIA: 76.25%, val/accuracy: 93.75%, val/strict_accuracy: 74.22%\n",
      "Epoch 857: lr: 1.43e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 85.00%, val/accuracy: 93.75%, val/strict_accuracy: 74.02%\n",
      "Epoch 858: lr: 1.42e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0015, val/IIA: 84.38%, val/accuracy: 91.87%, val/strict_accuracy: 69.49%\n",
      "Epoch 859: lr: 1.41e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0017, val/IIA: 81.25%, val/accuracy: 92.50%, val/strict_accuracy: 68.28%\n",
      "Epoch 860: lr: 1.40e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0003, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 85.00%, val/accuracy: 92.19%, val/strict_accuracy: 71.05%\n",
      "Epoch 861: lr: 1.39e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 85.62%, val/accuracy: 93.75%, val/strict_accuracy: 74.18%\n",
      "Epoch 862: lr: 1.38e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 86.56%, val/accuracy: 93.75%, val/strict_accuracy: 73.75%\n",
      "Epoch 863: lr: 1.37e-04, train/iit_loss: 0.0015, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0015, val/IIA: 84.69%, val/accuracy: 91.87%, val/strict_accuracy: 70.00%\n",
      "Epoch 864: lr: 1.36e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 84.69%, val/accuracy: 92.19%, val/strict_accuracy: 70.78%\n",
      "Epoch 865: lr: 1.35e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.44%\n",
      "Epoch 866: lr: 1.34e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0029, val/IIA: 76.88%, val/accuracy: 93.44%, val/strict_accuracy: 73.71%\n",
      "Epoch 867: lr: 1.33e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 85.31%, val/accuracy: 92.81%, val/strict_accuracy: 72.66%\n",
      "Epoch 868: lr: 1.32e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.38%, val/accuracy: 92.81%, val/strict_accuracy: 71.25%\n",
      "Epoch 869: lr: 1.31e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 84.69%, val/accuracy: 92.19%, val/strict_accuracy: 70.78%\n",
      "Epoch 870: lr: 1.30e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.69%, val/accuracy: 92.81%, val/strict_accuracy: 71.84%\n",
      "Epoch 871: lr: 1.29e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0028, val/IIA: 77.19%, val/accuracy: 92.81%, val/strict_accuracy: 72.62%\n",
      "Epoch 872: lr: 1.28e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 74.22%\n",
      "Epoch 873: lr: 1.27e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0028, val/IIA: 76.88%, val/accuracy: 92.81%, val/strict_accuracy: 73.05%\n",
      "Epoch 874: lr: 1.26e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0014, val/IIA: 84.38%, val/accuracy: 92.81%, val/strict_accuracy: 71.25%\n",
      "Epoch 875: lr: 1.25e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0007, val/iit_loss: 0.0014, val/IIA: 85.00%, val/accuracy: 92.81%, val/strict_accuracy: 70.86%\n",
      "Epoch 876: lr: 1.24e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0028, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.28%\n",
      "Epoch 877: lr: 1.23e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 85.62%, val/accuracy: 94.06%, val/strict_accuracy: 74.69%\n",
      "Epoch 878: lr: 1.22e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 87.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.36%\n",
      "Epoch 879: lr: 1.21e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0028, val/IIA: 73.44%, val/accuracy: 92.81%, val/strict_accuracy: 71.05%\n",
      "Epoch 880: lr: 1.20e-04, train/iit_loss: 0.0007, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.38%, val/accuracy: 92.81%, val/strict_accuracy: 71.33%\n",
      "Epoch 881: lr: 1.19e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 85.00%, val/accuracy: 92.81%, val/strict_accuracy: 72.73%\n",
      "Epoch 882: lr: 1.18e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 85.94%, val/accuracy: 93.75%, val/strict_accuracy: 74.92%\n",
      "Epoch 883: lr: 1.17e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 76.88%, val/accuracy: 93.44%, val/strict_accuracy: 73.28%\n",
      "Epoch 884: lr: 1.16e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0014, val/IIA: 84.38%, val/accuracy: 92.81%, val/strict_accuracy: 71.45%\n",
      "Epoch 885: lr: 1.15e-04, train/iit_loss: 0.0014, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0028, val/IIA: 73.12%, val/accuracy: 93.44%, val/strict_accuracy: 71.17%\n",
      "Epoch 886: lr: 1.14e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 87.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.40%\n",
      "Epoch 887: lr: 1.13e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0028, val/IIA: 77.81%, val/accuracy: 94.06%, val/strict_accuracy: 74.88%\n",
      "Epoch 888: lr: 1.12e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 86.25%, val/accuracy: 93.75%, val/strict_accuracy: 74.30%\n",
      "Epoch 889: lr: 1.11e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0027, val/IIA: 76.25%, val/accuracy: 93.44%, val/strict_accuracy: 72.30%\n",
      "Epoch 890: lr: 1.10e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 84.06%, val/accuracy: 92.81%, val/strict_accuracy: 70.70%\n",
      "Epoch 891: lr: 1.09e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0014, val/IIA: 84.69%, val/accuracy: 93.44%, val/strict_accuracy: 71.60%\n",
      "Epoch 892: lr: 1.08e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 87.50%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 893: lr: 1.07e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 86.56%, val/accuracy: 93.75%, val/strict_accuracy: 74.18%\n",
      "Epoch 894: lr: 1.06e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0013, val/IIA: 85.62%, val/accuracy: 93.44%, val/strict_accuracy: 73.12%\n",
      "Epoch 895: lr: 1.05e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 75.00%, val/accuracy: 94.06%, val/strict_accuracy: 72.42%\n",
      "Epoch 896: lr: 1.04e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0027, val/IIA: 76.25%, val/accuracy: 93.44%, val/strict_accuracy: 73.09%\n",
      "Epoch 897: lr: 1.03e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0027, val/IIA: 77.19%, val/accuracy: 93.75%, val/strict_accuracy: 73.83%\n",
      "Epoch 898: lr: 1.02e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0026, val/IIA: 77.19%, val/accuracy: 93.75%, val/strict_accuracy: 73.91%\n",
      "Epoch 899: lr: 1.01e-04, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.25%, val/accuracy: 93.75%, val/strict_accuracy: 73.32%\n",
      "Epoch 900: lr: 1.00e-04, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 84.69%, val/accuracy: 94.06%, val/strict_accuracy: 72.38%\n",
      "Epoch 901: lr: 9.90e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.25%, val/accuracy: 94.06%, val/strict_accuracy: 72.89%\n",
      "Epoch 902: lr: 9.80e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 87.19%, val/accuracy: 93.75%, val/strict_accuracy: 73.95%\n",
      "Epoch 903: lr: 9.70e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 87.19%, val/accuracy: 93.75%, val/strict_accuracy: 73.95%\n",
      "Epoch 904: lr: 9.60e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 85.62%, val/accuracy: 94.06%, val/strict_accuracy: 73.01%\n",
      "Epoch 905: lr: 9.50e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.94%, val/accuracy: 94.06%, val/strict_accuracy: 72.77%\n",
      "Epoch 906: lr: 9.40e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.25%, val/accuracy: 94.06%, val/strict_accuracy: 72.77%\n",
      "Epoch 907: lr: 9.30e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 87.50%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 908: lr: 9.20e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 77.19%, val/accuracy: 93.75%, val/strict_accuracy: 74.37%\n",
      "Epoch 909: lr: 9.10e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.56%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 910: lr: 9.00e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.25%, val/accuracy: 94.06%, val/strict_accuracy: 72.97%\n",
      "Epoch 911: lr: 8.90e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 85.94%, val/accuracy: 94.06%, val/strict_accuracy: 72.58%\n",
      "Epoch 912: lr: 8.80e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.94%, val/accuracy: 94.06%, val/strict_accuracy: 72.58%\n",
      "Epoch 913: lr: 8.70e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 94.06%, val/strict_accuracy: 72.93%\n",
      "Epoch 914: lr: 8.60e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0026, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 73.16%\n",
      "Epoch 915: lr: 8.50e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 88.13%, val/accuracy: 93.75%, val/strict_accuracy: 73.75%\n",
      "Epoch 916: lr: 8.40e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 73.75%\n",
      "Epoch 917: lr: 8.30e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 93.75%, val/strict_accuracy: 73.13%\n",
      "Epoch 918: lr: 8.20e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 75.94%, val/accuracy: 94.06%, val/strict_accuracy: 72.70%\n",
      "Epoch 919: lr: 8.10e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0013, val/IIA: 85.00%, val/accuracy: 92.50%, val/strict_accuracy: 72.19%\n",
      "Epoch 920: lr: 8.00e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0013, val/IIA: 86.56%, val/accuracy: 94.06%, val/strict_accuracy: 72.27%\n",
      "Epoch 921: lr: 7.90e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 87.81%, val/accuracy: 93.75%, val/strict_accuracy: 73.32%\n",
      "Epoch 922: lr: 7.80e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 77.19%, val/accuracy: 93.75%, val/strict_accuracy: 74.77%\n",
      "Epoch 923: lr: 7.70e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 87.19%, val/accuracy: 93.75%, val/strict_accuracy: 74.61%\n",
      "Epoch 924: lr: 7.60e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 76.56%, val/accuracy: 94.06%, val/strict_accuracy: 73.01%\n",
      "Epoch 925: lr: 7.50e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0026, val/IIA: 74.37%, val/accuracy: 92.50%, val/strict_accuracy: 72.50%\n",
      "Epoch 926: lr: 7.40e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0026, val/IIA: 75.94%, val/accuracy: 94.06%, val/strict_accuracy: 72.66%\n",
      "Epoch 927: lr: 7.30e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 94.38%, val/strict_accuracy: 73.48%\n",
      "Epoch 928: lr: 7.20e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 73.52%\n",
      "Epoch 929: lr: 7.10e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 94.38%, val/strict_accuracy: 73.36%\n",
      "Epoch 930: lr: 7.00e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0025, val/IIA: 76.56%, val/accuracy: 94.38%, val/strict_accuracy: 73.36%\n",
      "Epoch 931: lr: 6.90e-05, train/iit_loss: 0.0013, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 76.25%, val/accuracy: 94.06%, val/strict_accuracy: 72.89%\n",
      "Epoch 932: lr: 6.80e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 76.56%, val/accuracy: 94.38%, val/strict_accuracy: 73.44%\n",
      "Epoch 933: lr: 6.70e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 76.56%, val/accuracy: 94.38%, val/strict_accuracy: 73.36%\n",
      "Epoch 934: lr: 6.60e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.56%, val/accuracy: 93.75%, val/strict_accuracy: 73.40%\n",
      "Epoch 935: lr: 6.50e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.25%, val/accuracy: 94.06%, val/strict_accuracy: 73.01%\n",
      "Epoch 936: lr: 6.40e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.25%, val/accuracy: 94.06%, val/strict_accuracy: 73.05%\n",
      "Epoch 937: lr: 6.30e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 938: lr: 6.20e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 73.59%\n",
      "Epoch 939: lr: 6.10e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 940: lr: 6.00e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 73.59%\n",
      "Epoch 941: lr: 5.90e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 942: lr: 5.80e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.44%, val/strict_accuracy: 73.44%\n",
      "Epoch 943: lr: 5.70e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.36%\n",
      "Epoch 944: lr: 5.60e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 945: lr: 5.50e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 86.87%, val/accuracy: 93.75%, val/strict_accuracy: 73.52%\n",
      "Epoch 946: lr: 5.40e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.44%, val/strict_accuracy: 73.32%\n",
      "Epoch 947: lr: 5.30e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.32%\n",
      "Epoch 948: lr: 5.20e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.32%\n",
      "Epoch 949: lr: 5.10e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 76.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.20%\n",
      "Epoch 950: lr: 5.00e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 72.85%\n",
      "Epoch 951: lr: 4.90e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.75%, val/strict_accuracy: 73.36%\n",
      "Epoch 952: lr: 4.80e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 87.19%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 953: lr: 4.70e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 87.81%, val/accuracy: 93.75%, val/strict_accuracy: 73.63%\n",
      "Epoch 954: lr: 4.60e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 87.81%, val/accuracy: 93.75%, val/strict_accuracy: 73.44%\n",
      "Epoch 955: lr: 4.50e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 88.13%, val/accuracy: 93.75%, val/strict_accuracy: 73.59%\n",
      "Epoch 956: lr: 4.40e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.20%\n",
      "Epoch 957: lr: 4.30e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 91.87%, val/strict_accuracy: 72.50%\n",
      "Epoch 958: lr: 4.20e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 75.63%, val/accuracy: 92.50%, val/strict_accuracy: 72.50%\n",
      "Epoch 959: lr: 4.10e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0025, val/IIA: 75.94%, val/accuracy: 92.50%, val/strict_accuracy: 72.54%\n",
      "Epoch 960: lr: 4.00e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 91.87%, val/strict_accuracy: 72.66%\n",
      "Epoch 961: lr: 3.90e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.20%\n",
      "Epoch 962: lr: 3.80e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 88.13%, val/accuracy: 93.75%, val/strict_accuracy: 73.59%\n",
      "Epoch 963: lr: 3.70e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 93.75%, val/strict_accuracy: 73.48%\n",
      "Epoch 964: lr: 3.60e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 88.13%, val/accuracy: 93.75%, val/strict_accuracy: 73.55%\n",
      "Epoch 965: lr: 3.50e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.24%\n",
      "Epoch 966: lr: 3.40e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 93.44%, val/strict_accuracy: 72.97%\n",
      "Epoch 967: lr: 3.30e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 91.87%, val/strict_accuracy: 72.66%\n",
      "Epoch 968: lr: 3.20e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 85.94%, val/accuracy: 91.87%, val/strict_accuracy: 72.70%\n",
      "Epoch 969: lr: 3.10e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 93.44%, val/strict_accuracy: 73.13%\n",
      "Epoch 970: lr: 3.00e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.28%\n",
      "Epoch 971: lr: 2.90e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.75%, val/strict_accuracy: 73.52%\n",
      "Epoch 972: lr: 2.80e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.52%\n",
      "Epoch 973: lr: 2.70e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.32%\n",
      "Epoch 974: lr: 2.60e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.25%, val/accuracy: 93.44%, val/strict_accuracy: 73.40%\n",
      "Epoch 975: lr: 2.50e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.32%\n",
      "Epoch 976: lr: 2.40e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 85.94%, val/accuracy: 93.44%, val/strict_accuracy: 73.36%\n",
      "Epoch 977: lr: 2.30e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.36%\n",
      "Epoch 978: lr: 2.20e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.40%\n",
      "Epoch 979: lr: 2.10e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.52%\n",
      "Epoch 980: lr: 2.00e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.87%, val/accuracy: 93.44%, val/strict_accuracy: 73.52%\n",
      "Epoch 981: lr: 1.90e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.55%\n",
      "Epoch 982: lr: 1.80e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 87.81%, val/accuracy: 93.44%, val/strict_accuracy: 73.55%\n",
      "Epoch 983: lr: 1.70e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.52%\n",
      "Epoch 984: lr: 1.60e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 76.88%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 985: lr: 1.50e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 986: lr: 1.40e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 77.19%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 987: lr: 1.30e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.44%\n",
      "Epoch 988: lr: 1.20e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.44%\n",
      "Epoch 989: lr: 1.10e-05, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 990: lr: 1.00e-05, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 991: lr: 9.00e-06, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 992: lr: 8.00e-06, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 993: lr: 7.00e-06, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 994: lr: 6.00e-06, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.48%\n",
      "Epoch 995: lr: 5.00e-06, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.44%\n",
      "Epoch 996: lr: 4.00e-06, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.40%\n",
      "Epoch 997: lr: 3.00e-06, train/iit_loss: 0.0006, train/behavior_loss: 0.0002, train/strict_loss: 0.0001, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.44%\n",
      "Epoch 998: lr: 2.00e-06, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0003, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.44%\n",
      "Epoch 999: lr: 1.00e-06, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0006, val/iit_loss: 0.0025, val/IIA: 77.50%, val/accuracy: 93.44%, val/strict_accuracy: 73.40%\n",
      "Epoch 1000: lr: 0.00e+00, train/iit_loss: 0.0012, train/behavior_loss: 0.0002, train/strict_loss: 0.0002, val/iit_loss: 0.0012, val/IIA: 86.56%, val/accuracy: 93.44%, val/strict_accuracy: 73.40%\n"
     ]
    }
   ],
   "source": [
    "model_pair.train(\n",
    "    train_set=this_train_set,\n",
    "    test_set=this_test_set,\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
