{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadb87f4-0900-402d-bcb9-80d8ab780d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08091f5a-0c98-4367-80e3-7611a4f5d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Finding sample substrings:   0%|          | 0/6 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the markers [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\n",
      "tensor(0.) 0\n",
      "BOS ( ) ) ( ) ( ( ( ) ) ( ) ( ) ( ) ( ) ( ( ( ) ) ( ) ) ) ) ( ( ) ) ) ( ( ) ) ) ) ) PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ( ( ) ) ) ( ( ) ) ) ) ( ) ( ( ( ( ) ( ) ) ( ( ) ( ( ( ( ) ) ) ) ) ( ) ) ) ) ( PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ( ) ) ) ( ) ) ( ( ( ( ) ( ( ) ( ( ( ( ) ( ) ) ( ) ( ( ) ) ) ( ( ) ( ( ( ( ) ( PAD\n",
      "tensor(0.) 0\n",
      "BOS ) ) ( ) ) ( ) ) ) ) ) ) ( ( ( ( ( ( ) ( ( ) ( ) ( ) ) ( ( ) ) ) ) ( ( ( ) ) ( ( PAD\n",
      "tensor(1.) 1\n",
      "BOS ( ( ( ) ) ( ( ) ( ( ) ) ) ( ) ( ( ) ) ) PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(1.) 1\n",
      "BOS ( ( ) ) PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(1.) 1\n",
      "BOS ( ( ( ) ) ) PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(1.) 1\n",
      "BOS ( ( ( ) ) ) ( ( ) ) PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ( ) ) ) ( PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ) ) ( ) ( ( ( ) ) ( ) ( ) ( ) ( ) ( ( ( ) ) ( ) ) ) ) ( ( PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ) ) ( PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ( ( ) ) ) ( ( ) ) ) ) ( ) ( ( ( ( ) ( ) ) ( ( ) ( ( ( ( ) ) ) ) ) PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ( ( ( ) ) ( ) ( ( ( ( ) ( ) ) PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ( PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ( ( ) ) ( ( ) ( ( ) ) PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "tensor(0.) 0\n",
      "BOS ( ( ( ) ) ( ( ) PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from paren_checker import HighLevelParensBalanceChecker, BalancedParensDataset, SequentialParensDataset\n",
    "from paren_checker import test_HL_parens_components\n",
    "test_HL_parens_components()\n",
    "balance_checker = HighLevelParensBalanceChecker()\n",
    "dset = BalancedParensDataset(\n",
    "        N_samples = 5,\n",
    "        n_ctx = 42, #accounts for a BOS and a PAD\n",
    "        seed = 42\n",
    "    )\n",
    "print('the markers', dset.get_dataset()['markers'])\n",
    "for item in dset.get_dataset():\n",
    "    output = balance_checker(t.Tensor(item['tokens'])[None,:])\n",
    "    print(output[0,-1], item['labels'])\n",
    "    print(''.join(item['str_tokens']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33c5b0b-abee-4cf4-a17f-73868919b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-2): 3 x TransformerBlock(\n",
      "      (ln1): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n",
      "\n",
      "Correspondence:\n",
      "input_hook {LLNode(name='blocks.0.hook_resid_pre', index=[:], subspace=None)}\n",
      "elevation_hook {LLNode(name='blocks.0.attn.hook_z', index=[:, :, 0, :], subspace=None)}\n",
      "elevation_check_hook {LLNode(name='blocks.1.mlp.hook_post', index=[:], subspace=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "       dtype=torch.int32))}\n",
      "horizon_check_hook {LLNode(name='blocks.1.mlp.hook_post', index=[:], subspace=tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63],\n",
      "       dtype=torch.int32))}\n",
      "horizon_lookback_hook {LLNode(name='blocks.2.attn.hook_z', index=[:, :, 1, :], subspace=None)}\n",
      "balance_check_hook {LLNode(name='blocks.2.mlp.hook_post', index=[:], subspace=None)}\n",
      "\n",
      "Unused:\n",
      "LLNode(name='blocks.0.mlp.hook_post', index=[:], subspace=None)\n",
      "LLNode(name='blocks.1.attn.hook_z', index=[:, :, 0, :], subspace=None)\n",
      "LLNode(name='blocks.1.attn.hook_z', index=[:, :, 1, :], subspace=None)\n",
      "LLNode(name='blocks.2.attn.hook_z', index=[:, :, 0, :], subspace=None)\n"
     ]
    }
   ],
   "source": [
    "from paren_checker import get_LL_parens_model_and_correspondence\n",
    "ll_model, corr, unused_nodes = get_LL_parens_model_and_correspondence()\n",
    "print(\"Model:\")\n",
    "print(ll_model)\n",
    "print(\"\\nCorrespondence:\")\n",
    "for k, i in corr.items():\n",
    "    print(k, i)\n",
    "print(\"\\nUnused:\")\n",
    "for n in unused_nodes:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ad087a-b145-48f4-ab2c-75ecc6f7b2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Finding sample substrings:   0%|          | 0/125000 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97472, 4)\n"
     ]
    }
   ],
   "source": [
    "dset = BalancedParensDataset(\n",
    "        N_samples = 100_000,\n",
    "        n_ctx = 42, #accounts for a BOS and a PAD\n",
    "        seed = 42\n",
    "    )\n",
    "dataset = dset.get_dataset()\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8849e1c-0a95-4b12-97d3-8c94278adb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8793335d7ff049c4ad3770b484885c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Epoch 1/10:   0%|          | 0/10 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainerSIIT(\n\u001b[1;32m      9\u001b[0m     ll_model\u001b[38;5;241m=\u001b[39mll_model,\n\u001b[1;32m     10\u001b[0m     hl_model\u001b[38;5;241m=\u001b[39mbalance_checker,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#epoch 6: iia = 1, loss = 0.0027, epoch 7: iia = 1, loss = 0.00296; epoch 8: iia=1, loss = 0.00696; epoch 9: iia=1, loss = 0.00611; epoch 10: iia=1, loss=0.00277\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/quick-experiments/siit_utils.py:252\u001b[0m, in \u001b[0;36mModelTrainerSIIT.train\u001b[0;34m(self, epochs, use_wandb, lr, **optim_kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:631\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    616\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    624\u001b[0m ]:\n\u001b[1;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    635\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformer_lens/hook_points.py:566\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    553\u001b[0m     names_filter,\n\u001b[1;32m    554\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    561\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    562\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    563\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    564\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    565\u001b[0m ):\n\u001b[0;32m--> 566\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    568\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:550\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    547\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    548\u001b[0m         )\n\u001b[0;32m--> 550\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:159\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    152\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    153\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    155\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:195\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    169\u001b[0m     query_input: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     position_bias: Optional[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 head_index pos kv_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    shortformer_pos_embed is only used if self.cfg.positional_embedding_type == \"shortformer\", else defaults to None and is irrelevant. See HookedTransformerConfig for more details\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    attention_mask is the attention mask for padded tokens. Defaults to None.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_qkv_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache_entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# Appends the new keys and values to the cached values, and automatically updates the cache\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         kv_cache_pos_offset \u001b[38;5;241m=\u001b[39m past_kv_cache_entry\u001b[38;5;241m.\u001b[39mpast_keys\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py:356\u001b[0m, in \u001b[0;36mAbstractAttention.calculate_qkv_matrices\u001b[0;34m(self, query_input, key_input, value_input)\u001b[0m\n\u001b[1;32m    340\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_q(\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;66;03m# call bitsandbytes method to dequantize and multiply\u001b[39;00m\n\u001b[1;32m    342\u001b[0m         bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_Q\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 356\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_q\u001b[49m(attn_fn(query_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_Q, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_Q))\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mload_in_4bit:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_K, Params4bit):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1699\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1698\u001b[0m     _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 1699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _parameters:\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _parameters[name]\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from siit_utils import ModelTrainerSIIT\n",
    "from paren_checker import paren_checker_loss_fn as loss_fn\n",
    "\n",
    "t.manual_seed(150) #150 for 10 epochs does pretty well; might be better at 6 epochs.\n",
    "ll_model, corr, unused_nodes = get_LL_parens_model_and_correspondence(n_ctx = 42)\n",
    "\n",
    "trainer = ModelTrainerSIIT(\n",
    "    ll_model=ll_model,\n",
    "    hl_model=balance_checker,\n",
    "    dataset=dataset,\n",
    "    corr=corr,\n",
    "    unused_nodes=unused_nodes,\n",
    "    loss_fn=loss_fn,\n",
    "    baseline_weight = 1,\n",
    "    iit_weight = 1,\n",
    "    siit_weight = 1,\n",
    "    batch_size = 512,\n",
    "    device = 'cuda'\n",
    ")\n",
    "#epoch 6: iia = 1, loss = 0.0027, epoch 7: iia = 1, loss = 0.00296; epoch 8: iia=1, loss = 0.00696; epoch 9: iia=1, loss = 0.00611; epoch 10: iia=1, loss=0.00277\n",
    "results = trainer.train(epochs=100, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a51fc16-d9fe-497b-bb03-88490768679b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0ce0f352d0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(results['train_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77d0dac6-6c3e-4a0b-8716-42d5b01cb3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3049.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "steps = np.arange(len(np.array(results['train_IIA'])))\n",
    "plt.semilogy(steps, 1 - np.array(results['train_IIA']))\n",
    "plt.xlim(0, steps.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f6f419e-938c-4f75-b9d7-f6cc712322b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9407844760201194,\n",
       " 0.9913758116883117,\n",
       " 0.9642857142857143,\n",
       " 0.9990361201298701,\n",
       " 0.9987824675324676,\n",
       " 0.9999492694805194,\n",
       " 0.9996448863636364,\n",
       " 0.9997463474025974,\n",
       " 0.9996448863636364,\n",
       " 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['test_IIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61f9b230-1fc9-47d9-b7dc-70aea22b25db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 0/19495, Ablated Err: 1/19495\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from siit_utils import make_post_ablation_hook\n",
    "tot = 0\n",
    "errors = 0\n",
    "ablated_errors = 0\n",
    "for b in trainer.test_dataloaders[0]:\n",
    "    tokens, labels = b\n",
    "    logits, cache = ll_model.run_with_cache(tokens)\n",
    "    output_labels = t.round(t.sigmoid(logits)[:,-1,-1])\n",
    "    sumdiff = (labels.cuda() != output_labels.float()).sum().item()\n",
    "\n",
    "    hooks = []\n",
    "    for node in unused_nodes:\n",
    "        hooks.append((node.name, make_post_ablation_hook(ll_node=node, ll_cache=cache, method='mean')))\n",
    "    ablated_logits = ll_model.run_with_hooks(tokens, fwd_hooks=hooks)\n",
    "    \n",
    "    ablated_labels = t.round(t.sigmoid(ablated_logits)[:,-1,-1])\n",
    "    ablated_sumdiff = (labels.cuda() != ablated_labels.float()).sum().item()\n",
    "\n",
    "    errors += sumdiff\n",
    "    ablated_errors += ablated_sumdiff\n",
    "    tot += labels.numel()\n",
    "print(f'Errors: {errors}/{tot}, Ablated Err: {ablated_errors}/{tot}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4175b6e-6308-455d-bff9-5cece1ab0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add lines to save model to huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed4627-f8eb-4ccc-be69-35beb1138f07",
   "metadata": {},
   "source": [
    "# Take a peek at the attention pattern on the important heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cce14-255f-43a2-b427-3149481a8475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c580dbba-f101-4bf4-a1ea-15f9a50b801d",
   "metadata": {},
   "source": [
    "# SAEs -- vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51bb0308-b17b-405a-95fb-d7d0895b5197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Encoded: [3, 0, 1, 0, 1, 2, 2, 2]\n",
      "Decoded: BOS ( ) ( ) PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "from paren_checker import create_paren_checker_tokenizer\n",
    "tokenizer = create_paren_checker_tokenizer()\n",
    "ll_model.tokenizer = tokenizer #attach to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc6b5b5f-8485-4cc6-8f0b-0f2f3c1a72cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_head_index 0\n",
      "d_in 8\n",
      "wandb_project benchmark_saes\n",
      "Run name: 32-L1-0.2-LR-0.0003-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 0.00336\n",
      "Lower bound: n_contexts_per_buffer (millions): 8e-05\n",
      "Total training steps: 9300\n",
      "Total wandb updates: 930\n",
      "n_tokens_per_feature_sampling_window (millions): 903.168\n",
      "n_tokens_per_dead_feature_window (millions): 451.584\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.15e+07\n",
      "Using Ghost Grads.\n"
     ]
    }
   ],
   "source": [
    "from sae_utils import make_sae_lens_config\n",
    "sae_lens_cfg = make_sae_lens_config(\n",
    "    model=ll_model,\n",
    "    hook_name=\"blocks.0.attn.hook_z\", \n",
    "    hook_layer=0, \n",
    "    l1_coefficient=0.2,\n",
    "    l1_warm_up_steps = 0,\n",
    "    hook_head_index=0, \n",
    "    context_size=ll_model.cfg.n_ctx,\n",
    "    d_in=ll_model.cfg.d_head,\n",
    "    device = 'cuda',\n",
    "    checkpoint_path = f\"$HOME/persistent-storage/tracr_saes/parens_sae_checkpoints\",\n",
    "    wandb_project =  \"benchmark_saes\",\n",
    "    training_tokens = 100_000_000,\n",
    "    batch_size = 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eacc2c33-2a8d-446b-8fb3-8e5e7cc1f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanhanders\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/quick-experiments/wandb/run-20240711_185327-w66x2lqo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evanhanders/benchmark_saes/runs/w66x2lqo' target=\"_blank\">32-L1-0.2-LR-0.0003-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/w66x2lqo' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/w66x2lqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE:   0%|                                                                         | 0/100000000 [00:00<?, ?it/s]/opt/venv/lib/python3.10/site-packages/sae_lens/training/activations_store.py:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "1800| MSE Loss 0.044 | L1 0.141:  19%|██████▊                            | 19353600/100000000 [01:35<06:37, 202837.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/dccr8n2y/20009472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3700| MSE Loss 0.033 | L1 0.127:  40%|█████████████▉                     | 39782400/100000000 [03:23<05:16, 190404.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/dccr8n2y/40008192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5500| MSE Loss 0.033 | L1 0.142:  59%|████████████████████▋              | 59136000/100000000 [05:01<03:23, 201183.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/dccr8n2y/60006912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400| MSE Loss 0.028 | L1 0.145:  80%|███████████████████████████▊       | 79564800/100000000 [06:48<01:50, 184838.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/dccr8n2y/80005632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9300| MSE Loss 0.033 | L1 0.151: 100%|██████████████████████████████████▉| 99993600/100000000 [08:32<00:00, 198136.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/dccr8n2y/final_100004352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9300| MSE Loss 0.033 | L1 0.151: 100%|██████████████████████████████████▉| 99993600/100000000 [08:33<00:00, 194713.44it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.037 MB of 0.037 MB uploaded (0.009 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 17.4%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▅▃▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▅▆▇██▇▇▇▇████████▇██▇██████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▅▅▂▁▁▂▂▃▂▂▂▂▁▁▁▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▁▁▂▂▂▂</td></tr><tr><td>metrics/l0</td><td>█▆▄▃▃▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▄▂▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▂▅█</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁██</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁▁▁█</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▂▃▁▁▂▃▄▂▃▃▅▅▃▃▄▅▅▄▃▅▅▆▅▆▅▆▅▃▄▇▇█▆▅▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.2</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>99993600</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.00414</td></tr><tr><td>losses/l1_loss</td><td>0.75696</td></tr><tr><td>losses/mse_loss</td><td>0.03315</td></tr><tr><td>losses/overall_loss</td><td>0.18868</td></tr><tr><td>metrics/explained_variance</td><td>0.84839</td></tr><tr><td>metrics/explained_variance_std</td><td>0.22962</td></tr><tr><td>metrics/l0</td><td>3.1625</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-3.54624</td></tr><tr><td>sparsity/below_1e-5</td><td>13</td></tr><tr><td>sparsity/below_1e-6</td><td>1</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>163.84375</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">32-L1-0.2-LR-0.0003-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/w66x2lqo' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/w66x2lqo</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240711_185327-w66x2lqo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_utils import train_sae\n",
    "\n",
    "#I need to be able to tell the SAE to ignore certain tokens during training.\n",
    "sae, store = train_sae(ll_model, sae_lens_cfg, dataset, batch_size=256)#, ignore_tokens=[])#2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13048231-5087-43d3-928d-84b862df9e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12938/718347451.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t.tensor(dataset['tokens']).int(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1ec3ebfadc41f686d91362c41849c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(  0%|          | 0/381 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens torch.Size([97472, 42])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4093824, 9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae_utils import make_token_df\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "n_activations_sum = t.zeros(sae.cfg.d_sae).to(sae.device)\n",
    "input_tokens_list = []\n",
    "learned_activations = []\n",
    "extras_list = []\n",
    "extra_hook = 'left_paren_hook'\n",
    "extra_name = 'left_parens'\n",
    "\n",
    "t_dataset = TensorDataset(\n",
    "    t.tensor(dataset['tokens']).int(), \n",
    "    t.tensor(dataset['labels']).float()\n",
    ")\n",
    "dataloader  = DataLoader(t_dataset, batch_size=256, shuffle = False)\n",
    "\n",
    "#go through the training dataset and get max activations for each feature\n",
    "total_inputs = 0\n",
    "for batch in tqdm(dataloader):\n",
    "    tokens, labels = batch\n",
    "    total_inputs += tokens.numel()\n",
    "    logits, cache = ll_model.run_with_cache(tokens)\n",
    "    sae_in = cache[sae.cfg.hook_name]\n",
    "    if sae.cfg.hook_head_index is not None:\n",
    "        sae_in = sae_in[:,:,sae.cfg.hook_head_index,:] #I think this is how attn head indexing works...\n",
    "    activations = sae.encode(sae_in)\n",
    "    # print(activations.shape, labels.shape, tokens.shape, tokens.numel())\n",
    "    activations[t.isin(tokens.int(), t.Tensor([2, 3]).int())] = 0\n",
    "\n",
    "    # For sparsity calculation\n",
    "    n_new_activations = (activations > 0).sum(dim=(0,1)) #sum over batch and ctx\n",
    "    n_activations_sum = n_activations_sum + n_new_activations\n",
    "\n",
    "    # Save tokens and activations\n",
    "    input_tokens_list.append(tokens.cpu())\n",
    "    learned_activations.append(activations.to(t.float16).cpu().reshape(-1, sae.cfg.d_sae))\n",
    "\n",
    "    #HL output\n",
    "    hl_output, hl_cache = balance_checker.run_with_cache(tokens)\n",
    "    extra = hl_cache[extra_hook]\n",
    "    extras_list.append(extra.cpu())\n",
    "\n",
    "    # if total_inputs > 100_000:\n",
    "    #     break\n",
    "sparsity = n_activations_sum / total_inputs\n",
    "tokens = t.cat(input_tokens_list).to(int)\n",
    "extras = t.cat(extras_list).to(int)\n",
    "token_df = make_token_df(ll_model, tokens, len_prefix=ll_model.cfg.n_ctx, \n",
    "                         extra_token_labels={extra_name : extras})\n",
    "learned_activations = t.cat(learned_activations).to(t.float16)\n",
    "token_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a00cb9d1-c7ed-41a9-ace9-ee83ce9661bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8f8a148f23465480955f64a4413629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Feature:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498f432e550840b78da23c5b62f92f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens import utils\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def update_dataframe(feature_id):\n",
    "    token_df[\"activation\"] = utils.to_numpy(learned_activations[:,feature_id])\n",
    "    df = token_df[['str_tokens','prefix', 'suffix',  'context', 'activation', extra_name]]\n",
    "    df = df.sort_values(\"activation\", ascending=False).head(100)\n",
    "    # display(df[df['activation'] > 0].style.background_gradient(\"coolwarm\"))\n",
    "    unique = df[['str_tokens', 'prefix', 'activation', extra_name]].drop_duplicates()\n",
    "    display(unique[unique['activation'] > 0].head(100).style.background_gradient(\"coolwarm\"))\n",
    "\n",
    "# Define the dropdown menu for 'feat'\n",
    "feat_dropdown = widgets.Dropdown(\n",
    "    options=range(sae.cfg.d_sae),\n",
    "    value=0,\n",
    "    description='Feature:',\n",
    ")\n",
    "\n",
    "# Create an interactive output widget\n",
    "output = widgets.interactive_output(\n",
    "    update_dataframe, \n",
    "    {\n",
    "        'feature_id': feat_dropdown,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the dropdown menu and output\n",
    "display(feat_dropdown, output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c383c1-d5a0-45a7-a328-83261f98f6db",
   "metadata": {},
   "source": [
    "sae for attn 0 head 0:\n",
    "\n",
    "TBD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7216dd9-1741-41dc-aa50-74aef1dd1b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_in 64\n",
      "wandb_project benchmark_saes\n",
      "Run name: 256-L1-0.2-LR-0.0003-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 0.00336\n",
      "Lower bound: n_contexts_per_buffer (millions): 8e-05\n",
      "Total training steps: 9300\n",
      "Total wandb updates: 930\n",
      "n_tokens_per_feature_sampling_window (millions): 903.168\n",
      "n_tokens_per_dead_feature_window (millions): 451.584\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.15e+07\n",
      "Using Ghost Grads.\n"
     ]
    }
   ],
   "source": [
    "from sae_utils import make_sae_lens_config\n",
    "sae_lens_cfg = make_sae_lens_config(\n",
    "    model=ll_model, \n",
    "    hook_name=\"blocks.0.mlp.hook_post\", \n",
    "    hook_layer=0, \n",
    "    l1_coefficient=0.2,\n",
    "    l1_warm_up_steps = 0,\n",
    "    context_size=ll_model.cfg.n_ctx,\n",
    "    d_in=ll_model.cfg.d_mlp,\n",
    "    device = 'cuda',\n",
    "    checkpoint_path = f\"$HOME/persistent-storage/tracr_saes/parens_sae_checkpoints\",\n",
    "    wandb_project =  \"benchmark_saes\",\n",
    "    training_tokens = 100_000_000,\n",
    "    batch_size = 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ab00b93-9307-482f-89fb-42286e694da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/quick-experiments/wandb/run-20240711_203830-wzwdcw5r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evanhanders/benchmark_saes/runs/wzwdcw5r' target=\"_blank\">256-L1-0.2-LR-0.0003-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/wzwdcw5r' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/wzwdcw5r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE:   0%|                                                                         | 0/100000000 [00:00<?, ?it/s]/opt/venv/lib/python3.10/site-packages/sae_lens/training/activations_store.py:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "1800| MSE Loss 0.214 | L1 0.991:  19%|██████▊                            | 19353600/100000000 [01:44<07:40, 175175.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/1ct4tqbi/20009472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3700| MSE Loss 0.196 | L1 0.907:  40%|█████████████▉                     | 39782400/100000000 [03:37<05:34, 179809.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/1ct4tqbi/40008192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5500| MSE Loss 0.245 | L1 0.811:  59%|████████████████████▋              | 59136000/100000000 [05:23<03:54, 174439.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/1ct4tqbi/60006912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400| MSE Loss 0.189 | L1 1.034:  80%|███████████████████████████▊       | 79564800/100000000 [07:17<01:53, 179617.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/1ct4tqbi/80005632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9300| MSE Loss 0.121 | L1 0.773: 100%|██████████████████████████████████▉| 99993600/100000000 [09:08<00:00, 182530.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/1ct4tqbi/final_100004352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9300| MSE Loss 0.121 | L1 0.773: 100%|██████████████████████████████████▉| 99993600/100000000 [09:08<00:00, 182207.75it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.662 MB of 0.662 MB uploaded (0.009 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 1.3%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▃▂▂▂▂▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▆▇▇████▇█████████▇█████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>metrics/l0</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▂▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▇██</td></tr><tr><td>sparsity/below_1e-6</td><td>▁██▅</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▃▁▁▁▁█▁▁▁▁▃▁▁▃▁▁▁▁▃▃▃▁▁▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▂▂▃▅▃▃▃▆█▄▄▄▅▆▄▄▅▆█▅▅▅▅▆▅▅▆▆█▆▇▆▅▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.2</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>99993600</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.00189</td></tr><tr><td>losses/l1_loss</td><td>3.86355</td></tr><tr><td>losses/mse_loss</td><td>0.12071</td></tr><tr><td>losses/overall_loss</td><td>0.89531</td></tr><tr><td>metrics/explained_variance</td><td>0.97582</td></tr><tr><td>metrics/explained_variance_std</td><td>0.02435</td></tr><tr><td>metrics/l0</td><td>12.69821</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-2.80084</td></tr><tr><td>sparsity/below_1e-5</td><td>48</td></tr><tr><td>sparsity/below_1e-6</td><td>12</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>89.12109</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">256-L1-0.2-LR-0.0003-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/wzwdcw5r' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/wzwdcw5r</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240711_203830-wzwdcw5r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_utils import train_sae\n",
    "\n",
    "#I need to be able to tell the SAE to ignore certain tokens during training.\n",
    "sae, store = train_sae(ll_model, sae_lens_cfg, dataset)#, ignore_tokens=[])#2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a034e274-5923-4936-af0e-5613db407885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12938/3317634938.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t.tensor(dataset['tokens']).int(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa68ef470534b259422701031a605cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(  0%|          | 0/381 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens torch.Size([97472, 42])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4093824, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae_utils import make_token_df\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "n_activations_sum = t.zeros(sae.cfg.d_sae).to(sae.device)\n",
    "input_tokens_list = []\n",
    "learned_activations = []\n",
    "extras_list = []\n",
    "extra_hook = 'elevation_hook'\n",
    "extra_name = 'elevation'\n",
    "\n",
    "t_dataset = TensorDataset(\n",
    "    t.tensor(dataset['tokens']).int(), \n",
    "    t.tensor(dataset['labels']).float()\n",
    ")\n",
    "dataloader  = DataLoader(t_dataset, batch_size=256, shuffle = False)\n",
    "\n",
    "#go through the training dataset and get max activations for each feature\n",
    "total_inputs = 0\n",
    "for batch in tqdm(dataloader):\n",
    "    tokens, labels = batch\n",
    "    total_inputs += tokens.numel()\n",
    "    logits, cache = ll_model.run_with_cache(tokens)\n",
    "    sae_in = cache[sae.cfg.hook_name]\n",
    "    if sae.cfg.hook_head_index is not None:\n",
    "        sae_in = sae_in[:,:,sae.cfg.hook_head_index,:] #I think this is how attn head indexing works...\n",
    "    activations = sae.encode(sae_in)\n",
    "    # print(activations.shape, labels.shape, tokens.shape, tokens.numel())\n",
    "    activations[t.isin(tokens.int(), t.Tensor([2, 3]).int())] = 0\n",
    "\n",
    "    # For sparsity calculation\n",
    "    n_new_activations = (activations > 0).sum(dim=(0,1)) #sum over batch and ctx\n",
    "    n_activations_sum = n_activations_sum + n_new_activations\n",
    "\n",
    "    # Save tokens and activations\n",
    "    input_tokens_list.append(tokens.cpu())\n",
    "    learned_activations.append(activations.to(t.float16).cpu().reshape(-1, sae.cfg.d_sae))\n",
    "\n",
    "    #HL output\n",
    "    hl_output, hl_cache = balance_checker.run_with_cache(tokens)\n",
    "    extra = hl_cache[extra_hook]\n",
    "    extras_list.append(extra.cpu())\n",
    "\n",
    "    # if total_inputs > 100_000:\n",
    "    #     break\n",
    "sparsity = n_activations_sum / total_inputs\n",
    "tokens = t.cat(input_tokens_list).to(int)\n",
    "extras = t.cat(extras_list).to(int)\n",
    "token_df = make_token_df(ll_model, tokens, len_prefix=ll_model.cfg.n_ctx, \n",
    "                         extra_token_labels={extra_name : extras})\n",
    "learned_activations = t.cat(learned_activations).to(t.float16)\n",
    "token_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ecfb13a-0839-435e-9bfc-24e940af966a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a3d2ea70f840bca8c98028048eec16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Feature:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2334e6ed4947446bae62826bd1a91849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens import utils\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def update_dataframe(feature_id):\n",
    "    token_df[\"activation\"] = utils.to_numpy(learned_activations[:,feature_id])\n",
    "    df = token_df[['str_tokens','prefix', 'suffix',  'context', 'activation', 'elevation']]\n",
    "    df = df.sort_values(\"activation\", ascending=False).head(100)\n",
    "    # display(df[df['activation'] > 0].style.background_gradient(\"coolwarm\"))\n",
    "    unique = df[['str_tokens', 'prefix', 'activation', 'elevation']].drop_duplicates()\n",
    "    display(unique[unique['activation'] > 0].head(100).style.background_gradient(\"coolwarm\"))\n",
    "\n",
    "# Define the dropdown menu for 'feat'\n",
    "feat_dropdown = widgets.Dropdown(\n",
    "    options=range(sae.cfg.d_sae),\n",
    "    value=0,\n",
    "    description='Feature:',\n",
    ")\n",
    "\n",
    "# Create an interactive output widget\n",
    "output = widgets.interactive_output(\n",
    "    update_dataframe, \n",
    "    {\n",
    "        'feature_id': feat_dropdown,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the dropdown menu and output\n",
    "display(feat_dropdown, output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92cc1002-48b6-4b8a-b01c-0b8dd23d9cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.1\n"
     ]
    }
   ],
   "source": [
    "import sae_lens\n",
    "print(sae_lens.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd529f94-c986-44c1-a023-85f2096b13d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SAELens -- top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a1643ea-ffec-44d1-909c-f6a48d37b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_fn topk\n",
      "activation_fn_kwargs {'k': 4}\n",
      "mse_loss_normalization None\n",
      "d_in 64\n",
      "wandb_project benchmark_saes\n",
      "Run name: 256-L1-0.1-LR-0.0003-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 0.02688\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 9300\n",
      "Total wandb updates: 930\n",
      "n_tokens_per_feature_sampling_window (millions): 903.168\n",
      "n_tokens_per_dead_feature_window (millions): 451.584\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.15e+07\n",
      "Using Ghost Grads.\n"
     ]
    }
   ],
   "source": [
    "from sae_utils import make_topk_sae_lens_config\n",
    "sae_lens_cfg = make_topk_sae_lens_config(\n",
    "    model=ll_model, \n",
    "    hook_name=\"blocks.0.mlp.hook_post\", \n",
    "    hook_layer=0, \n",
    "    k=4, #choose smallest value that achieves good mse loss.\n",
    "    context_size=ll_model.cfg.n_ctx,\n",
    "    d_in=ll_model.cfg.d_mlp,\n",
    "    device = 'cuda',\n",
    "    checkpoint_path = f\"$HOME/persistent-storage/tracr_saes/parens_sae_checkpoints\",\n",
    "    wandb_project =  \"benchmark_saes\",\n",
    "    training_tokens = 100_000_000,\n",
    "    batch_size = 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa44881-eeb0-4bdf-b692-4bd977c9b119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: the training dataset contains fewer samples (97472) than the number of samples required by your training configuration (100000000). This will result in multiple training epochs and some samples being used more than once.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wqc1tcgn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.273 MB of 0.273 MB uploaded (0.002 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss</td><td>█▆▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▁▄▇██████████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>losses/mse_loss</td><td>█▆▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▆▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>███████████████████████████████████▅▁▄█▅</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>▁█</td></tr><tr><td>sparsity/below_1e-5</td><td>█▁</td></tr><tr><td>sparsity/below_1e-6</td><td>█▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▅▃▁▁▁▃▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▂▄▅▆▇▇███▂▂▃▃▃▃▄▄▄▃▃▃▄▄▄▄▄▄▄▃▃▂▂▂▃▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.1</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>45696000</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0126</td></tr><tr><td>losses/l1_loss</td><td>8.73607</td></tr><tr><td>losses/mse_loss</td><td>0.8065</td></tr><tr><td>losses/overall_loss</td><td>1.6927</td></tr><tr><td>metrics/explained_variance</td><td>0.92588</td></tr><tr><td>metrics/explained_variance_std</td><td>0.06244</td></tr><tr><td>metrics/l0</td><td>4.0</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-4.18563</td></tr><tr><td>sparsity/below_1e-5</td><td>76</td></tr><tr><td>sparsity/below_1e-6</td><td>29</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>183.13281</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">256-topk-4-LR-0.0003-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/wqc1tcgn' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/wqc1tcgn</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240711_220643-wqc1tcgn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wqc1tcgn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3a67e8c4e74c67bf87b2a607dc274f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114499510990248, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/quick-experiments/wandb/run-20240711_221648-mh01kdsc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evanhanders/benchmark_saes/runs/mh01kdsc' target=\"_blank\">256-topk-4-LR-0.0003-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/mh01kdsc' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/mh01kdsc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training SAE:   0%|                                                                         | 0/100000000 [00:00<?, ?it/s]\u001b[A\u001b[A/opt/venv/lib/python3.10/site-packages/sae_lens/training/activations_store.py:264: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "/opt/venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (25elroub) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "1200| MSE Loss 1.594 | L1 0.940:  13%|████▍                             | 12902400/100000000 [18:41<2:06:08, 11507.49it/s]\n",
      "\n",
      "\n",
      "100| MSE Loss 13.181 | L1 0.473:   0%|                                                      | 0/100000000 [00:11<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100| MSE Loss 13.181 | L1 0.473:   1%|▍                                    | 1075200/100000000 [00:11<18:06, 91024.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "100| MSE Loss 13.181 | L1 0.473:   1%|▍                                    | 1075200/100000000 [00:22<18:06, 91024.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "200| MSE Loss 6.829 | L1 0.650:   1%|▍                                     | 1075200/100000000 [00:23<18:06, 91024.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "200| MSE Loss 6.829 | L1 0.650:   2%|▊                                     | 2150400/100000000 [00:23<18:06, 90018.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "300| MSE Loss 4.432 | L1 0.649:   2%|▊                                     | 2150400/100000000 [00:35<18:06, 90018.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "300| MSE Loss 4.432 | L1 0.649:   3%|█▏                                    | 3225600/100000000 [00:35<18:00, 89562.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "300| MSE Loss 4.432 | L1 0.649:   3%|█▏                                    | 3225600/100000000 [00:46<18:00, 89562.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "400| MSE Loss 3.290 | L1 0.601:   3%|█▏                                    | 3225600/100000000 [00:48<18:00, 89562.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "400| MSE Loss 3.290 | L1 0.601:   4%|█▋                                    | 4300800/100000000 [00:48<18:05, 88198.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "500| MSE Loss 2.650 | L1 0.578:   4%|█▋                                    | 4300800/100000000 [01:01<18:05, 88198.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "500| MSE Loss 2.650 | L1 0.578:   5%|██                                    | 5376000/100000000 [01:01<18:16, 86288.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "500| MSE Loss 2.650 | L1 0.578:   5%|██                                    | 5376000/100000000 [01:12<18:16, 86288.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "600| MSE Loss 2.199 | L1 0.563:   5%|██                                    | 5376000/100000000 [01:12<18:16, 86288.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "600| MSE Loss 2.199 | L1 0.563:   6%|██▍                                   | 6451200/100000000 [01:12<17:32, 88884.25it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "700| MSE Loss 1.961 | L1 0.562:   6%|██▍                                   | 6451200/100000000 [01:25<17:32, 88884.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "700| MSE Loss 1.961 | L1 0.562:   8%|██▊                                   | 7526400/100000000 [01:25<17:32, 87889.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "700| MSE Loss 1.961 | L1 0.562:   8%|██▊                                   | 7526400/100000000 [01:37<17:32, 87889.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "800| MSE Loss 1.781 | L1 0.554:   8%|██▊                                   | 7526400/100000000 [01:37<17:32, 87889.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "800| MSE Loss 1.781 | L1 0.554:   9%|███▎                                  | 8601600/100000000 [01:37<17:23, 87551.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "900| MSE Loss 1.595 | L1 0.551:   9%|███▎                                  | 8601600/100000000 [01:50<17:23, 87551.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "900| MSE Loss 1.595 | L1 0.551:  10%|███▋                                  | 9676800/100000000 [01:50<17:32, 85839.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "900| MSE Loss 1.595 | L1 0.551:  10%|███▋                                  | 9676800/100000000 [02:02<17:32, 85839.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "1000| MSE Loss 1.508 | L1 0.548:  10%|███▌                                 | 9676800/100000000 [02:04<17:32, 85839.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "1000| MSE Loss 1.508 | L1 0.548:  11%|███▊                                | 10752000/100000000 [02:04<17:48, 83536.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "1000| MSE Loss 1.508 | L1 0.548:  11%|███▊                                | 10752000/100000000 [02:17<17:48, 83536.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "1100| MSE Loss 1.436 | L1 0.545:  11%|███▊                                | 10752000/100000000 [02:17<17:48, 83536.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "1100| MSE Loss 1.436 | L1 0.545:  12%|████▎                               | 11827200/100000000 [02:17<17:48, 82523.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "1200| MSE Loss 1.305 | L1 0.537:  12%|████▎                               | 11827200/100000000 [02:31<17:48, 82523.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "1200| MSE Loss 1.305 | L1 0.537:  13%|████▋                               | 12902400/100000000 [02:31<17:53, 81164.34it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1200| MSE Loss 1.305 | L1 0.537:  13%|████▋                               | 12902400/100000000 [02:42<17:53, 81164.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "1300| MSE Loss 1.271 | L1 0.536:  13%|████▋                               | 12902400/100000000 [02:45<17:53, 81164.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "1300| MSE Loss 1.271 | L1 0.536:  14%|█████                               | 13977600/100000000 [02:45<17:54, 80071.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "1300| MSE Loss 1.271 | L1 0.536:  14%|█████                               | 13977600/100000000 [02:57<17:54, 80071.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "1400| MSE Loss 1.173 | L1 0.531:  14%|█████                               | 13977600/100000000 [02:57<17:54, 80071.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "1400| MSE Loss 1.173 | L1 0.531:  15%|█████▍                              | 15052800/100000000 [02:57<17:19, 81751.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "1500| MSE Loss 1.117 | L1 0.520:  15%|█████▍                              | 15052800/100000000 [03:09<17:19, 81751.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "1500| MSE Loss 1.117 | L1 0.520:  16%|█████▊                              | 16128000/100000000 [03:09<16:39, 83910.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "1600| MSE Loss 1.074 | L1 0.523:  16%|█████▊                              | 16128000/100000000 [03:22<16:39, 83910.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "1600| MSE Loss 1.074 | L1 0.523:  17%|██████▏                             | 17203200/100000000 [03:22<16:28, 83782.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "1600| MSE Loss 1.074 | L1 0.523:  17%|██████▏                             | 17203200/100000000 [03:32<16:28, 83782.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "1700| MSE Loss 1.041 | L1 0.521:  17%|██████▏                             | 17203200/100000000 [03:35<16:28, 83782.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "1700| MSE Loss 1.041 | L1 0.521:  18%|██████▌                             | 18278400/100000000 [03:35<16:19, 83428.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "1800| MSE Loss 0.988 | L1 0.515:  18%|██████▌                             | 18278400/100000000 [03:47<16:19, 83428.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "1800| MSE Loss 0.988 | L1 0.515:  19%|██████▉                             | 19353600/100000000 [03:47<15:44, 85368.89it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n",
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/lob29g44/20009472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1800| MSE Loss 0.988 | L1 0.515:  19%|██████▉                             | 19353600/100000000 [03:57<15:44, 85368.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "1900| MSE Loss 0.959 | L1 0.513:  19%|██████▉                             | 19353600/100000000 [04:00<15:44, 85368.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "1900| MSE Loss 0.959 | L1 0.513:  20%|███████▎                            | 20428800/100000000 [04:00<15:33, 85271.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "2000| MSE Loss 0.894 | L1 0.508:  20%|███████▎                            | 20428800/100000000 [04:12<15:33, 85271.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "2000| MSE Loss 0.894 | L1 0.508:  22%|███████▋                            | 21504000/100000000 [04:12<15:06, 86605.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "2000| MSE Loss 0.894 | L1 0.508:  22%|███████▋                            | 21504000/100000000 [04:22<15:06, 86605.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "2100| MSE Loss 0.900 | L1 0.505:  22%|███████▋                            | 21504000/100000000 [04:24<15:06, 86605.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "2100| MSE Loss 0.900 | L1 0.505:  23%|████████▏                           | 22579200/100000000 [04:24<14:52, 86775.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "2200| MSE Loss 0.902 | L1 0.505:  23%|████████▏                           | 22579200/100000000 [04:36<14:52, 86775.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "2200| MSE Loss 0.902 | L1 0.505:  24%|████████▌                           | 23654400/100000000 [04:36<14:29, 87774.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "2200| MSE Loss 0.902 | L1 0.505:  24%|████████▌                           | 23654400/100000000 [04:47<14:29, 87774.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "2300| MSE Loss 0.875 | L1 0.497:  24%|████████▌                           | 23654400/100000000 [04:48<14:29, 87774.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "2300| MSE Loss 0.875 | L1 0.497:  25%|████████▉                           | 24729600/100000000 [04:48<14:14, 88085.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "2400| MSE Loss 0.822 | L1 0.492:  25%|████████▉                           | 24729600/100000000 [05:00<14:14, 88085.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "2400| MSE Loss 0.822 | L1 0.492:  26%|█████████▎                          | 25804800/100000000 [05:00<13:48, 89606.12it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2500| MSE Loss 0.823 | L1 0.486:  26%|█████████▎                          | 25804800/100000000 [05:12<13:48, 89606.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "2500| MSE Loss 0.823 | L1 0.486:  27%|█████████▋                          | 26880000/100000000 [05:12<13:35, 89669.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "2500| MSE Loss 0.823 | L1 0.486:  27%|█████████▋                          | 26880000/100000000 [05:22<13:35, 89669.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "2600| MSE Loss 0.827 | L1 0.489:  27%|█████████▋                          | 26880000/100000000 [05:24<13:35, 89669.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "2600| MSE Loss 0.827 | L1 0.489:  28%|██████████                          | 27955200/100000000 [05:24<13:24, 89519.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "2700| MSE Loss 0.787 | L1 0.483:  28%|██████████                          | 27955200/100000000 [05:37<13:24, 89519.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "2700| MSE Loss 0.787 | L1 0.483:  29%|██████████▍                         | 29030400/100000000 [05:37<13:36, 86930.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "2700| MSE Loss 0.787 | L1 0.483:  29%|██████████▍                         | 29030400/100000000 [05:47<13:36, 86930.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "2800| MSE Loss 0.758 | L1 0.477:  29%|██████████▍                         | 29030400/100000000 [05:50<13:36, 86930.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "2800| MSE Loss 0.758 | L1 0.477:  30%|██████████▊                         | 30105600/100000000 [05:50<13:45, 84631.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "2800| MSE Loss 0.758 | L1 0.477:  30%|██████████▊                         | 30105600/100000000 [06:02<13:45, 84631.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "2900| MSE Loss 0.768 | L1 0.481:  30%|██████████▊                         | 30105600/100000000 [06:04<13:45, 84631.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "2900| MSE Loss 0.768 | L1 0.481:  31%|███████████▏                        | 31180800/100000000 [06:04<13:42, 83709.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "3000| MSE Loss 0.753 | L1 0.477:  31%|███████████▏                        | 31180800/100000000 [06:17<13:42, 83709.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "3000| MSE Loss 0.753 | L1 0.477:  32%|███████████▌                        | 32256000/100000000 [06:17<13:46, 81973.37it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3000| MSE Loss 0.753 | L1 0.477:  32%|███████████▌                        | 32256000/100000000 [06:28<13:46, 81973.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "3100| MSE Loss 0.755 | L1 0.475:  32%|███████████▌                        | 32256000/100000000 [06:31<13:46, 81973.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "3100| MSE Loss 0.755 | L1 0.475:  33%|███████████▉                        | 33331200/100000000 [06:31<13:46, 80690.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "3100| MSE Loss 0.755 | L1 0.475:  33%|███████████▉                        | 33331200/100000000 [06:42<13:46, 80690.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "3200| MSE Loss 0.719 | L1 0.466:  33%|███████████▉                        | 33331200/100000000 [06:44<13:46, 80690.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "3200| MSE Loss 0.719 | L1 0.466:  34%|████████████▍                       | 34406400/100000000 [06:44<13:31, 80820.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "3300| MSE Loss 0.727 | L1 0.469:  34%|████████████▍                       | 34406400/100000000 [06:57<13:31, 80820.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "3300| MSE Loss 0.727 | L1 0.469:  35%|████████████▊                       | 35481600/100000000 [06:57<13:09, 81711.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "3300| MSE Loss 0.727 | L1 0.469:  35%|████████████▊                       | 35481600/100000000 [07:08<13:09, 81711.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "3400| MSE Loss 0.709 | L1 0.473:  35%|████████████▊                       | 35481600/100000000 [07:09<13:09, 81711.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "3400| MSE Loss 0.709 | L1 0.473:  37%|█████████████▏                      | 36556800/100000000 [07:09<12:38, 83636.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "3500| MSE Loss 0.694 | L1 0.469:  37%|█████████████▏                      | 36556800/100000000 [07:22<12:38, 83636.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "3500| MSE Loss 0.694 | L1 0.469:  38%|█████████████▌                      | 37632000/100000000 [07:22<12:21, 84149.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "3500| MSE Loss 0.694 | L1 0.469:  38%|█████████████▌                      | 37632000/100000000 [07:32<12:21, 84149.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "3600| MSE Loss 0.707 | L1 0.464:  38%|█████████████▌                      | 37632000/100000000 [07:34<12:21, 84149.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "3600| MSE Loss 0.707 | L1 0.464:  39%|█████████████▉                      | 38707200/100000000 [07:34<11:50, 86307.83it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3700| MSE Loss 0.699 | L1 0.464:  39%|█████████████▉                      | 38707200/100000000 [07:45<11:50, 86307.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "3700| MSE Loss 0.699 | L1 0.464:  40%|██████████████▎                     | 39782400/100000000 [07:45<11:26, 87701.29it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/lob29g44/40008192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3800| MSE Loss 0.665 | L1 0.459:  40%|██████████████▎                     | 39782400/100000000 [07:57<11:26, 87701.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "3800| MSE Loss 0.665 | L1 0.459:  41%|██████████████▋                     | 40857600/100000000 [07:57<11:10, 88170.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "3800| MSE Loss 0.665 | L1 0.459:  41%|██████████████▋                     | 40857600/100000000 [08:08<11:10, 88170.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "3900| MSE Loss 0.660 | L1 0.451:  41%|██████████████▋                     | 40857600/100000000 [08:09<11:10, 88170.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "3900| MSE Loss 0.660 | L1 0.451:  42%|███████████████                     | 41932800/100000000 [08:09<10:53, 88913.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "4000| MSE Loss 0.660 | L1 0.458:  42%|███████████████                     | 41932800/100000000 [08:21<10:53, 88913.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "4000| MSE Loss 0.660 | L1 0.458:  43%|███████████████▍                    | 43008000/100000000 [08:21<10:31, 90316.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "4100| MSE Loss 0.642 | L1 0.460:  43%|███████████████▍                    | 43008000/100000000 [08:32<10:31, 90316.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "4100| MSE Loss 0.642 | L1 0.460:  44%|███████████████▊                    | 44083200/100000000 [08:32<10:10, 91585.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "4100| MSE Loss 0.642 | L1 0.460:  44%|███████████████▊                    | 44083200/100000000 [08:43<10:10, 91585.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "4200| MSE Loss 0.643 | L1 0.456:  44%|███████████████▊                    | 44083200/100000000 [08:43<10:10, 91585.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "4200| MSE Loss 0.643 | L1 0.456:  45%|████████████████▎                   | 45158400/100000000 [08:43<09:46, 93546.74it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "4300| MSE Loss 0.633 | L1 0.455:  45%|████████████████▎                   | 45158400/100000000 [08:54<09:46, 93546.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "4300| MSE Loss 0.633 | L1 0.455:  46%|████████████████▋                   | 46233600/100000000 [08:54<09:32, 93915.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "4400| MSE Loss 0.619 | L1 0.454:  46%|████████████████▋                   | 46233600/100000000 [09:05<09:32, 93915.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "4400| MSE Loss 0.619 | L1 0.454:  47%|█████████████████                   | 47308800/100000000 [09:05<09:11, 95511.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "4500| MSE Loss 0.641 | L1 0.457:  47%|█████████████████                   | 47308800/100000000 [09:17<09:11, 95511.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "4500| MSE Loss 0.641 | L1 0.457:  48%|█████████████████▍                  | 48384000/100000000 [09:17<09:07, 94255.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "4500| MSE Loss 0.641 | L1 0.457:  48%|█████████████████▍                  | 48384000/100000000 [09:28<09:07, 94255.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "4600| MSE Loss 0.607 | L1 0.452:  48%|█████████████████▍                  | 48384000/100000000 [09:29<09:07, 94255.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "4600| MSE Loss 0.607 | L1 0.452:  49%|█████████████████▊                  | 49459200/100000000 [09:29<09:11, 91690.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "4700| MSE Loss 0.573 | L1 0.454:  49%|█████████████████▊                  | 49459200/100000000 [09:42<09:11, 91690.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "4700| MSE Loss 0.573 | L1 0.454:  51%|██████████████████▏                 | 50534400/100000000 [09:42<09:17, 88772.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "4700| MSE Loss 0.573 | L1 0.454:  51%|██████████████████▏                 | 50534400/100000000 [09:53<09:17, 88772.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "4800| MSE Loss 0.598 | L1 0.451:  51%|██████████████████▏                 | 50534400/100000000 [09:56<09:17, 88772.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "4800| MSE Loss 0.598 | L1 0.451:  52%|██████████████████▌                 | 51609600/100000000 [09:56<09:18, 86650.86it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "4800| MSE Loss 0.598 | L1 0.451:  52%|██████████████████▌                 | 51609600/100000000 [10:08<09:18, 86650.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "4900| MSE Loss 0.563 | L1 0.445:  52%|██████████████████▌                 | 51609600/100000000 [10:09<09:18, 86650.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "4900| MSE Loss 0.563 | L1 0.445:  53%|██████████████████▉                 | 52684800/100000000 [10:09<09:14, 85325.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "5000| MSE Loss 0.553 | L1 0.450:  53%|██████████████████▉                 | 52684800/100000000 [10:21<09:14, 85325.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "5000| MSE Loss 0.553 | L1 0.450:  54%|███████████████████▎                | 53760000/100000000 [10:21<09:03, 85045.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "5000| MSE Loss 0.553 | L1 0.450:  54%|███████████████████▎                | 53760000/100000000 [10:33<09:03, 85045.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "5100| MSE Loss 0.552 | L1 0.447:  54%|███████████████████▎                | 53760000/100000000 [10:34<09:03, 85045.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "5100| MSE Loss 0.552 | L1 0.447:  55%|███████████████████▋                | 54835200/100000000 [10:34<08:54, 84527.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "5200| MSE Loss 0.545 | L1 0.452:  55%|███████████████████▋                | 54835200/100000000 [10:47<08:54, 84527.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "5200| MSE Loss 0.545 | L1 0.452:  56%|████████████████████▏               | 55910400/100000000 [10:47<08:43, 84288.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "5200| MSE Loss 0.545 | L1 0.452:  56%|████████████████████▏               | 55910400/100000000 [10:58<08:43, 84288.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "5300| MSE Loss 0.530 | L1 0.445:  56%|████████████████████▏               | 55910400/100000000 [11:00<08:43, 84288.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "5300| MSE Loss 0.530 | L1 0.445:  57%|████████████████████▌               | 56985600/100000000 [11:00<08:33, 83775.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "5400| MSE Loss 0.570 | L1 0.452:  57%|████████████████████▌               | 56985600/100000000 [11:11<08:33, 83775.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "5400| MSE Loss 0.570 | L1 0.452:  58%|████████████████████▉               | 58060800/100000000 [11:11<07:55, 88238.90it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5500| MSE Loss 0.541 | L1 0.446:  58%|████████████████████▉               | 58060800/100000000 [11:22<07:55, 88238.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "5500| MSE Loss 0.541 | L1 0.446:  59%|█████████████████████▎              | 59136000/100000000 [11:22<07:30, 90785.33it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/lob29g44/60006912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5600| MSE Loss 0.509 | L1 0.447:  59%|█████████████████████▎              | 59136000/100000000 [11:33<07:30, 90785.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "5600| MSE Loss 0.509 | L1 0.447:  60%|█████████████████████▋              | 60211200/100000000 [11:33<07:12, 92047.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "5600| MSE Loss 0.509 | L1 0.447:  60%|█████████████████████▋              | 60211200/100000000 [11:44<07:12, 92047.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "5700| MSE Loss 0.541 | L1 0.445:  60%|█████████████████████▋              | 60211200/100000000 [11:44<07:12, 92047.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "5700| MSE Loss 0.541 | L1 0.445:  61%|██████████████████████              | 61286400/100000000 [11:44<06:55, 93080.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "5800| MSE Loss 0.522 | L1 0.443:  61%|██████████████████████              | 61286400/100000000 [11:56<06:55, 93080.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "5800| MSE Loss 0.522 | L1 0.443:  62%|██████████████████████▍             | 62361600/100000000 [11:56<06:42, 93502.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "5900| MSE Loss 0.505 | L1 0.442:  62%|██████████████████████▍             | 62361600/100000000 [12:07<06:42, 93502.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "5900| MSE Loss 0.505 | L1 0.442:  63%|██████████████████████▊             | 63436800/100000000 [12:07<06:27, 94284.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "6000| MSE Loss 0.498 | L1 0.438:  63%|██████████████████████▊             | 63436800/100000000 [12:18<06:27, 94284.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "6000| MSE Loss 0.498 | L1 0.438:  65%|███████████████████████▏            | 64512000/100000000 [12:18<06:12, 95200.79it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "6000| MSE Loss 0.498 | L1 0.438:  65%|███████████████████████▏            | 64512000/100000000 [12:28<06:12, 95200.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "6100| MSE Loss 0.505 | L1 0.443:  65%|███████████████████████▏            | 64512000/100000000 [12:29<06:12, 95200.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "6100| MSE Loss 0.505 | L1 0.443:  66%|███████████████████████▌            | 65587200/100000000 [12:29<06:00, 95391.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "6200| MSE Loss 0.514 | L1 0.445:  66%|███████████████████████▌            | 65587200/100000000 [12:40<06:00, 95391.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "6200| MSE Loss 0.514 | L1 0.445:  67%|███████████████████████▉            | 66662400/100000000 [12:40<05:47, 95867.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "6300| MSE Loss 0.494 | L1 0.446:  67%|███████████████████████▉            | 66662400/100000000 [12:51<05:47, 95867.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "6300| MSE Loss 0.494 | L1 0.446:  68%|████████████████████████▍           | 67737600/100000000 [12:51<05:33, 96645.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "6400| MSE Loss 0.496 | L1 0.444:  68%|████████████████████████▍           | 67737600/100000000 [13:03<05:33, 96645.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "6400| MSE Loss 0.496 | L1 0.444:  69%|████████████████████████▊           | 68812800/100000000 [13:03<05:28, 95000.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "6400| MSE Loss 0.496 | L1 0.444:  69%|████████████████████████▊           | 68812800/100000000 [13:14<05:28, 95000.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "6500| MSE Loss 0.489 | L1 0.440:  69%|████████████████████████▊           | 68812800/100000000 [13:15<05:28, 95000.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "6500| MSE Loss 0.489 | L1 0.440:  70%|█████████████████████████▏          | 69888000/100000000 [13:15<05:23, 93062.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "6600| MSE Loss 0.494 | L1 0.438:  70%|█████████████████████████▏          | 69888000/100000000 [13:27<05:23, 93062.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "6600| MSE Loss 0.494 | L1 0.438:  71%|█████████████████████████▌          | 70963200/100000000 [13:27<05:13, 92541.03it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "6600| MSE Loss 0.494 | L1 0.438:  71%|█████████████████████████▌          | 70963200/100000000 [13:38<05:13, 92541.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "6700| MSE Loss 0.476 | L1 0.435:  71%|█████████████████████████▌          | 70963200/100000000 [13:40<05:13, 92541.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "6700| MSE Loss 0.476 | L1 0.435:  72%|█████████████████████████▉          | 72038400/100000000 [13:40<05:12, 89582.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "6800| MSE Loss 0.473 | L1 0.434:  72%|█████████████████████████▉          | 72038400/100000000 [13:51<05:12, 89582.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "6800| MSE Loss 0.473 | L1 0.434:  73%|██████████████████████████▎         | 73113600/100000000 [13:51<04:58, 90112.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "6900| MSE Loss 0.476 | L1 0.439:  73%|██████████████████████████▎         | 73113600/100000000 [14:03<04:58, 90112.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "6900| MSE Loss 0.476 | L1 0.439:  74%|██████████████████████████▋         | 74188800/100000000 [14:03<04:45, 90532.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "6900| MSE Loss 0.476 | L1 0.439:  74%|██████████████████████████▋         | 74188800/100000000 [14:14<04:45, 90532.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "7000| MSE Loss 0.483 | L1 0.438:  74%|██████████████████████████▋         | 74188800/100000000 [14:15<04:45, 90532.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "7000| MSE Loss 0.483 | L1 0.438:  75%|███████████████████████████         | 75264000/100000000 [14:15<04:31, 91014.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "7100| MSE Loss 0.454 | L1 0.437:  75%|███████████████████████████         | 75264000/100000000 [14:27<04:31, 91014.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "7100| MSE Loss 0.454 | L1 0.437:  76%|███████████████████████████▍        | 76339200/100000000 [14:27<04:19, 91266.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "7200| MSE Loss 0.469 | L1 0.433:  76%|███████████████████████████▍        | 76339200/100000000 [14:38<04:19, 91266.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "7200| MSE Loss 0.469 | L1 0.433:  77%|███████████████████████████▊        | 77414400/100000000 [14:38<04:06, 91638.37it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "7200| MSE Loss 0.469 | L1 0.433:  77%|███████████████████████████▊        | 77414400/100000000 [14:49<04:06, 91638.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "7300| MSE Loss 0.484 | L1 0.442:  77%|███████████████████████████▊        | 77414400/100000000 [14:49<04:06, 91638.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "7300| MSE Loss 0.484 | L1 0.442:  78%|████████████████████████████▎       | 78489600/100000000 [14:49<03:51, 92914.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "7400| MSE Loss 0.455 | L1 0.434:  78%|████████████████████████████▎       | 78489600/100000000 [15:00<03:51, 92914.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "7400| MSE Loss 0.455 | L1 0.434:  80%|████████████████████████████▋       | 79564800/100000000 [15:00<03:32, 96316.98it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/lob29g44/80005632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "7500| MSE Loss 0.453 | L1 0.431:  80%|████████████████████████████▋       | 79564800/100000000 [15:11<03:32, 96316.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "7500| MSE Loss 0.453 | L1 0.431:  81%|█████████████████████████████       | 80640000/100000000 [15:11<03:22, 95834.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "7600| MSE Loss 0.467 | L1 0.436:  81%|█████████████████████████████       | 80640000/100000000 [15:22<03:22, 95834.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "7600| MSE Loss 0.467 | L1 0.436:  82%|█████████████████████████████▍      | 81715200/100000000 [15:22<03:08, 96930.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "7700| MSE Loss 0.445 | L1 0.434:  82%|█████████████████████████████▍      | 81715200/100000000 [15:33<03:08, 96930.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "7700| MSE Loss 0.445 | L1 0.434:  83%|█████████████████████████████▊      | 82790400/100000000 [15:33<02:57, 96825.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "7800| MSE Loss 0.456 | L1 0.438:  83%|█████████████████████████████▊      | 82790400/100000000 [15:44<02:57, 96825.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "7800| MSE Loss 0.456 | L1 0.438:  84%|██████████████████████████████▏     | 83865600/100000000 [15:44<02:45, 97331.33it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "7800| MSE Loss 0.456 | L1 0.438:  84%|██████████████████████████████▏     | 83865600/100000000 [15:54<02:45, 97331.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "7900| MSE Loss 0.456 | L1 0.440:  84%|██████████████████████████████▏     | 83865600/100000000 [15:55<02:45, 97331.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "7900| MSE Loss 0.456 | L1 0.440:  85%|██████████████████████████████▌     | 84940800/100000000 [15:55<02:35, 97121.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "8000| MSE Loss 0.449 | L1 0.440:  85%|██████████████████████████████▌     | 84940800/100000000 [16:06<02:35, 97121.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "8000| MSE Loss 0.449 | L1 0.440:  86%|██████████████████████████████▉     | 86016000/100000000 [16:06<02:23, 97583.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "8100| MSE Loss 0.431 | L1 0.431:  86%|██████████████████████████████▉     | 86016000/100000000 [16:16<02:23, 97583.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "8100| MSE Loss 0.431 | L1 0.431:  87%|███████████████████████████████▎    | 87091200/100000000 [16:16<02:10, 98836.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "8200| MSE Loss 0.424 | L1 0.432:  87%|███████████████████████████████▎    | 87091200/100000000 [16:28<02:10, 98836.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "8200| MSE Loss 0.424 | L1 0.432:  88%|███████████████████████████████▋    | 88166400/100000000 [16:28<02:00, 97995.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "8300| MSE Loss 0.448 | L1 0.434:  88%|███████████████████████████████▋    | 88166400/100000000 [16:39<02:00, 97995.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "8300| MSE Loss 0.448 | L1 0.434:  89%|████████████████████████████████▏   | 89241600/100000000 [16:39<01:50, 97428.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "8300| MSE Loss 0.448 | L1 0.434:  89%|████████████████████████████████▏   | 89241600/100000000 [16:49<01:50, 97428.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "8400| MSE Loss 0.431 | L1 0.434:  89%|████████████████████████████████▏   | 89241600/100000000 [16:51<01:50, 97428.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "8400| MSE Loss 0.431 | L1 0.434:  90%|████████████████████████████████▌   | 90316800/100000000 [16:51<01:43, 93839.07it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "8500| MSE Loss 0.419 | L1 0.434:  90%|████████████████████████████████▌   | 90316800/100000000 [17:04<01:43, 93839.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "8500| MSE Loss 0.419 | L1 0.434:  91%|████████████████████████████████▉   | 91392000/100000000 [17:04<01:34, 91352.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "8500| MSE Loss 0.419 | L1 0.434:  91%|████████████████████████████████▉   | 91392000/100000000 [17:15<01:34, 91352.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "8600| MSE Loss 0.398 | L1 0.427:  91%|████████████████████████████████▉   | 91392000/100000000 [17:16<01:34, 91352.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "8600| MSE Loss 0.398 | L1 0.427:  92%|█████████████████████████████████▎  | 92467200/100000000 [17:16<01:23, 90214.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "8700| MSE Loss 0.392 | L1 0.431:  92%|█████████████████████████████████▎  | 92467200/100000000 [17:28<01:23, 90214.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "8700| MSE Loss 0.392 | L1 0.431:  94%|█████████████████████████████████▋  | 93542400/100000000 [17:28<01:11, 90114.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "8700| MSE Loss 0.392 | L1 0.431:  94%|█████████████████████████████████▋  | 93542400/100000000 [17:39<01:11, 90114.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "8800| MSE Loss 0.420 | L1 0.432:  94%|█████████████████████████████████▋  | 93542400/100000000 [17:40<01:11, 90114.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "8800| MSE Loss 0.420 | L1 0.432:  95%|██████████████████████████████████  | 94617600/100000000 [17:40<00:59, 90747.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "8800| MSE Loss 0.420 | L1 0.432:  95%|██████████████████████████████████  | 94617600/100000000 [17:50<00:59, 90747.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "8900| MSE Loss 0.417 | L1 0.432:  95%|██████████████████████████████████  | 94617600/100000000 [17:52<00:59, 90747.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "8900| MSE Loss 0.417 | L1 0.432:  96%|██████████████████████████████████▍ | 95692800/100000000 [17:52<00:47, 90569.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "9000| MSE Loss 0.409 | L1 0.432:  96%|██████████████████████████████████▍ | 95692800/100000000 [18:04<00:47, 90569.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "9000| MSE Loss 0.409 | L1 0.432:  97%|██████████████████████████████████▊ | 96768000/100000000 [18:04<00:35, 90256.80it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "9000| MSE Loss 0.409 | L1 0.432:  97%|██████████████████████████████████▊ | 96768000/100000000 [18:15<00:35, 90256.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "9100| MSE Loss 0.404 | L1 0.427:  97%|██████████████████████████████████▊ | 96768000/100000000 [18:16<00:35, 90256.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "9100| MSE Loss 0.404 | L1 0.427:  98%|███████████████████████████████████▏| 97843200/100000000 [18:16<00:23, 89908.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "9200| MSE Loss 0.395 | L1 0.425:  98%|███████████████████████████████████▏| 97843200/100000000 [18:26<00:23, 89908.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "9200| MSE Loss 0.395 | L1 0.425:  99%|███████████████████████████████████▌| 98918400/100000000 [18:26<00:11, 92681.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "9300| MSE Loss 0.398 | L1 0.429:  99%|███████████████████████████████████▌| 98918400/100000000 [18:37<00:11, 92681.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "9300| MSE Loss 0.398 | L1 0.429: 100%|███████████████████████████████████▉| 99993600/100000000 [18:37<00:00, 93958.42it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/parens_sae_checkpoints/lob29g44/final_100004352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9300| MSE Loss 0.398 | L1 0.429: 100%|███████████████████████████████████▉| 99993600/100000000 [18:38<00:00, 89389.85it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181b3d44493443cebba8dba3e99b3529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.662 MB of 0.662 MB uploaded (0.009 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 1.3%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▁█▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█████████████▆█▅▆█▆▄▆█▃▅▅▇▁▄▁▃▃▂▄▃▃▃▃▃▃▄</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>▁▄▇█</td></tr><tr><td>sparsity/below_1e-5</td><td>█▄▁▂</td></tr><tr><td>sparsity/below_1e-6</td><td>█▂▁▃</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▅▁▁▁▁▅▁▁▅▁▁█▁▁▁▁▁▁▅▁▁▁▅▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▅▅▆▆▄▅▅▇█▇▇▇▇█▆▆▅▆▇▆▆▆▆▆▅▅▄▄▄▄▅▅▅▅▆▅▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.1</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>99993600</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.00622</td></tr><tr><td>losses/l1_loss</td><td>4.29483</td></tr><tr><td>losses/mse_loss</td><td>0.39798</td></tr><tr><td>losses/overall_loss</td><td>0.83368</td></tr><tr><td>metrics/explained_variance</td><td>0.96611</td></tr><tr><td>metrics/explained_variance_std</td><td>0.02908</td></tr><tr><td>metrics/l0</td><td>3.9189</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-3.88106</td></tr><tr><td>sparsity/below_1e-5</td><td>100</td></tr><tr><td>sparsity/below_1e-6</td><td>35</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>147.58594</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">256-topk-4-LR-0.0003-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/mh01kdsc' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/mh01kdsc</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240711_221648-mh01kdsc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_utils import train_sae\n",
    "\n",
    "#I need to be able to tell the SAE to ignore certain tokens during training.\n",
    "sae, store = train_sae(ll_model, sae_lens_cfg, dataset.shuffle(seed=101))#, ignore_tokens=[])#2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb01f50-1ef9-4866-9a4e-581a7f56cce8",
   "metadata": {},
   "source": [
    "# SAELens -- gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a37b51c0-f3e7-4d62-8aa8-580e15d589b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture gated\n",
      "use_ghost_grads False\n",
      "d_in 64\n",
      "wandb_project benchmark_saes\n",
      "Run name: 256-L1-0.2-LR-0.0003-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 0.02688\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 9300\n",
      "Total wandb updates: 930\n",
      "n_tokens_per_feature_sampling_window (millions): 903.168\n",
      "n_tokens_per_dead_feature_window (millions): 451.584\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.15e+07\n"
     ]
    }
   ],
   "source": [
    "from sae_utils import make_gated_sae_lens_config\n",
    "\n",
    "#seems a lot better than top-k in terms of L0 and MSE.\n",
    "sae_lens_cfg = make_gated_sae_lens_config(\n",
    "    model=ll_model, \n",
    "    hook_name=\"blocks.0.mlp.hook_post\", \n",
    "    hook_layer=0, \n",
    "    l1_coefficient=0.2,\n",
    "    l1_warm_up_steps = 0,\n",
    "    context_size=ll_model.cfg.n_ctx,\n",
    "    d_in=ll_model.cfg.d_mlp,\n",
    "    device = 'cuda',\n",
    "    checkpoint_path = f\"$HOME/persistent-storage/tracr_saes/parens_sae_checkpoints\",\n",
    "    wandb_project =  \"benchmark_saes\",\n",
    "    training_tokens = 100_000_000,\n",
    "    batch_size = 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694da2b-25b0-4984-a9a1-05e1db025b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: the training dataset contains fewer samples (97472) than the number of samples required by your training configuration (100000000). This will result in multiple training epochs and some samples being used more than once.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jmdwk0hp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.409 MB of 0.409 MB uploaded (0.004 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 2.2%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▂▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁██</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▆█</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▂▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███████</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.1</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>69350400</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.06149</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>5.01996</td></tr><tr><td>losses/mse_loss</td><td>0.05029</td></tr><tr><td>losses/overall_loss</td><td>0.61377</td></tr><tr><td>metrics/explained_variance</td><td>0.99557</td></tr><tr><td>metrics/explained_variance_std</td><td>0.0037</td></tr><tr><td>metrics/l0</td><td>20.52381</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-1.59449</td></tr><tr><td>sparsity/below_1e-5</td><td>10</td></tr><tr><td>sparsity/below_1e-6</td><td>10</td></tr><tr><td>sparsity/dead_features</td><td>8</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>161.46094</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">256-gated-L1-0.1-LR-0.0003-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/jmdwk0hp' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/jmdwk0hp</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 11 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240711_230207-jmdwk0hp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jmdwk0hp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b2673a6d8d49968119cc5f5500e8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113975362645256, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/quick-experiments/wandb/run-20240711_231450-qpswq6wq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evanhanders/benchmark_saes/runs/qpswq6wq' target=\"_blank\">256-gated-L1-0.2-LR-0.0003-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/qpswq6wq' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/qpswq6wq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SAE:   0%|                                                                         | 0/100000000 [00:00<?, ?it/s]\u001b[A/opt/venv/lib/python3.10/site-packages/sae_lens/training/activations_store.py:264: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "\n",
      "100| MSE Loss 2.448 | L1 4.634:   0%|                                                       | 0/100000000 [00:10<?, ?it/s]\u001b[A\n",
      "100| MSE Loss 2.448 | L1 4.634:   1%|▍                                    | 1075200/100000000 [00:10<15:49, 104151.21it/s]\u001b[A\n",
      "200| MSE Loss 0.694 | L1 2.562:   1%|▍                                    | 1075200/100000000 [00:21<15:49, 104151.21it/s]\u001b[A\n",
      "200| MSE Loss 0.694 | L1 2.562:   2%|▊                                    | 2150400/100000000 [00:21<16:07, 101136.19it/s]\u001b[A\n",
      "300| MSE Loss 0.416 | L1 2.088:   2%|▊                                    | 2150400/100000000 [00:31<16:07, 101136.19it/s]\u001b[A\n",
      "300| MSE Loss 0.416 | L1 2.088:   3%|█▏                                   | 3225600/100000000 [00:31<15:58, 100922.02it/s]\u001b[A\n",
      "400| MSE Loss 0.311 | L1 1.833:   3%|█▏                                   | 3225600/100000000 [00:43<15:58, 100922.02it/s]\u001b[A\n",
      "400| MSE Loss 0.311 | L1 1.833:   4%|█▋                                    | 4300800/100000000 [00:43<16:05, 99086.69it/s]\u001b[A\n",
      "400| MSE Loss 0.311 | L1 1.833:   4%|█▋                                    | 4300800/100000000 [00:54<16:05, 99086.69it/s]\u001b[A\n",
      "500| MSE Loss 0.262 | L1 1.676:   4%|█▋                                    | 4300800/100000000 [00:54<16:05, 99086.69it/s]\u001b[A\n",
      "500| MSE Loss 0.262 | L1 1.676:   5%|██                                    | 5376000/100000000 [00:54<16:24, 96121.05it/s]\u001b[A\n",
      "600| MSE Loss 0.224 | L1 1.554:   5%|██                                    | 5376000/100000000 [01:06<16:24, 96121.05it/s]\u001b[A\n",
      "600| MSE Loss 0.224 | L1 1.554:   6%|██▍                                   | 6451200/100000000 [01:06<16:37, 93823.92it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "700| MSE Loss 0.197 | L1 1.472:   6%|██▍                                   | 6451200/100000000 [01:18<16:37, 93823.92it/s]\u001b[A\n",
      "700| MSE Loss 0.197 | L1 1.472:   8%|██▊                                   | 7526400/100000000 [01:18<16:37, 92669.41it/s]\u001b[A\n",
      "700| MSE Loss 0.197 | L1 1.472:   8%|██▊                                   | 7526400/100000000 [01:29<16:37, 92669.41it/s]\u001b[A\n",
      "800| MSE Loss 0.186 | L1 1.409:   8%|██▊                                   | 7526400/100000000 [01:30<16:37, 92669.41it/s]\u001b[A\n",
      "800| MSE Loss 0.186 | L1 1.409:   9%|███▎                                  | 8601600/100000000 [01:30<16:36, 91726.82it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from sae_utils import train_sae\n",
    "\n",
    "sae, store = train_sae(ll_model, sae_lens_cfg, dataset.shuffle(seed=101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1ce2f-d424-4776-af4e-bc5b0f7880d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
