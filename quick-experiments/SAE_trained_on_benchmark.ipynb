{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjaAr6kxJ7r9"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "InFxPfHh7Shf",
    "outputId": "76b5529f-d9b2-4feb-ebbc-88d481b01baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'InterpBench'...\n",
      "remote: Enumerating objects: 225, done.\u001b[K\n",
      "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
      "remote: Compressing objects: 100% (205/205), done.\u001b[K\n",
      "remote: Total 225 (delta 71), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
      "Receiving objects: 100% (225/225), 385.53 KiB | 3.04 MiB/s, done.\n",
      "Resolving deltas: 100% (71/71), done.\n",
      "Filtering content: 100% (55/55), 82.13 MiB | 35.62 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/cybershiptrooper/InterpBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG6rfeq2J-FR"
   },
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OP0i3Tse7yEh",
    "outputId": "da12e5fc-f122-4d39-85f8-482c77748e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ActivationsStore', 'Adam', 'Any', 'FINETUNING_PARAMETERS', 'HookedRootModule', 'L1Scheduler', 'LanguageModelSAERunnerConfig', 'SAETrainer', 'TrainSAEOutput', 'TrainStepOutput', 'TrainingSAE', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__version__', '_log_feature_sparsity', '_update_sae_lens_training_version', 'cast', 'contextlib', 'dataclass', 'get_lr_scheduler', 'run_evals', 'torch', 'tqdm', 'wandb']\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "from transformer_lens import HookedTransformer\n",
    "from circuits_benchmark.transformers.hooked_tracr_transformer import HookedTracrTransformer\n",
    "\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from functools import partial\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "from sae_lens import SAEConfig, SAE, TrainingSAEConfig, TrainingSAE, ActivationsStore, CacheActivationsRunnerConfig, LanguageModelSAERunnerConfig\n",
    "from sae_lens.training.sae_trainer import SAETrainer\n",
    "from sae_utils import make_gated_sae_lens_config, train_sae\n",
    "\n",
    "import sae_lens\n",
    "print(dir(sae_lens.training.sae_trainer))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlB0DKXDt_V6"
   },
   "source": [
    "# Load and configure benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OtOpy8z6ALGU"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'iit.model_pairs.ll_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcircuits_benchmark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcases\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcase_3\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcase_3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m task \u001b[38;5;241m=\u001b[39m case_3\u001b[38;5;241m.\u001b[39mCase3()\n\u001b[1;32m      4\u001b[0m hl_model \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mget_hl_model()\n",
      "File \u001b[0;32m/workspace/circuits-benchmark/circuits_benchmark/benchmark/cases/case_3.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuits_benchmark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocabs\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuits_benchmark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_programs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_frac_prevs\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuits_benchmark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtracr_benchmark_case\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TracrBenchmarkCase\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtracr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrasp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rasp\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCase3\u001b[39;00m(TracrBenchmarkCase):\n",
      "File \u001b[0;32m/workspace/circuits-benchmark/circuits_benchmark/benchmark/tracr_benchmark_case.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01miit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_pairs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model_pair\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelPair\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01miit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_pairs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mll_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLModel\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01miit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorrespondence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Correspondence\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaxtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Float\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'iit.model_pairs.ll_model'"
     ]
    }
   ],
   "source": [
    "import circuits_benchmark.benchmark.cases.case_3 as case_3\n",
    "\n",
    "task = case_3.Case3()\n",
    "hl_model = task.get_hl_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation tokens is handled in {hook_embed}\n",
      "Operation indices is handled in {blocks.1.attn.hook_k[0], blocks.1.attn.hook_q[0], hook_pos_embed}\n",
      "Operation is_x_3 is handled in {blocks.0.hook_mlp_in, blocks.0.hook_mlp_out}\n",
      "Operation frac_prevs_1 is handled in {blocks.1.hook_v_input[0], blocks.1.attn.hook_v[0], blocks.1.attn.hook_result[0]}\n",
      "Operation frac_prevs_1 is handled in {blocks.1.hook_resid_post}\n",
      "Operation select_2 is handled in {blocks.1.attn.hook_q[0], blocks.1.attn.hook_k[0]}\n"
     ]
    }
   ],
   "source": [
    "from circuits_benchmark.transformers.tracr_circuits_builder import build_tracr_circuits\n",
    "tracr_output = task.get_tracr_output()\n",
    "tracr_circuits = build_tracr_circuits(tracr_output.graph, tracr_output.craft_model, granularity=\"acdc_hooks\")\n",
    "for k, item in tracr_circuits.alignment.hl_to_ll_mapping.items():\n",
    "    print(f'Operation {k} is handled in {item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ytz3DejTAMwj",
    "outputId": "5f4a2b56-8149-4a0b-9ecb-7434dcd54401"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_idx = 3\n",
    "dir_name = f\"InterpBench/{task_idx}\"\n",
    "cfg_dict = pickle.load(open(f\"{dir_name}/ll_model_cfg.pkl\", \"rb\"))\n",
    "cfg = HookedTransformerConfig.from_dict(cfg_dict)\n",
    "model = HookedTransformer(cfg)\n",
    "weights = torch.load(f\"{dir_name}/ll_model.pth\")\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 320\n",
      "[['BOS', 'a', 'a', 'a', 'PAD'], ['BOS', 'a', 'a', 'b', 'PAD'], ['BOS', 'a', 'a', 'c', 'PAD'], ['BOS', 'a', 'a', 'x', 'PAD']]\n",
      "[['BOS', 0.0, 0.0, 0.0, 'PAD'], ['BOS', 0.0, 0.0, 0.0, 'PAD'], ['BOS', 0.0, 0.0, 0.0, 'PAD'], ['BOS', 0.0, 0.0, 0.3333333333333333, 'PAD']]\n"
     ]
    }
   ],
   "source": [
    "total_data = task.get_total_data_len()\n",
    "input_data, output_data = task.gen_all_data(task.get_min_seq_len(), task.get_max_seq_len())\n",
    "print(len(input_data), len(output_data))\n",
    "print(input_data[:4])\n",
    "print(output_data[:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DT6AJSAdARcn",
    "outputId": "fd97e71f-cbe0-40af-972c-b4eb2bc7de60"
   },
   "outputs": [],
   "source": [
    "# load high level model\n",
    "# import circuits_benchmark.utils.iit.correspondence as correspondence\n",
    "# from circuits_benchmark.utils.iit.dataset import get_unique_data\n",
    "# import iit.model_pairs as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXw8nhAeSzCg",
    "outputId": "e5b5b5d7-c7d7-4803-a311-520ae55b8d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 2, 2, 1],\n",
      "        [0, 2, 2, 3, 1],\n",
      "        [0, 2, 2, 4, 1],\n",
      "        ...,\n",
      "        [0, 5, 5, 5, 3],\n",
      "        [0, 5, 5, 5, 4],\n",
      "        [0, 5, 5, 5, 5]])\n",
      "Dataset({\n",
      "    features: ['string_tokens', 'tokens', 'labels'],\n",
      "    num_rows: 320\n",
      "})\n",
      "{'string_tokens': ['BOS', 'a', 'a', 'x', 'PAD'], 'tokens': [0, 2, 2, 5, 1], 'labels': \"['BOS', 0.0, 0.0, 0.3333333333333333, 'PAD']\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "# Create dataset of case inputs\n",
    "# dataset = get_unique_data(task, max_len=10_000)\n",
    "tokenized_data = hl_model.map_tracr_input_to_tl_input(input_data)\n",
    "print(tokenized_data)\n",
    "\n",
    "# Convert PyTorch tensors to lists\n",
    "string_tokens_list = input_data\n",
    "tokens_list = tokenized_data.tolist()\n",
    "labels_list = [str(label) for label in output_data]\n",
    "\n",
    "# Create a dictionary from the lists\n",
    "data_dict = {\n",
    "    \"string_tokens\": string_tokens_list,\n",
    "    \"tokens\": tokens_list,\n",
    "    \"labels\": labels_list\n",
    "}\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "hf_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "print(hf_dataset)\n",
    "print(hf_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KItWVrlgfx9v",
    "outputId": "5973470a-d801-48c8-9c68-c26e0b063744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 3)\n"
     ]
    }
   ],
   "source": [
    "print(hf_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfEZgq7_bP08",
    "outputId": "fb85194c-5196-461c-a60c-8253d5b24118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [0, 2, 3, 4, 5]\n",
      "Decoded: BOS a b c x\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "# Define your simple vocabulary\n",
    "vocab = {'BOS': 0, 'UNK': 1, 'a': 2, 'b': 3, 'c': 4, 'x': 5}\n",
    "# comes from task.get_vocab() and hl_model.map_tracr_input_to_tl_input\n",
    "\n",
    "# Create a Tokenizer with a WordLevel model\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"UNK\"))\n",
    "\n",
    "# Set the normalizer, pre-tokenizer, and decoder\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Convert to Hugging Face tokenizer\n",
    "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "# Add the special tokens to the Hugging Face tokenizer\n",
    "hf_tokenizer.add_special_tokens({\n",
    "    'unk_token': 'UNK',\n",
    "    'bos_token': 'BOS',\n",
    "    'cls_token': '[CLS]',\n",
    "    'sep_token': '[SEP]',\n",
    "    'pad_token': '[PAD]',\n",
    "    'mask_token': '[MASK]'\n",
    "})\n",
    "\n",
    "# Test the tokenizer\n",
    "encoded = hf_tokenizer.encode(\"BOS a b c x\")\n",
    "decoded = hf_tokenizer.decode(encoded)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oShVxNlpvfyn"
   },
   "outputs": [],
   "source": [
    "model.tokenizer = hf_tokenizer #attach to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eilYaeGcLuMY",
    "outputId": "e3ae7db7-3487-4411-fe62-52bfe94b78fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.2500]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.0000]]], device='cuda:0', grad_fn=<SliceBackward0>) [['BOS', 0.0, 0.0, 0.0, 'PAD'], ['BOS', 0.0, 0.0, 0.0, 'PAD'], ['BOS', 0.0, 0.0, 0.0, 'PAD'], ['BOS', 0.0, 0.0, 0.3333333333333333, 'PAD'], ['BOS', 0.0, 0.0, 0.0, 'PAD']]\n"
     ]
    }
   ],
   "source": [
    "_, cache = model.run_with_cache(tokenized_data)\n",
    "output = hl_model(tokenized_data) #TODO: why are these different calls?\n",
    "print(output[:5], output_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w16J8LtAO4WU",
    "outputId": "817b52ed-d62d-4a0f-ef8f-d5bb159480c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.hook_q_input', 'blocks.0.hook_k_input', 'blocks.0.hook_v_input', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.attn.hook_result', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.hook_mlp_in', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.hook_q_input', 'blocks.1.hook_k_input', 'blocks.1.hook_v_input', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.attn.hook_result', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.hook_mlp_in', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post']\n"
     ]
    }
   ],
   "source": [
    "print(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu8iSnvHNG38"
   },
   "source": [
    "# SAE-lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NTJJyQtOc4mG"
   },
   "outputs": [],
   "source": [
    "class RepeatActivationsStore(ActivationsStore):\n",
    "\n",
    "    def get_batch_tokens(self, batch_size: int | None = None):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        if not batch_size:\n",
    "            batch_size = self.store_batch_size_prompts\n",
    "        sequences = []\n",
    "        # the sequences iterator yields fully formed tokens of size context_size, so we just need to cat these into a batch\n",
    "        for _ in range(batch_size):\n",
    "            try:\n",
    "                sequences.append(next(self.iterable_sequences))\n",
    "            except StopIteration:\n",
    "                #shuffle self.dataset and restart\n",
    "                self.iterable_sequences = self._iterate_tokenized_sequences()\n",
    "                sequences.append(next(self.iterable_sequences))\n",
    "                # self.iterable_dataset = iter(self.dataset)\n",
    "                # s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            \n",
    "        return torch.stack(sequences, dim=0).to(self.model.W_E.device)\n",
    "    \n",
    "    def _get_next_dataset_tokens(self) -> torch.Tensor:\n",
    "        device = self.device\n",
    "        if not self.is_dataset_tokenized:\n",
    "            try:\n",
    "                s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            except StopIteration:\n",
    "                #shuffle self.dataset and restart\n",
    "                self.iterable_dataset = iter(self.dataset)\n",
    "                s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            tokens = (\n",
    "                self.model.to_tokens(\n",
    "                    s,\n",
    "                    truncate=False,\n",
    "                    move_to_device=True,\n",
    "                    prepend_bos=self.prepend_bos,\n",
    "                )\n",
    "                .squeeze(0)\n",
    "                .to(device)\n",
    "            )\n",
    "            assert (\n",
    "                len(tokens.shape) == 1\n",
    "            ), f\"tokens.shape should be 1D but was {tokens.shape}\"\n",
    "        else:\n",
    "            try:\n",
    "                s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            except StopIteration:\n",
    "                #shuffle self.dataset and restart\n",
    "                self.iterable_dataset = iter(self.dataset)\n",
    "                s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            tokens = torch.tensor(\n",
    "                s,\n",
    "                dtype=torch.long,\n",
    "                device=device,\n",
    "                requires_grad=False,\n",
    "            )\n",
    "            if (\n",
    "                not self.prepend_bos\n",
    "                and tokens[0] == self.model.tokenizer.bos_token_id  # type: ignore\n",
    "            ):\n",
    "                tokens = tokens[1:]\n",
    "        self.n_dataset_processed += 1\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cltcYcNv6Sb2"
   },
   "source": [
    "## Residual stream 0 -- tokens and position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhrGtjCBNZM7",
    "outputId": "01fb8c7d-491b-474e-c479-395a3065d72e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 48-L1-0.1-LR-0.0003-Tokens-1.500e+06\n",
      "n_tokens_per_buffer (millions): 0.0004\n",
      "Lower bound: n_contexts_per_buffer (millions): 8e-05\n",
      "Total training steps: 18750\n",
      "Total wandb updates: 1875\n",
      "n_tokens_per_feature_sampling_window (millions): 0.8\n",
      "n_tokens_per_dead_feature_window (millions): 0.4\n",
      "We will reset the sparsity calculation 9 times.\n",
      "Number tokens in sparsity calculation window: 1.60e+05\n",
      "Using Ghost Grads.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_sae_lens_config(hook_name: str, hook_layer: int, l1_coeff: float, training_tokens: int = 1_500_000):\n",
    "    return LanguageModelSAERunnerConfig(\n",
    "        # Data Generating Function (Model + Training Distribution)\n",
    "        model_name = \"case3\",\n",
    "        model_class_name = \"HookedTransformer\",\n",
    "        hook_name = hook_name,\n",
    "        hook_eval = \"NOT_IN_USE\",\n",
    "        hook_layer = hook_layer,\n",
    "        hook_head_index = None,\n",
    "        dataset_path = \"\",\n",
    "        dataset_trust_remote_code = False,\n",
    "        streaming = False,\n",
    "        is_dataset_tokenized = True,\n",
    "        context_size = 5,\n",
    "        use_cached_activations = False,\n",
    "        cached_activations_path = None,  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_head_index}\"\n",
    "    \n",
    "        # SAE Parameters\n",
    "        d_in = model.cfg.d_model,\n",
    "        d_sae = None,\n",
    "        b_dec_init_method = \"geometric_median\",\n",
    "        expansion_factor = 4,\n",
    "        activation_fn = \"relu\",  # relu, tanh-relu\n",
    "        normalize_sae_decoder = True,\n",
    "        noise_scale = 0.0,\n",
    "        from_pretrained_path = None,\n",
    "        apply_b_dec_to_input = False,\n",
    "        decoder_orthogonal_init = False,\n",
    "        decoder_heuristic_init = False,\n",
    "        init_encoder_as_decoder_transpose = False,\n",
    "    \n",
    "        # Activation Store Parameters\n",
    "        training_tokens = training_tokens,\n",
    "        finetuning_tokens = 0,\n",
    "        store_batch_size_prompts = 4,\n",
    "        normalize_activations = \"none\",  # none, expected_average_only_in (Anthropic April Update), constant_norm_rescale (Anthropic Feb Update)\n",
    "    \n",
    "        # Misc\n",
    "        device = device,\n",
    "        act_store_device = \"with_model\",  # will be set by post init if with_model\n",
    "        seed = 42,\n",
    "        dtype = \"float32\",  # type: ignore #\n",
    "        prepend_bos = False,\n",
    "    \n",
    "        # Performance - see compilation section of lm_runner.py for info\n",
    "        autocast = False,  # autocast to autocast_dtype during training\n",
    "        autocast_lm = False,  # autocast lm during activation fetching\n",
    "        compile_llm = False,  # use torch.compile on the LLM\n",
    "        llm_compilation_mode = None,  # which torch.compile mode to use\n",
    "        compile_sae = False,  # use torch.compile on the SAE\n",
    "        sae_compilation_mode = None,\n",
    "    \n",
    "        # Training Parameters\n",
    "    \n",
    "        ## Batch size\n",
    "        train_batch_size_tokens = 320//4,\n",
    "    \n",
    "        ## Adam\n",
    "        adam_beta1 = 0.9,\n",
    "        adam_beta2 = 0.999,\n",
    "    \n",
    "        ## Loss Function\n",
    "        mse_loss_normalization = None,\n",
    "        l1_coefficient = l1_coeff,\n",
    "        lp_norm = 1,\n",
    "        scale_sparsity_penalty_by_decoder_norm = False,\n",
    "        l1_warm_up_steps = 0,\n",
    "    \n",
    "        ## Learning Rate Schedule\n",
    "        lr = 3e-4,\n",
    "        lr_scheduler_name = \"constant\",  # constant, cosineannealing, cosineannealingwarmrestarts\n",
    "        lr_warm_up_steps = 0,\n",
    "        lr_end = None,  # only used for cosine annealing, default is lr / 10\n",
    "        lr_decay_steps = 0,\n",
    "        n_restart_cycles = 1,  # used only for cosineannealingwarmrestarts\n",
    "    \n",
    "        ## FineTuning\n",
    "        finetuning_method = None,  # scale, decoder or unrotated_decoder\n",
    "    \n",
    "        # Resampling protocol args\n",
    "        use_ghost_grads = True,  # want to change this to true on some timeline.\n",
    "        feature_sampling_window = 2000,\n",
    "        dead_feature_window = 1000,  # unless this window is larger feature sampling,\n",
    "        dead_feature_threshold = 1e-8,\n",
    "    \n",
    "        # Evals\n",
    "        n_eval_batches = 10,\n",
    "        eval_batch_size_prompts = None,  # useful if evals cause OOM\n",
    "    \n",
    "        # WANDB\n",
    "        log_to_wandb = True,\n",
    "        log_activations_store_to_wandb = False,\n",
    "        log_optimizer_state_to_wandb = False,\n",
    "        wandb_project = \"benchmark_saes\",\n",
    "        wandb_id = None,\n",
    "        run_name = None,\n",
    "        wandb_entity = None,\n",
    "        wandb_log_frequency = 10,\n",
    "        eval_every_n_wandb_logs = 100000000000, # Make this a really big number; currently fails because it tries to compute CE loss.\n",
    "        # Misc\n",
    "        resume = False,\n",
    "        n_checkpoints = 5,\n",
    "        checkpoint_path = f\"$HOME/persistent-storage/tracr_saes/task_{task_idx}_checkpoints\",\n",
    "        verbose = True,\n",
    "        model_kwargs = dict(),\n",
    "        model_from_pretrained_kwargs = dict(),\n",
    "        sae_lens_version = str(sae_lens.__version__),\n",
    "        sae_lens_training_version = str(sae_lens.__version__),\n",
    "    )\n",
    "\n",
    "runner_cfg = make_sae_lens_config( \"blocks.0.hook_resid_pre\", 0, l1_coeff=1e-1, training_tokens=1_500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814,
     "referenced_widgets": [
      "0f6a54a5242d4963959813217e192323",
      "3181d5a610e44d16a62010de88e34ea0",
      "a7a3ecba6fe84610b9d1626cd05f25df",
      "ed20c3fa79784d91870a84a098f04d33",
      "ccbc8b30e53545cfb311f2e6d49da4fc",
      "b7e63627055a42fd89d6e3dd2bfdfe42",
      "cdbfe0ae850443b1978bed6d0d7f2bbe",
      "bd9587b34348441da608ea83783adb63"
     ]
    },
    "id": "AMAR0c3xgL-F",
    "outputId": "b9d8cd42-b442-4693-a208-4e69157a6952",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.tokenizer = hf_tokenizer\n",
    "store = RepeatActivationsStore.from_config(model, runner_cfg, dataset=hf_dataset)\n",
    "sae = TrainingSAE(runner_cfg)\n",
    "trainer = SAETrainer(model, sae, store, save_checkpoint, cfg = runner_cfg)\n",
    "\n",
    "if runner_cfg.log_to_wandb:\n",
    "    wandb.init(\n",
    "        project=runner_cfg.wandb_project,\n",
    "        config=runner_cfg,\n",
    "        name=runner_cfg.run_name,\n",
    "        id=runner_cfg.wandb_id,\n",
    "    )\n",
    "trainer.fit()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(activations, k=20):\n",
    "    # Reshape\n",
    "    batch, ctx, feat = activations.shape\n",
    "    reshaped = activations.view(batch * ctx, feat)\n",
    "    \n",
    "    # Get the top-k samples and their indices\n",
    "    top_samples = torch.topk(reshaped, dim=0, k=k)\n",
    "    top_values = top_samples.values\n",
    "    top_indices = top_samples.indices\n",
    "    \n",
    "    # print(top_indices)  # Print the shape of the top-k values\n",
    "    \n",
    "    # Compute the original batch and ctx positions\n",
    "    original_batch_indices = top_indices // ctx\n",
    "    original_ctx_indices = top_indices % ctx\n",
    "    \n",
    "    return top_values.cpu(), original_batch_indices.cpu(), original_ctx_indices.cpu()\n",
    "\n",
    "\n",
    "# Function to update the DataFrame based on the selected feature\n",
    "def update_dataframe(feat, top_values, batch_indices, ctx_indices, live_features):\n",
    "    feat = live_features[feat]\n",
    "    k = 20\n",
    "    \n",
    "    print(f'feature {feat}')\n",
    "    \n",
    "    activations = top_values[:, feat]\n",
    "    best_tokens = tokens[batch_indices[:, feat]]\n",
    "    active_idx = ctx_indices[:, feat]\n",
    "    active_token = best_tokens[range(k), active_idx]\n",
    "    active_tokens = hf_tokenizer.decode(active_token)\n",
    "    full_strs = [hf_tokenizer.decode(t) for t in best_tokens]\n",
    "    \n",
    "    info = [\n",
    "        {\n",
    "            'tokens': toks,\n",
    "            'activation': act.item(),\n",
    "            'tok_idx': idx.item(),\n",
    "        }\n",
    "        for act, idx, toks in zip(activations, active_idx, full_strs)\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(info)\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.Tensor(hf_dataset['tokens']).to(int)\n",
    "logits, cache = model.run_with_cache(tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top values shape: torch.Size([20, 48])\n",
      "Batch indices shape: torch.Size([20, 48])\n",
      "Context indices shape: torch.Size([20, 48])\n",
      "torch.Size([320, 5, 48])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "activations = sae.encode(cache[sae.cfg.hook_name])\n",
    "top_values, batch_indices, ctx_indices = get_top_k(activations)\n",
    "print(\"Top values shape:\", top_values.shape)\n",
    "print(\"Batch indices shape:\", batch_indices.shape)\n",
    "print(\"Context indices shape:\", ctx_indices.shape)\n",
    "\n",
    "print(activations.shape)\n",
    "# print(batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a2dc2fa9344b358f07cfe6bdea1fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Feature:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3c5efd81b2412a9bdc1f2af4e06947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "live_features = top_values[0,:].nonzero().flatten()\n",
    "print(live_features.shape[0])\n",
    "\n",
    "# Define the dropdown menu for 'feat'\n",
    "feat_dropdown = widgets.Dropdown(\n",
    "    options=range(live_features.shape[0]),\n",
    "    value=0,\n",
    "    description='Feature:',\n",
    ")\n",
    "\n",
    "\n",
    "dataframe_func = partial(\n",
    "    update_dataframe, \n",
    "    top_values=top_values, \n",
    "    batch_indices=batch_indices, \n",
    "    ctx_indices=ctx_indices,\n",
    "    live_features=live_features\n",
    ")\n",
    "\n",
    "# Create an interactive output widget\n",
    "output = widgets.interactive_output(\n",
    "    dataframe_func, \n",
    "    {\n",
    "        'feat': feat_dropdown,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the dropdown menu and output\n",
    "display(feat_dropdown, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model https://wandb.ai/evanhanders/benchmark_saes/artifacts/model/sae_case3_blocks.0.hook_resid_pre_48/v5\n",
    "\n",
    "* Feature 0: x at idx=1\n",
    "* 2: x at idx=1\n",
    "* 3: UNK at idx=4\n",
    "* 4: a at idx=3\n",
    "* 6: BOS\n",
    "* 7: c at idx=4\n",
    "* 14: x at idx=3\n",
    "* 15: UNK at idx=4\n",
    "* 16: b at idx=2\n",
    "* 17: x at idx=4\n",
    "* 23: b at idx=3\n",
    "* 26: x at idx=3\n",
    "* 28: x at idx=2\n",
    "* 30: c at idx=3\n",
    "* 35: a at idx=1\n",
    "* 38: c at idx=2\n",
    "* 39: x at idx=4\n",
    "* 41: x at idx=1\n",
    "* 42: x at idx=2\n",
    "* 46: b at idx=4\n",
    "* 47: BOS\n",
    "\n",
    "There are a lot of double-features:\n",
    "* 2 & 41 (x @ 1)\n",
    "* 3 & 15 (UNK @ 4)\n",
    "* 6 & 47 (BOS)\n",
    "* 14 & 26 (x @ 3)\n",
    "* 17 & 39 (x @ 4)\n",
    "* 28 & 42 (x @ 2)\n",
    "\n",
    "Then there are these single features, ordered by letter:\n",
    "* 35: a @ 1\n",
    "* 16: b @ 2\n",
    "* 23: b @ 3\n",
    "* 46: b @ 4\n",
    "* 38: c @ 2\n",
    "* 30: c @ 3\n",
    "* 7: c @ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8pv-bJG6hoS"
   },
   "source": [
    "## Block 0 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE9ua53A6hoc",
    "outputId": "160fa32b-6f72-4b48-f4cd-c37fef9ae281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 48-L1-0.1-LR-0.0003-Tokens-1.500e+06\n",
      "n_tokens_per_buffer (millions): 0.0004\n",
      "Lower bound: n_contexts_per_buffer (millions): 8e-05\n",
      "Total training steps: 18750\n",
      "Total wandb updates: 1875\n",
      "n_tokens_per_feature_sampling_window (millions): 0.8\n",
      "n_tokens_per_dead_feature_window (millions): 0.4\n",
      "We will reset the sparsity calculation 9 times.\n",
      "Number tokens in sparsity calculation window: 1.60e+05\n",
      "Using Ghost Grads.\n"
     ]
    }
   ],
   "source": [
    "hook_name = \"blocks.1.hook_attn_out\"\n",
    "layer = 1\n",
    "runner_cfg = make_sae_lens_config( hook_name, layer, l1_coeff=1e-1, training_tokens=1_500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 744,
     "referenced_widgets": [
      "21a5279ec677451ba58792e4854a2616",
      "8a20bc7d13734632abf89aba5de29b78",
      "8f8c44017f6c4e48b4afab8c576bb969",
      "777cdae240644161879a2bc28a8fa1f8",
      "8c5ac6c09c6548f8bc2371707b67ad26",
      "8a1aee4c93d54e8bb96ec57425b69fc2",
      "e8b78e2de79b4174977cdd22187942ba",
      "9c5bdaaefca648ae92a6ea23c36b6304"
     ]
    },
    "id": "LoziykTq6hoc",
    "outputId": "585b9f5e-eb8a-4f42-885c-beb09de1d96c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/quick-experiments/wandb/run-20240701_213223-7i19nlpb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evanhanders/benchmark_saes/runs/7i19nlpb' target=\"_blank\">48-L1-0.1-LR-0.0003-Tokens-1.500e+06</a></strong> to <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/7i19nlpb' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/7i19nlpb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE:   0%|                                                                           | 0/1500000 [00:00<?, ?it/s]/opt/venv/lib/python3.10/site-packages/sae_lens/training/activations_store.py:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "3700| MSE Loss 0.007 | L1 0.150:  20%|████████                                 | 296000/1500000 [01:06<04:32, 4414.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/xfae89eo/300080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7500| MSE Loss 0.004 | L1 0.124:  40%|████████████████▍                        | 600000/1500000 [02:14<03:19, 4520.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/xfae89eo/600080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11200| MSE Loss 0.005 | L1 0.180:  60%|███████████████████████▉                | 896000/1500000 [03:21<02:18, 4354.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/xfae89eo/900080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000| MSE Loss 0.006 | L1 0.175:  80%|███████████████████████████████▏       | 1200000/1500000 [04:28<01:05, 4595.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/xfae89eo/1200080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18700| MSE Loss 0.003 | L1 0.143: 100%|██████████████████████████████████████▉| 1496000/1500000 [05:36<00:00, 4512.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/xfae89eo/final_1500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18700| MSE Loss 0.003 | L1 0.143: 100%|██████████████████████████████████████▉| 1496000/1500000 [05:37<00:00, 4434.40it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.050 MB of 0.050 MB uploaded (0.009 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 13.8%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▄▂▃▂▂▂▂▂▁▂▂▂▁▂▁▁▂▁▁▂▂▁▂▂▂▂▂▁▂▂▁▂▂▂▂▂▁▂▂</td></tr><tr><td>losses/mse_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁█████▇█████████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▂▁▂▂▁▂▂▂▁▁▂▂▁▂▁▁▁▁▁▂▁▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▅▃▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▁▁▂▃▃▃▃▄</td></tr><tr><td>sparsity/below_1e-5</td><td>▁█▆▅▄▃▁▂▂</td></tr><tr><td>sparsity/below_1e-6</td><td>▁█▇▄▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▃▄▆▇▆▅▅▆█▅▅▄▄▃▄▂▁▂▁▁▂▁▂▂▁▁▂▂▂▂▃▁▂▂▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▂▃▄▅▆▆▇▆██▇█▇▇▇▇▄▄▄▃▄▄▄▃▄▃▃▃▃▃▃▃▃▃▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.1</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>1500000</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.00034</td></tr><tr><td>losses/l1_loss</td><td>1.42284</td></tr><tr><td>losses/mse_loss</td><td>0.00425</td></tr><tr><td>losses/overall_loss</td><td>0.14687</td></tr><tr><td>metrics/explained_variance</td><td>0.99573</td></tr><tr><td>metrics/explained_variance_std</td><td>0.00645</td></tr><tr><td>metrics/l0</td><td>3.975</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-2.94515</td></tr><tr><td>sparsity/below_1e-5</td><td>1</td></tr><tr><td>sparsity/below_1e-6</td><td>0</td></tr><tr><td>sparsity/dead_features</td><td>1</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>315.5625</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">48-L1-0.1-LR-0.0003-Tokens-1.500e+06</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/7i19nlpb' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/7i19nlpb</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240701_213223-7i19nlpb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.tokenizer = hf_tokenizer\n",
    "store = RepeatActivationsStore.from_config(model, runner_cfg, dataset=hf_dataset)\n",
    "sae = TrainingSAE(runner_cfg)\n",
    "trainer = SAETrainer(model, sae, store, save_checkpoint, cfg = runner_cfg)\n",
    "\n",
    "if runner_cfg.log_to_wandb:\n",
    "    wandb.init(\n",
    "        project=runner_cfg.wandb_project,\n",
    "        config=runner_cfg,\n",
    "        name=runner_cfg.run_name,\n",
    "        id=runner_cfg.wandb_id,\n",
    "    )\n",
    "trainer.fit()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top values shape: torch.Size([20, 48])\n",
      "Batch indices shape: torch.Size([20, 48])\n",
      "Context indices shape: torch.Size([20, 48])\n",
      "torch.Size([320, 5, 48])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "activations = sae.encode(cache[sae.cfg.hook_name])\n",
    "top_values, batch_indices, ctx_indices = get_top_k(activations)\n",
    "print(\"Top values shape:\", top_values.shape)\n",
    "print(\"Batch indices shape:\", batch_indices.shape)\n",
    "print(\"Context indices shape:\", ctx_indices.shape)\n",
    "\n",
    "print(activations.shape)\n",
    "# print(batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live features: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b7602cb3e74b03b660c0a3f65d064c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Feature:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be36bb3fd69f481284c0f9757908ade8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "live_features = top_values[0,:].nonzero().flatten()\n",
    "print(f'Live features: {live_features.shape[0]}')\n",
    "\n",
    "# Define the dropdown menu for 'feat'\n",
    "feat_dropdown = widgets.Dropdown(\n",
    "    options=range(live_features.shape[0]),\n",
    "    value=0,\n",
    "    description='Feature:',\n",
    ")\n",
    "\n",
    "dataframe_func = partial(\n",
    "    update_dataframe, \n",
    "    top_values=top_values, \n",
    "    batch_indices=batch_indices, \n",
    "    ctx_indices=ctx_indices,\n",
    "    live_features=live_features\n",
    ")\n",
    "\n",
    "# Create an interactive output widget\n",
    "output = widgets.interactive_output(\n",
    "    dataframe_func, \n",
    "    {\n",
    "        'feat': feat_dropdown,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the dropdown menu and output\n",
    "display(feat_dropdown, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNK / BOS:\n",
    "\n",
    "* 30: Fires on UNK\n",
    "* 32: Fires on UNK\n",
    "\n",
    "On x:\n",
    "\n",
    "* 7: x occurring after a bunch of other x's.\n",
    "* 8: x @ 1.\n",
    "* 9: x @ 2 with x @ 1.\n",
    "* 23: x @ 4 when context starts with c or a?\n",
    "* 25: x @ 1.\n",
    "* 33: Fires at the end of strings of x's\n",
    "* 44: Fires on x @ 1\n",
    "* 45: Mostly fires on the end of strings of 3 x's?\n",
    "* 47: fires on c @ 4 when there are no x's in context.\n",
    "\n",
    "On not-x:\n",
    "\n",
    "* 3: Non-x after full string of x's.\n",
    "* 5: Non-x after full string of non-x.\n",
    "* 13: a @ 1\n",
    "* 18: a @ 1\n",
    "* 26: Fires on not-x when the beginning of the sequence is all x's.\n",
    "* 29: Fires on not-x @ 4 when seq starts (not-x) x x\n",
    "* 39: just like 29.\n",
    "* 41: Fires on a @ 1\n",
    "* \n",
    "\n",
    "Other:\n",
    "\n",
    "* 22: Fires near the end of sequences like (non-x) x x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8pv-bJG6hoS"
   },
   "source": [
    "## Block 0 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE9ua53A6hoc",
    "outputId": "160fa32b-6f72-4b48-f4cd-c37fef9ae281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 48-L1-0.1-LR-0.0003-Tokens-1.500e+06\n",
      "n_tokens_per_buffer (millions): 0.0004\n",
      "Lower bound: n_contexts_per_buffer (millions): 8e-05\n",
      "Total training steps: 18750\n",
      "Total wandb updates: 1875\n",
      "n_tokens_per_feature_sampling_window (millions): 0.8\n",
      "n_tokens_per_dead_feature_window (millions): 0.4\n",
      "We will reset the sparsity calculation 9 times.\n",
      "Number tokens in sparsity calculation window: 1.60e+05\n",
      "Using Ghost Grads.\n"
     ]
    }
   ],
   "source": [
    "hook_name = \"blocks.0.hook_mlp_out\"\n",
    "runner_cfg = make_sae_lens_config( hook_name, 0, l1_coeff=1e-1, training_tokens=1_500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 744,
     "referenced_widgets": [
      "21a5279ec677451ba58792e4854a2616",
      "8a20bc7d13734632abf89aba5de29b78",
      "8f8c44017f6c4e48b4afab8c576bb969",
      "777cdae240644161879a2bc28a8fa1f8",
      "8c5ac6c09c6548f8bc2371707b67ad26",
      "8a1aee4c93d54e8bb96ec57425b69fc2",
      "e8b78e2de79b4174977cdd22187942ba",
      "9c5bdaaefca648ae92a6ea23c36b6304"
     ]
    },
    "id": "LoziykTq6hoc",
    "outputId": "585b9f5e-eb8a-4f42-885c-beb09de1d96c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanhanders\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/quick-experiments/wandb/run-20240701_210900-y6cwlp6x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evanhanders/benchmark_saes/runs/y6cwlp6x' target=\"_blank\">48-L1-0.1-LR-0.0003-Tokens-1.500e+06</a></strong> to <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/y6cwlp6x' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/y6cwlp6x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE:   0%|                                                                           | 0/1500000 [00:00<?, ?it/s]/opt/venv/lib/python3.10/site-packages/sae_lens/training/activations_store.py:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "3700| MSE Loss 0.009 | L1 0.092:  20%|████████                                 | 296000/1500000 [00:51<03:32, 5661.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/7dwx0lya/300080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7500| MSE Loss 0.005 | L1 0.090:  40%|████████████████▍                        | 600000/1500000 [01:43<02:34, 5817.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/7dwx0lya/600080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11200| MSE Loss 0.005 | L1 0.086:  60%|███████████████████████▉                | 896000/1500000 [02:34<01:45, 5730.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/7dwx0lya/900080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000| MSE Loss 0.005 | L1 0.068:  80%|███████████████████████████████▏       | 1200000/1500000 [03:28<00:50, 5991.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/7dwx0lya/1200080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18700| MSE Loss 0.005 | L1 0.065: 100%|██████████████████████████████████████▉| 1496000/1500000 [04:20<00:00, 5750.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving $HOME/persistent-storage/tracr_saes/task_3_checkpoints/7dwx0lya/final_1500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18700| MSE Loss 0.005 | L1 0.065: 100%|██████████████████████████████████████▉| 1496000/1500000 [04:21<00:00, 5717.20it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.050 MB of 0.050 MB uploaded (0.009 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 13.8%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>losses/mse_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▂▂▁▂▁▁▂▁▂▂▂▁▂▂▁▁▂▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▅▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▂▁▂▁▁▂▂▂</td></tr><tr><td>sparsity/below_1e-5</td><td>▁█▆▆▁▃▁▃▁</td></tr><tr><td>sparsity/below_1e-6</td><td>▁█▆▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▃▆▆█▆▅▄▄▅▄▄▃▃▃▅▃▁▁▃▃▁▂▃▃▁▃▂▂▁▂▁▃▃▂▃▂▃</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▃▄▅▆█▇█▇▇▇▇▇▆▆▆▆▆▅▅▆▅▅▅▆▅▅▅▆▅▅▆▄▆▄▆▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.1</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>1500000</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.00033</td></tr><tr><td>losses/l1_loss</td><td>0.78859</td></tr><tr><td>losses/mse_loss</td><td>0.00398</td></tr><tr><td>losses/overall_loss</td><td>0.08316</td></tr><tr><td>metrics/explained_variance</td><td>0.99307</td></tr><tr><td>metrics/explained_variance_std</td><td>0.00353</td></tr><tr><td>metrics/l0</td><td>2.325</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-3.0199</td></tr><tr><td>sparsity/below_1e-5</td><td>0</td></tr><tr><td>sparsity/below_1e-6</td><td>0</td></tr><tr><td>sparsity/dead_features</td><td>1</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>333.58334</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">48-L1-0.1-LR-0.0003-Tokens-1.500e+06</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/y6cwlp6x' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/y6cwlp6x</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240701_210900-y6cwlp6x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.tokenizer = hf_tokenizer\n",
    "store = RepeatActivationsStore.from_config(model, runner_cfg, dataset=hf_dataset)\n",
    "sae = TrainingSAE(runner_cfg)\n",
    "trainer = SAETrainer(model, sae, store, save_checkpoint, cfg = runner_cfg)\n",
    "\n",
    "if runner_cfg.log_to_wandb:\n",
    "    wandb.init(\n",
    "        project=runner_cfg.wandb_project,\n",
    "        config=runner_cfg,\n",
    "        name=runner_cfg.run_name,\n",
    "        id=runner_cfg.wandb_id,\n",
    "    )\n",
    "trainer.fit()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top values shape: torch.Size([20, 48])\n",
      "Batch indices shape: torch.Size([20, 48])\n",
      "Context indices shape: torch.Size([20, 48])\n",
      "torch.Size([320, 5, 48])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "activations = sae.encode(cache[sae.cfg.hook_name])\n",
    "top_values, batch_indices, ctx_indices = get_top_k(activations)\n",
    "print(\"Top values shape:\", top_values.shape)\n",
    "print(\"Batch indices shape:\", batch_indices.shape)\n",
    "print(\"Context indices shape:\", ctx_indices.shape)\n",
    "\n",
    "print(activations.shape)\n",
    "# print(batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2d53cb765944adac4897d15562e6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Feature:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17), value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b89fdafe84489b9ff26132446ef0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "live_features = top_values[0,:].nonzero().flatten()\n",
    "print(live_features.shape[0])\n",
    "\n",
    "# Define the dropdown menu for 'feat'\n",
    "feat_dropdown = widgets.Dropdown(\n",
    "    options=range(live_features.shape[0]),\n",
    "    value=0,\n",
    "    description='Feature:',\n",
    ")\n",
    "\n",
    "dataframe_func = partial(\n",
    "    update_dataframe, \n",
    "    top_values=top_values, \n",
    "    batch_indices=batch_indices, \n",
    "    ctx_indices=ctx_indices,\n",
    "    live_features=live_features\n",
    ")\n",
    "\n",
    "# Create an interactive output widget\n",
    "output = widgets.interactive_output(\n",
    "    dataframe_func, \n",
    "    {\n",
    "        'feat': feat_dropdown,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the dropdown menu and output\n",
    "display(feat_dropdown, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNK / BOS:\n",
    "\n",
    "* 0: UNK on 4\n",
    "* 10: UNK on 4\n",
    "* 22: UNK on 4\n",
    "* 25: BOS\n",
    "\n",
    "Counting x:\n",
    "\n",
    "* 13: x on 1\n",
    "* 36: x on 1\n",
    "* 38: x on 1\n",
    "* 30: x on 3 (no x before)\n",
    "* 33: x on 2 (with x on 1)\n",
    "* 39: x on 2 (with x on 1)\n",
    "* 11: x on 4 (fires more strongly for more x's early in context?)\n",
    "\n",
    "Not counting x:\n",
    "\n",
    "* 3: b on 3 (no x in 1-3)\n",
    "* 4: c on 3 (no x in 1-3)\n",
    "* 17: c on 2 (no x in 1)\n",
    "* 19: b on 1\n",
    "* 23: b on 1\n",
    "* 35: c on 4 (with no x before)\n",
    "* 37: b or a on 4 (with many x's before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKMyUW2v6xf1",
    "outputId": "4b94b55c-6f2b-4a38-882c-263770294595"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize the wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "\n",
    "\n",
    "# Get the artifact from the old run\n",
    "artifact = api.artifact('evanhanders/benchmark_saes/sae_case3_blocks.0.hook_mlp_out_48:latest')\n",
    "\n",
    "# Download the artifact to a specified directory\n",
    "artifact_mlp_out = artifact.download(\"./mlp_out_0\")\n",
    "\n",
    "\n",
    "artifact = api.artifact('evanhanders/benchmark_saes/sae_case3_blocks.1.hook_resid_pre_48:latest')\n",
    "\n",
    "# Download the artifact to a specified directory\n",
    "artifact_residual_1 = artifact.download(\"./hook_resid_pre_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ffqow8NZUl7b"
   },
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae_mlp = SAE.load_from_pretrained(artifact_mlp_out, device=device)\n",
    "sae_residual = SAE.load_from_pretrained(artifact_residual_1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cfg.tokenizer_prepends_bos=False\n",
    "model.cfg.default_prepend_bos=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.Tensor(hf_dataset['tokens']).to(int)\n",
    "logits, cache = model.run_with_cache(tokens, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_mlp = sae_mlp.encode(cache[sae_mlp.cfg.hook_name])\n",
    "activations_resid_1 = sae_residual.encode(cache[sae_residual.cfg.hook_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP-0 features:\n",
    "1 - on tok 4 when 1-3 are all x\n",
    "2 - weak 'b' on tok 1\n",
    "6 - 'x' on pos 2\n",
    "7 - very weak, pos 4, when there are no x's?\n",
    "12 - 'x' on pos 1\n",
    "16 - weak 'b' on pos 1.\n",
    "18 - BOS\n",
    "19 - Strong fire on x on tok 4\n",
    "20 - weak c on tok 3\n",
    "21 - BOS\n",
    "22 - weak c on tok 2\n",
    "23 - weak 'x' on pos 4 or 2.\n",
    "26 - BOS\n",
    "28 - BOS\n",
    "30 - weak 'c' on tok 3\n",
    "33 - 'x' on tok 3.\n",
    "34 - 'x' on tok 1\n",
    "39 - 'b' on tok 3\n",
    "40 - 'x' on tok 2\n",
    "42 - 'x' on tok 3\n",
    "43 - BOS\n",
    "46 - 'c' on tok 1\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cltcYcNv6Sb2"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f6a54a5242d4963959813217e192323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3181d5a610e44d16a62010de88e34ea0",
       "IPY_MODEL_a7a3ecba6fe84610b9d1626cd05f25df"
      ],
      "layout": "IPY_MODEL_ed20c3fa79784d91870a84a098f04d33"
     }
    },
    "21a5279ec677451ba58792e4854a2616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a20bc7d13734632abf89aba5de29b78",
       "IPY_MODEL_8f8c44017f6c4e48b4afab8c576bb969"
      ],
      "layout": "IPY_MODEL_777cdae240644161879a2bc28a8fa1f8"
     }
    },
    "3181d5a610e44d16a62010de88e34ea0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccbc8b30e53545cfb311f2e6d49da4fc",
      "placeholder": "​",
      "style": "IPY_MODEL_b7e63627055a42fd89d6e3dd2bfdfe42",
      "value": "0.056 MB of 0.056 MB uploaded (0.008 MB deduped)\r"
     }
    },
    "777cdae240644161879a2bc28a8fa1f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a1aee4c93d54e8bb96ec57425b69fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a20bc7d13734632abf89aba5de29b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c5ac6c09c6548f8bc2371707b67ad26",
      "placeholder": "​",
      "style": "IPY_MODEL_8a1aee4c93d54e8bb96ec57425b69fc2",
      "value": "0.056 MB of 0.056 MB uploaded (0.008 MB deduped)\r"
     }
    },
    "8c5ac6c09c6548f8bc2371707b67ad26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f8c44017f6c4e48b4afab8c576bb969": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8b78e2de79b4174977cdd22187942ba",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c5bdaaefca648ae92a6ea23c36b6304",
      "value": 1
     }
    },
    "9c5bdaaefca648ae92a6ea23c36b6304": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a7a3ecba6fe84610b9d1626cd05f25df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdbfe0ae850443b1978bed6d0d7f2bbe",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd9587b34348441da608ea83783adb63",
      "value": 1
     }
    },
    "b7e63627055a42fd89d6e3dd2bfdfe42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd9587b34348441da608ea83783adb63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ccbc8b30e53545cfb311f2e6d49da4fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdbfe0ae850443b1978bed6d0d7f2bbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8b78e2de79b4174977cdd22187942ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed20c3fa79784d91870a84a098f04d33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
