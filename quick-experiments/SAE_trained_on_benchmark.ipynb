{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjaAr6kxJ7r9"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  circuits-benchmark\t    project_template\n",
      "README.md   cluster_setup.egg-info  quick_experiments\n"
     ]
    }
   ],
   "source": [
    "# !rm /workspace/pyproject.toml\n",
    "! ls /workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/circuits-benchmark/tracr/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib.util\n",
    "import importlib.machinery\n",
    "\n",
    "# Specify the directory with your local package\n",
    "local_directory = \"/workspace/circuits-benchmark\"\n",
    "\n",
    "# The package name\n",
    "package_name = \"tracr\"\n",
    "\n",
    "# Construct the full path to the package's __init__.py file\n",
    "package_init_file = f\"{local_directory}/{package_name}/__init__.py\"\n",
    "\n",
    "# Load the package from the specified file\n",
    "spec = importlib.util.spec_from_file_location(package_name, package_init_file)\n",
    "local_package = importlib.util.module_from_spec(spec)\n",
    "sys.modules[package_name] = local_package\n",
    "spec.loader.exec_module(local_package)\n",
    "\n",
    "# Now you can use the tracr package as usual\n",
    "import tracr\n",
    "\n",
    "# Example usage of tracr\n",
    "print(tracr.__file__)\n",
    "from tracr.compiler.compiling import TracrOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuits_benchmark.benchmark.cases.case_3 as case_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "/workspace/circuits-benchmark\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/pyproject.toml'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(PROJECT_ROOT)\n\u001b[1;32m      8\u001b[0m update_proj_root()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdetect_project_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspace/circuits-benchmark/circuits_benchmark/utils/project_paths.py:18\u001b[0m, in \u001b[0;36mdetect_project_root\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m PROJECT_ROOT\n\u001b[1;32m     17\u001b[0m current_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mcurdir)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyproject.toml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     19\u001b[0m   current_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_path, os\u001b[38;5;241m.\u001b[39mpardir))\n\u001b[1;32m     21\u001b[0m PROJECT_ROOT \u001b[38;5;241m=\u001b[39m current_path\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from circuits_benchmark.utils.project_paths import detect_project_root\n",
    "from circuits_benchmark.utils.project_paths import PROJECT_ROOT\n",
    "def update_proj_root():\n",
    "    global PROJECT_ROOT\n",
    "    print(PROJECT_ROOT)\n",
    "    PROJECT_ROOT = \"/workspace/circuits-benchmark\"\n",
    "    print(PROJECT_ROOT)\n",
    "update_proj_root()\n",
    "print(detect_project_root())\n",
    "\n",
    "# import os\n",
    "\n",
    "\n",
    "# PROJECT_ROOT: str | None = None\n",
    "\n",
    "\n",
    "# def detect_project_root() -> str:\n",
    "#   \"\"\"\n",
    "#   Detects the root of the project by looking for a known file in the project.\n",
    "#   :return: the path to the root of the project.\n",
    "#   \"\"\"\n",
    "#   global PROJECT_ROOT\n",
    "#   if PROJECT_ROOT is not None:\n",
    "#     # If the project root has already been detected, return it.\n",
    "#     return PROJECT_ROOT\n",
    "\n",
    "#   current_path = os.path.abspath(os.path.curdir)\n",
    "#   while not os.path.exists(os.path.join(current_path, \"pyproject.toml\")):\n",
    "#     print(current_path)\n",
    "#     current_path = os.path.abspath(os.path.join(current_path, os.pardir))\n",
    "\n",
    "#   PROJECT_ROOT = current_path\n",
    "\n",
    "#   return current_path\n",
    "\n",
    "# print(detect_project_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "InFxPfHh7Shf",
    "outputId": "76b5529f-d9b2-4feb-ebbc-88d481b01baa"
   },
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/cybershiptrooper/InterpBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG6rfeq2J-FR"
   },
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OP0i3Tse7yEh",
    "outputId": "da12e5fc-f122-4d39-85f8-482c77748e2e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "from transformer_lens import HookedTransformer\n",
    "from circuits_benchmark.transformers.hooked_tracr_transformer import HookedTracrTransformer\n",
    "\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "from sae_lens import SAEConfig, SAE, TrainingSAEConfig, TrainingSAE, ActivationsStore, CacheActivationsRunnerConfig, LanguageModelSAERunnerConfig\n",
    "from sae_lens.training.sae_trainer import SAETrainer\n",
    "\n",
    "import sae_lens\n",
    "print(dir(sae_lens.training.sae_trainer))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouaNs_5kvQk1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from safetensors.torch import save_file\n",
    "import wandb\n",
    "from sae_lens.sae import SAE_CFG_PATH, SAE_WEIGHTS_PATH, SPARSITY_PATH\n",
    "\n",
    "def save_checkpoint(\n",
    "        trainer: SAETrainer,\n",
    "        checkpoint_name: int | str,\n",
    "        wandb_aliases: list[str] | None = None,\n",
    "    ) -> str:\n",
    "        \"\"\" Lightly modified from https://github.com/jbloomAus/SAELens/blob/v3.5.0/sae_lens/sae_training_runner.py#L161C5-L210C31 \"\"\"\n",
    "\n",
    "        sae = trainer.sae\n",
    "        os.makedirs(trainer.cfg.checkpoint_path, exist_ok=True)\n",
    "        checkpoint_path = f\"{trainer.cfg.checkpoint_path}/{checkpoint_name}\"\n",
    "\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "        path = f\"{checkpoint_path}\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        if sae.cfg.normalize_sae_decoder:\n",
    "            sae.set_decoder_norm_to_unit_norm()\n",
    "        sae.save_model(path)\n",
    "\n",
    "        # let's over write the cfg file with the trainer cfg, which is a super set of the original cfg.\n",
    "        # and should not cause issues but give us more info about SAEs we trained in SAE Lens.\n",
    "        config = trainer.cfg.to_dict()\n",
    "        with open(f\"{path}/cfg.json\", \"w\") as f:\n",
    "            json.dump(config, f)\n",
    "        if trainer.cfg.log_to_wandb:\n",
    "            print(f'saving {path}')\n",
    "            wandb.save(path)\n",
    "\n",
    "\n",
    "        log_feature_sparsities = {\"sparsity\": trainer.log_feature_sparsity}\n",
    "\n",
    "        log_feature_sparsity_path = f\"{path}/{SPARSITY_PATH}\"\n",
    "        save_file(log_feature_sparsities, log_feature_sparsity_path)\n",
    "\n",
    "        if trainer.cfg.log_to_wandb and os.path.exists(log_feature_sparsity_path):\n",
    "            model_artifact = wandb.Artifact(\n",
    "                f\"{sae.get_name()}\",\n",
    "                type=\"model\",\n",
    "                metadata=dict(trainer.cfg.__dict__),\n",
    "            )\n",
    "\n",
    "            model_artifact.add_file(f\"{path}/{SAE_WEIGHTS_PATH}\")\n",
    "            model_artifact.add_file(f\"{path}/{SAE_CFG_PATH}\")\n",
    "\n",
    "            wandb.log_artifact(model_artifact, aliases=wandb_aliases)\n",
    "\n",
    "            sparsity_artifact = wandb.Artifact(\n",
    "                f\"{sae.get_name()}_log_feature_sparsity\",\n",
    "                type=\"log_feature_sparsity\",\n",
    "                metadata=dict(trainer.cfg.__dict__),\n",
    "            )\n",
    "            sparsity_artifact.add_file(log_feature_sparsity_path)\n",
    "            wandb.log_artifact(sparsity_artifact)\n",
    "\n",
    "        return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtOpy8z6ALGU"
   },
   "outputs": [],
   "source": [
    "import circuits_benchmark.benchmark.cases.case_3 as case_3\n",
    "\n",
    "task = case_3.Case3()\n",
    "task_idx = task.get_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ytz3DejTAMwj",
    "outputId": "5f4a2b56-8149-4a0b-9ecb-7434dcd54401"
   },
   "outputs": [],
   "source": [
    "dir_name = f\"InterpBench/{task_idx}\"\n",
    "cfg_dict = pickle.load(open(f\"{dir_name}/ll_model_cfg.pkl\", \"rb\"))\n",
    "cfg = HookedTransformerConfig.from_dict(cfg_dict)\n",
    "model = HookedTransformer(cfg)\n",
    "weights = torch.load(f\"{dir_name}/ll_model.pth\")\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlB0DKXDt_V6"
   },
   "source": [
    "# Load and configure benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DT6AJSAdARcn",
    "outputId": "fd97e71f-cbe0-40af-972c-b4eb2bc7de60"
   },
   "outputs": [],
   "source": [
    "%cd /workspace/circuits-benchmark\n",
    "# load high level model\n",
    "from circuits_benchmark.utils.iit import make_iit_hl_model\n",
    "import circuits_benchmark.utils.iit.correspondence as correspondence\n",
    "from circuits_benchmark.utils.iit.dataset import get_unique_data\n",
    "import iit.model_pairs as mp\n",
    "from datasets import Dataset\n",
    "\n",
    "def make_model_pair(benchmark_case):\n",
    "    hl_model = benchmark_case.build_transformer_lens_model()\n",
    "    hl_model = make_iit_hl_model(hl_model, eval_mode=True)\n",
    "    tracr_output = benchmark_case.get_tracr_output()\n",
    "    hl_ll_corr = correspondence.TracrCorrespondence.from_output(\n",
    "            case=benchmark_case, tracr_output=tracr_output\n",
    "        )\n",
    "    model_pair = mp.StrictIITModelPair(hl_model, model, hl_ll_corr)\n",
    "    return model_pair, hl_model\n",
    "\n",
    "model_pair, hl_model = make_model_pair(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXw8nhAeSzCg",
    "outputId": "e5b5b5d7-c7d7-4803-a311-520ae55b8d4a"
   },
   "outputs": [],
   "source": [
    "# Create dataset of case inputs\n",
    "dataset = get_unique_data(task, max_len=10_000)\n",
    "tokenized_data = hl_model.map_tracr_input_to_tl_input(dataset.data)\n",
    "\n",
    "# Convert PyTorch tensors to lists\n",
    "string_tokens_list = dataset.data.tolist()\n",
    "tokens_list = tokenized_data.tolist()\n",
    "labels_list = [str(label) for label in dataset.labels]\n",
    "\n",
    "# Create a dictionary from the lists\n",
    "data_dict = {\n",
    "    \"string_tokens\": string_tokens_list,\n",
    "    \"tokens\": tokens_list,\n",
    "    \"labels\": labels_list\n",
    "}\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "hf_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "print(hf_dataset)\n",
    "print(hf_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KItWVrlgfx9v",
    "outputId": "5973470a-d801-48c8-9c68-c26e0b063744"
   },
   "outputs": [],
   "source": [
    "print(hf_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfEZgq7_bP08",
    "outputId": "fb85194c-5196-461c-a60c-8253d5b24118"
   },
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "# Define your simple vocabulary\n",
    "vocab = {'BOS': 0, 'UNK': 1, 'a': 2, 'b': 3, 'c': 4, 'x': 5}\n",
    "# comes from task.get_vocab() and hl_model.map_tracr_input_to_tl_input\n",
    "\n",
    "# Create a Tokenizer with a WordLevel model\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"UNK\"))\n",
    "\n",
    "# Set the normalizer, pre-tokenizer, and decoder\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Convert to Hugging Face tokenizer\n",
    "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "# Add the special tokens to the Hugging Face tokenizer\n",
    "hf_tokenizer.add_special_tokens({\n",
    "    'unk_token': 'UNK',\n",
    "    'bos_token': 'BOS',\n",
    "    'cls_token': '[CLS]',\n",
    "    'sep_token': '[SEP]',\n",
    "    'pad_token': '[PAD]',\n",
    "    'mask_token': '[MASK]'\n",
    "})\n",
    "\n",
    "# Test the tokenizer\n",
    "encoded = hf_tokenizer.encode(\"BOS a b c x\")\n",
    "decoded = hf_tokenizer.decode(encoded)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oShVxNlpvfyn"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hf_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mhf_tokenizer\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hf_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model.tokenizer = hf_tokenizer #attach to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eilYaeGcLuMY",
    "outputId": "e3ae7db7-3487-4411-fe62-52bfe94b78fc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, cache \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_cache(\u001b[43mtokenized_data\u001b[49m)\n\u001b[1;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m hl_model(tokenized_data) \u001b[38;5;66;03m#TODO: why are these different calls?\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(output[:\u001b[38;5;241m5\u001b[39m], dataset\u001b[38;5;241m.\u001b[39mlabels[:\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_data' is not defined"
     ]
    }
   ],
   "source": [
    "_, cache = model.run_with_cache(tokenized_data)\n",
    "output = hl_model(tokenized_data) #TODO: why are these different calls?\n",
    "print(output[:5], dataset.labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w16J8LtAO4WU",
    "outputId": "817b52ed-d62d-4a0f-ef8f-d5bb159480c1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcache\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cache' is not defined"
     ]
    }
   ],
   "source": [
    "print(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu8iSnvHNG38"
   },
   "source": [
    "# SAE-lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_by85gQ_g1w",
    "outputId": "a871622b-355a-46d3-826e-fa1730167e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "\u001b[0m\u001b[01;34mcircuits-benchmark\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "%pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTJJyQtOc4mG"
   },
   "outputs": [],
   "source": [
    "class RepeatActivationsStore(ActivationsStore):\n",
    "    def _get_next_dataset_tokens(self) -> torch.Tensor:\n",
    "        device = self.device\n",
    "        if not self.is_dataset_tokenized:\n",
    "            try:\n",
    "                s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            except StopIteration:\n",
    "                #shuffle self.dataset and restart\n",
    "                self.iterable_dataset = iter(self.dataset)\n",
    "                s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            tokens = (\n",
    "                self.model.to_tokens(\n",
    "                    s,\n",
    "                    truncate=False,\n",
    "                    move_to_device=True,\n",
    "                    prepend_bos=self.prepend_bos,\n",
    "                )\n",
    "                .squeeze(0)\n",
    "                .to(device)\n",
    "            )\n",
    "            assert (\n",
    "                len(tokens.shape) == 1\n",
    "            ), f\"tokens.shape should be 1D but was {tokens.shape}\"\n",
    "        else:\n",
    "            try:\n",
    "                s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            except StopIteration:\n",
    "                #shuffle self.dataset and restart\n",
    "                self.iterable_dataset = iter(self.dataset)\n",
    "                s = next(self.iterable_dataset)[self.tokens_column]\n",
    "            tokens = torch.tensor(\n",
    "                s,\n",
    "                dtype=torch.long,\n",
    "                device=device,\n",
    "                requires_grad=False,\n",
    "            )\n",
    "            if (\n",
    "                not self.prepend_bos\n",
    "                and tokens[0] == self.model.tokenizer.bos_token_id  # type: ignore\n",
    "            ):\n",
    "                tokens = tokens[1:]\n",
    "        self.n_dataset_processed += 1\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cltcYcNv6Sb2"
   },
   "source": [
    "## Residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhrGtjCBNZM7",
    "outputId": "01fb8c7d-491b-474e-c479-395a3065d72e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 48-L1-0.1-LR-0.0003-Tokens-5.000e+06\n",
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.0064\n",
      "Total training steps: 10000\n",
      "Total wandb updates: 1000\n",
      "n_tokens_per_feature_sampling_window (millions): 5.0\n",
      "n_tokens_per_dead_feature_window (millions): 2.5\n",
      "We will reset the sparsity calculation 5 times.\n",
      "Number tokens in sparsity calculation window: 1.00e+06\n",
      "Using Ghost Grads.\n"
     ]
    }
   ],
   "source": [
    "L1_coeff = 1e-1\n",
    "training_tokens = 5_000_000\n",
    "runner_cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distribution)\n",
    "    model_name = \"case3\",\n",
    "    model_class_name = \"HookedTransformer\",\n",
    "    hook_name = \"blocks.1.hook_resid_pre\",\n",
    "    hook_eval = \"NOT_IN_USE\",\n",
    "    hook_layer = 1,\n",
    "    hook_head_index = None,\n",
    "    dataset_path = \"\",\n",
    "    dataset_trust_remote_code = False,\n",
    "    streaming = False,\n",
    "    is_dataset_tokenized = True,\n",
    "    context_size = 5,\n",
    "    use_cached_activations = False,\n",
    "    cached_activations_path = None,  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_head_index}\"\n",
    "\n",
    "    # SAE Parameters\n",
    "    d_in = model.cfg.d_model,\n",
    "    d_sae = None,\n",
    "    b_dec_init_method = \"geometric_median\",\n",
    "    expansion_factor = 4,\n",
    "    activation_fn = \"relu\",  # relu, tanh-relu\n",
    "    normalize_sae_decoder = True,\n",
    "    noise_scale = 0.0,\n",
    "    from_pretrained_path = None,\n",
    "    apply_b_dec_to_input = False,\n",
    "    decoder_orthogonal_init = False,\n",
    "    decoder_heuristic_init = False,\n",
    "    init_encoder_as_decoder_transpose = False,\n",
    "\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = hf_dataset.shape[0],\n",
    "    training_tokens = training_tokens,\n",
    "    finetuning_tokens = 0,\n",
    "    store_batch_size_prompts = 5,\n",
    "    normalize_activations = \"none\",  # none, expected_average_only_in (Anthropic April Update), constant_norm_rescale (Anthropic Feb Update)\n",
    "\n",
    "    # Misc\n",
    "    device = device,\n",
    "    act_store_device = \"with_model\",  # will be set by post init if with_model\n",
    "    seed = 42,\n",
    "    dtype = \"float32\",  # type: ignore #\n",
    "    prepend_bos = False,\n",
    "\n",
    "    # Performance - see compilation section of lm_runner.py for info\n",
    "    autocast = False,  # autocast to autocast_dtype during training\n",
    "    autocast_lm = False,  # autocast lm during activation fetching\n",
    "    compile_llm = False,  # use torch.compile on the LLM\n",
    "    llm_compilation_mode = None,  # which torch.compile mode to use\n",
    "    compile_sae = False,  # use torch.compile on the SAE\n",
    "    sae_compilation_mode = None,\n",
    "\n",
    "    # Training Parameters\n",
    "\n",
    "    ## Batch size\n",
    "    train_batch_size_tokens = 5*100,\n",
    "\n",
    "    ## Adam\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.999,\n",
    "\n",
    "    ## Loss Function\n",
    "    mse_loss_normalization = None,\n",
    "    l1_coefficient = L1_coeff,\n",
    "    lp_norm = 1,\n",
    "    scale_sparsity_penalty_by_decoder_norm = False,\n",
    "    l1_warm_up_steps = 0,\n",
    "\n",
    "    ## Learning Rate Schedule\n",
    "    lr = 3e-4,\n",
    "    lr_scheduler_name = \"constant\",  # constant, cosineannealing, cosineannealingwarmrestarts\n",
    "    lr_warm_up_steps = 0,\n",
    "    lr_end = None,  # only used for cosine annealing, default is lr / 10\n",
    "    lr_decay_steps = 0,\n",
    "    n_restart_cycles = 1,  # used only for cosineannealingwarmrestarts\n",
    "\n",
    "    ## FineTuning\n",
    "    finetuning_method = None,  # scale, decoder or unrotated_decoder\n",
    "\n",
    "    # Resampling protocol args\n",
    "    use_ghost_grads = True,  # want to change this to true on some timeline.\n",
    "    feature_sampling_window = 2000,\n",
    "    dead_feature_window = 1000,  # unless this window is larger feature sampling,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "\n",
    "    # Evals\n",
    "    n_eval_batches = 10,\n",
    "    eval_batch_size_prompts = None,  # useful if evals cause OOM\n",
    "\n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    log_activations_store_to_wandb = False,\n",
    "    log_optimizer_state_to_wandb = False,\n",
    "    wandb_project = \"benchmark_saes\",\n",
    "    wandb_id = None,\n",
    "    run_name = None,\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency = 10,\n",
    "    eval_every_n_wandb_logs = 100000000000, # Make this a really big number; currently fails because it tries to compute CE loss.\n",
    "    # Misc\n",
    "    resume = False,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    verbose = True,\n",
    "    model_kwargs = dict(),\n",
    "    model_from_pretrained_kwargs = dict(),\n",
    "    sae_lens_version = str(sae_lens.__version__),\n",
    "    sae_lens_training_version = str(sae_lens.__version__),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814,
     "referenced_widgets": [
      "0f6a54a5242d4963959813217e192323",
      "3181d5a610e44d16a62010de88e34ea0",
      "a7a3ecba6fe84610b9d1626cd05f25df",
      "ed20c3fa79784d91870a84a098f04d33",
      "ccbc8b30e53545cfb311f2e6d49da4fc",
      "b7e63627055a42fd89d6e3dd2bfdfe42",
      "cdbfe0ae850443b1978bed6d0d7f2bbe",
      "bd9587b34348441da608ea83783adb63"
     ]
    },
    "id": "AMAR0c3xgL-F",
    "outputId": "b9d8cd42-b442-4693-a208-4e69157a6952"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240624_224532-lv7zf8wr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evanhanders/benchmark_saes/runs/lv7zf8wr' target=\"_blank\">48-L1-0.1-LR-0.0003-Tokens-5.000e+06</a></strong> to <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/lv7zf8wr' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/lv7zf8wr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000| MSE Loss 0.027 | L1 0.199:  20%|██        | 1000000/5000000 [04:09<15:58, 4171.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/ubcrpfrv/1000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000| MSE Loss 0.018 | L1 0.144:  40%|████      | 2000000/5000000 [08:07<11:37, 4303.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/ubcrpfrv/2000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000| MSE Loss 0.013 | L1 0.126:  60%|██████    | 3000000/5000000 [12:11<08:02, 4147.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/ubcrpfrv/3000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000| MSE Loss 0.011 | L1 0.120:  80%|████████  | 4000000/5000000 [16:10<03:51, 4316.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/ubcrpfrv/4000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000| MSE Loss 0.010 | L1 0.125: 100%|██████████| 5000000/5000000 [20:14<00:00, 4128.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/ubcrpfrv/final_5000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10000| MSE Loss 0.010 | L1 0.125: 100%|██████████| 5000000/5000000 [20:15<00:00, 4114.00it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6a54a5242d4963959813217e192323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.038 MB of 0.038 MB uploaded (0.008 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 15.1%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▆▅▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▃▂▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▅▅▁█▅</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁█▁▁█▁▁▁█▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▂▃▅▄▃▅▆▄▃▄▆▄▃▅▇▆▄▅▇▆▄▆▇▆▅▆▇▆▅▆█▆▅▆█▆▄▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.1</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>5000000</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.00084</td></tr><tr><td>losses/l1_loss</td><td>1.24649</td></tr><tr><td>losses/mse_loss</td><td>0.01008</td></tr><tr><td>losses/overall_loss</td><td>0.13557</td></tr><tr><td>metrics/explained_variance</td><td>0.9872</td></tr><tr><td>metrics/explained_variance_std</td><td>0.01382</td></tr><tr><td>metrics/l0</td><td>4.456</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-2.1664</td></tr><tr><td>sparsity/below_1e-5</td><td>1</td></tr><tr><td>sparsity/below_1e-6</td><td>0</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>227.64584</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">48-L1-0.1-LR-0.0003-Tokens-5.000e+06</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/lv7zf8wr' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/lv7zf8wr</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240624_224532-lv7zf8wr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any, cast\n",
    "\n",
    "model.tokenizer = hf_tokenizer\n",
    "store = RepeatActivationsStore.from_config(model, runner_cfg, dataset=hf_dataset)\n",
    "sae = TrainingSAE(runner_cfg)\n",
    "trainer = SAETrainer(model, sae, store, save_checkpoint, cfg = runner_cfg)\n",
    "\n",
    "if runner_cfg.log_to_wandb:\n",
    "    wandb.init(\n",
    "        project=runner_cfg.wandb_project,\n",
    "        config=cast(Any, runner_cfg),\n",
    "        name=runner_cfg.run_name,\n",
    "        id=runner_cfg.wandb_id,\n",
    "    )\n",
    "trainer.fit()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kW2CbCZ7qiZ2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8pv-bJG6hoS"
   },
   "source": [
    "## Residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE9ua53A6hoc",
    "outputId": "160fa32b-6f72-4b48-f4cd-c37fef9ae281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 48-L1-0.1-LR-0.0003-Tokens-5.000e+06\n",
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.0064\n",
      "Total training steps: 10000\n",
      "Total wandb updates: 1000\n",
      "n_tokens_per_feature_sampling_window (millions): 5.0\n",
      "n_tokens_per_dead_feature_window (millions): 2.5\n",
      "We will reset the sparsity calculation 5 times.\n",
      "Number tokens in sparsity calculation window: 1.00e+06\n",
      "Using Ghost Grads.\n"
     ]
    }
   ],
   "source": [
    "L1_coeff = 1e-1\n",
    "training_tokens = 5_000_000\n",
    "runner_cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distribution)\n",
    "    model_name = \"case3\",\n",
    "    model_class_name = \"HookedTransformer\",\n",
    "    hook_name = \"blocks.0.hook_mlp_out\",\n",
    "    hook_eval = \"NOT_IN_USE\",\n",
    "    hook_layer = 0,\n",
    "    hook_head_index = None,\n",
    "    dataset_path = \"\",\n",
    "    dataset_trust_remote_code = False,\n",
    "    streaming = False,\n",
    "    is_dataset_tokenized = True,\n",
    "    context_size = 5,\n",
    "    use_cached_activations = False,\n",
    "    cached_activations_path = None,  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_head_index}\"\n",
    "\n",
    "    # SAE Parameters\n",
    "    d_in = model.cfg.d_model,\n",
    "    d_sae = None,\n",
    "    b_dec_init_method = \"geometric_median\",\n",
    "    expansion_factor = 4,\n",
    "    activation_fn = \"relu\",  # relu, tanh-relu\n",
    "    normalize_sae_decoder = True,\n",
    "    noise_scale = 0.0,\n",
    "    from_pretrained_path = None,\n",
    "    apply_b_dec_to_input = False,\n",
    "    decoder_orthogonal_init = False,\n",
    "    decoder_heuristic_init = False,\n",
    "    init_encoder_as_decoder_transpose = False,\n",
    "\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = hf_dataset.shape[0],\n",
    "    training_tokens = training_tokens,\n",
    "    finetuning_tokens = 0,\n",
    "    store_batch_size_prompts = 5,\n",
    "    normalize_activations = \"none\",  # none, expected_average_only_in (Anthropic April Update), constant_norm_rescale (Anthropic Feb Update)\n",
    "\n",
    "    # Misc\n",
    "    device = device,\n",
    "    act_store_device = \"with_model\",  # will be set by post init if with_model\n",
    "    seed = 42,\n",
    "    dtype = \"float32\",  # type: ignore #\n",
    "    prepend_bos = False,\n",
    "\n",
    "    # Performance - see compilation section of lm_runner.py for info\n",
    "    autocast = False,  # autocast to autocast_dtype during training\n",
    "    autocast_lm = False,  # autocast lm during activation fetching\n",
    "    compile_llm = False,  # use torch.compile on the LLM\n",
    "    llm_compilation_mode = None,  # which torch.compile mode to use\n",
    "    compile_sae = False,  # use torch.compile on the SAE\n",
    "    sae_compilation_mode = None,\n",
    "\n",
    "    # Training Parameters\n",
    "\n",
    "    ## Batch size\n",
    "    train_batch_size_tokens = 5*100,\n",
    "\n",
    "    ## Adam\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.999,\n",
    "\n",
    "    ## Loss Function\n",
    "    mse_loss_normalization = None,\n",
    "    l1_coefficient = L1_coeff,\n",
    "    lp_norm = 1,\n",
    "    scale_sparsity_penalty_by_decoder_norm = False,\n",
    "    l1_warm_up_steps = 0,\n",
    "\n",
    "    ## Learning Rate Schedule\n",
    "    lr = 3e-4,\n",
    "    lr_scheduler_name = \"constant\",  # constant, cosineannealing, cosineannealingwarmrestarts\n",
    "    lr_warm_up_steps = 0,\n",
    "    lr_end = None,  # only used for cosine annealing, default is lr / 10\n",
    "    lr_decay_steps = 0,\n",
    "    n_restart_cycles = 1,  # used only for cosineannealingwarmrestarts\n",
    "\n",
    "    ## FineTuning\n",
    "    finetuning_method = None,  # scale, decoder or unrotated_decoder\n",
    "\n",
    "    # Resampling protocol args\n",
    "    use_ghost_grads = True,  # want to change this to true on some timeline.\n",
    "    feature_sampling_window = 2000,\n",
    "    dead_feature_window = 1000,  # unless this window is larger feature sampling,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "\n",
    "    # Evals\n",
    "    n_eval_batches = 10,\n",
    "    eval_batch_size_prompts = None,  # useful if evals cause OOM\n",
    "\n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    log_activations_store_to_wandb = False,\n",
    "    log_optimizer_state_to_wandb = False,\n",
    "    wandb_project = \"benchmark_saes\",\n",
    "    wandb_id = None,\n",
    "    run_name = None,\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency = 10,\n",
    "    eval_every_n_wandb_logs = 100000000000, # Make this a really big number; currently fails because it tries to compute CE loss.\n",
    "    # Misc\n",
    "    resume = False,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    verbose = True,\n",
    "    model_kwargs = dict(),\n",
    "    model_from_pretrained_kwargs = dict(),\n",
    "    sae_lens_version = str(sae_lens.__version__),\n",
    "    sae_lens_training_version = str(sae_lens.__version__),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFykBbfzCw9k"
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 744,
     "referenced_widgets": [
      "21a5279ec677451ba58792e4854a2616",
      "8a20bc7d13734632abf89aba5de29b78",
      "8f8c44017f6c4e48b4afab8c576bb969",
      "777cdae240644161879a2bc28a8fa1f8",
      "8c5ac6c09c6548f8bc2371707b67ad26",
      "8a1aee4c93d54e8bb96ec57425b69fc2",
      "e8b78e2de79b4174977cdd22187942ba",
      "9c5bdaaefca648ae92a6ea23c36b6304"
     ]
    },
    "id": "LoziykTq6hoc",
    "outputId": "585b9f5e-eb8a-4f42-885c-beb09de1d96c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanhanders\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240624_230603-2canux1z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evanhanders/benchmark_saes/runs/2canux1z' target=\"_blank\">48-L1-0.1-LR-0.0003-Tokens-5.000e+06</a></strong> to <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/2canux1z' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/2canux1z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000| MSE Loss 0.028 | L1 0.134:  20%|██        | 1000000/5000000 [03:06<12:01, 5541.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/3p1891vk/1000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000| MSE Loss 0.018 | L1 0.098:  40%|████      | 2000000/5000000 [06:07<08:50, 5658.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/3p1891vk/2000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000| MSE Loss 0.014 | L1 0.094:  60%|██████    | 3000000/5000000 [09:10<05:58, 5581.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/3p1891vk/3000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000| MSE Loss 0.012 | L1 0.094:  80%|████████  | 4000000/5000000 [12:11<02:55, 5683.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/3p1891vk/4000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000| MSE Loss 0.011 | L1 0.091: 100%|██████████| 5000000/5000000 [15:16<00:00, 5508.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints/3p1891vk/final_5000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10000| MSE Loss 0.011 | L1 0.091: 100%|██████████| 5000000/5000000 [15:16<00:00, 5453.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a5279ec677451ba58792e4854a2616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.038 MB of 0.038 MB uploaded (0.008 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 15.1%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁███████████████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▃▂▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▆▃▁█</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▃█▃▁▁▃▁▁▁▁▁▃▁▁▁▁▁▁▁▃▁▃▁▁▃▁▁▁▁▁▃▁▁▃</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▂▂▃▃▃▅▅▆▅▅▇▅▄▅▆▆▅▅▇▇▅▅▇▆▅▆█▇▆▆█▇▆▆▇▇▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.1</td></tr><tr><td>details/current_learning_rate</td><td>0.0003</td></tr><tr><td>details/n_training_tokens</td><td>5000000</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0009</td></tr><tr><td>losses/l1_loss</td><td>0.91036</td></tr><tr><td>losses/mse_loss</td><td>0.01078</td></tr><tr><td>losses/overall_loss</td><td>0.10271</td></tr><tr><td>metrics/explained_variance</td><td>0.97033</td></tr><tr><td>metrics/explained_variance_std</td><td>0.04429</td></tr><tr><td>metrics/l0</td><td>4.47</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-2.4882</td></tr><tr><td>sparsity/below_1e-5</td><td>3</td></tr><tr><td>sparsity/below_1e-6</td><td>0</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>246.9375</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">48-L1-0.1-LR-0.0003-Tokens-5.000e+06</strong> at: <a href='https://wandb.ai/evanhanders/benchmark_saes/runs/2canux1z' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes/runs/2canux1z</a><br/> View project at: <a href='https://wandb.ai/evanhanders/benchmark_saes' target=\"_blank\">https://wandb.ai/evanhanders/benchmark_saes</a><br/>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240624_230603-2canux1z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any, cast\n",
    "\n",
    "model.tokenizer = hf_tokenizer\n",
    "store = RepeatActivationsStore.from_config(model, runner_cfg, dataset=hf_dataset)\n",
    "sae = TrainingSAE(runner_cfg)\n",
    "trainer = SAETrainer(model, sae, store, save_checkpoint, cfg = runner_cfg)\n",
    "\n",
    "if runner_cfg.log_to_wandb:\n",
    "    wandb.init(\n",
    "        project=runner_cfg.wandb_project,\n",
    "        config=cast(Any, runner_cfg),\n",
    "        name=runner_cfg.run_name,\n",
    "        id=runner_cfg.wandb_id,\n",
    "    )\n",
    "trainer.fit()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKMyUW2v6xf1",
    "outputId": "4b94b55c-6f2b-4a38-882c-263770294595"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize the wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "\n",
    "\n",
    "# Get the artifact from the old run\n",
    "artifact = api.artifact('evanhanders/benchmark_saes/sae_case3_blocks.0.hook_mlp_out_48:v32')\n",
    "\n",
    "# Download the artifact to a specified directory\n",
    "artifact_dir = artifact.download(\"/content/sample_data/mlp_out_0\")\n",
    "\n",
    "\n",
    "artifact = api.artifact('evanhanders/benchmark_saes/sae_case3_blocks.1.hook_resid_pre_48:v5')\n",
    "\n",
    "# Download the artifact to a specified directory\n",
    "artifact_dir = artifact.download(\"/content/sample_data/hook_resid_pre_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffqow8NZUl7b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cltcYcNv6Sb2"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f6a54a5242d4963959813217e192323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3181d5a610e44d16a62010de88e34ea0",
       "IPY_MODEL_a7a3ecba6fe84610b9d1626cd05f25df"
      ],
      "layout": "IPY_MODEL_ed20c3fa79784d91870a84a098f04d33"
     }
    },
    "21a5279ec677451ba58792e4854a2616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a20bc7d13734632abf89aba5de29b78",
       "IPY_MODEL_8f8c44017f6c4e48b4afab8c576bb969"
      ],
      "layout": "IPY_MODEL_777cdae240644161879a2bc28a8fa1f8"
     }
    },
    "3181d5a610e44d16a62010de88e34ea0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccbc8b30e53545cfb311f2e6d49da4fc",
      "placeholder": "​",
      "style": "IPY_MODEL_b7e63627055a42fd89d6e3dd2bfdfe42",
      "value": "0.056 MB of 0.056 MB uploaded (0.008 MB deduped)\r"
     }
    },
    "777cdae240644161879a2bc28a8fa1f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a1aee4c93d54e8bb96ec57425b69fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a20bc7d13734632abf89aba5de29b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c5ac6c09c6548f8bc2371707b67ad26",
      "placeholder": "​",
      "style": "IPY_MODEL_8a1aee4c93d54e8bb96ec57425b69fc2",
      "value": "0.056 MB of 0.056 MB uploaded (0.008 MB deduped)\r"
     }
    },
    "8c5ac6c09c6548f8bc2371707b67ad26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f8c44017f6c4e48b4afab8c576bb969": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8b78e2de79b4174977cdd22187942ba",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c5bdaaefca648ae92a6ea23c36b6304",
      "value": 1
     }
    },
    "9c5bdaaefca648ae92a6ea23c36b6304": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a7a3ecba6fe84610b9d1626cd05f25df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdbfe0ae850443b1978bed6d0d7f2bbe",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd9587b34348441da608ea83783adb63",
      "value": 1
     }
    },
    "b7e63627055a42fd89d6e3dd2bfdfe42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd9587b34348441da608ea83783adb63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ccbc8b30e53545cfb311f2e6d49da4fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdbfe0ae850443b1978bed6d0d7f2bbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8b78e2de79b4174977cdd22187942ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed20c3fa79784d91870a84a098f04d33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
